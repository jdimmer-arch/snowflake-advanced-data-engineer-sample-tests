function hh(o,c){for(var l=0;l<c.length;l++){const u=c[l];if(typeof u!="string"&&!Array.isArray(u)){for(const h in u)if(h!=="default"&&!(h in o)){const g=Object.getOwnPropertyDescriptor(u,h);g&&Object.defineProperty(o,h,g.get?g:{enumerable:!0,get:()=>u[h]})}}}return Object.freeze(Object.defineProperty(o,Symbol.toStringTag,{value:"Module"}))}(function(){const c=document.createElement("link").relList;if(c&&c.supports&&c.supports("modulepreload"))return;for(const h of document.querySelectorAll('link[rel="modulepreload"]'))u(h);new MutationObserver(h=>{for(const g of h)if(g.type==="childList")for(const S of g.addedNodes)S.tagName==="LINK"&&S.rel==="modulepreload"&&u(S)}).observe(document,{childList:!0,subtree:!0});function l(h){const g={};return h.integrity&&(g.integrity=h.integrity),h.referrerPolicy&&(g.referrerPolicy=h.referrerPolicy),h.crossOrigin==="use-credentials"?g.credentials="include":h.crossOrigin==="anonymous"?g.credentials="omit":g.credentials="same-origin",g}function u(h){if(h.ep)return;h.ep=!0;const g=l(h);fetch(h.href,g)}})();function fh(o){return o&&o.__esModule&&Object.prototype.hasOwnProperty.call(o,"default")?o.default:o}var Ko={exports:{}},Pi={},Xo={exports:{}},te={};/**
 * @license React
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var cd;function mh(){if(cd)return te;cd=1;var o=Symbol.for("react.element"),c=Symbol.for("react.portal"),l=Symbol.for("react.fragment"),u=Symbol.for("react.strict_mode"),h=Symbol.for("react.profiler"),g=Symbol.for("react.provider"),S=Symbol.for("react.context"),y=Symbol.for("react.forward_ref"),k=Symbol.for("react.suspense"),A=Symbol.for("react.memo"),T=Symbol.for("react.lazy"),E=Symbol.iterator;function q(m){return m===null||typeof m!="object"?null:(m=E&&m[E]||m["@@iterator"],typeof m=="function"?m:null)}var z={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},W=Object.assign,P={};function C(m,R,ee){this.props=m,this.context=R,this.refs=P,this.updater=ee||z}C.prototype.isReactComponent={},C.prototype.setState=function(m,R){if(typeof m!="object"&&typeof m!="function"&&m!=null)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,m,R,"setState")},C.prototype.forceUpdate=function(m){this.updater.enqueueForceUpdate(this,m,"forceUpdate")};function Y(){}Y.prototype=C.prototype;function Q(m,R,ee){this.props=m,this.context=R,this.refs=P,this.updater=ee||z}var K=Q.prototype=new Y;K.constructor=Q,W(K,C.prototype),K.isPureReactComponent=!0;var X=Array.isArray,fe=Object.prototype.hasOwnProperty,me={current:null},Ae={key:!0,ref:!0,__self:!0,__source:!0};function le(m,R,ee){var ie,re={},oe=null,pe=null;if(R!=null)for(ie in R.ref!==void 0&&(pe=R.ref),R.key!==void 0&&(oe=""+R.key),R)fe.call(R,ie)&&!Ae.hasOwnProperty(ie)&&(re[ie]=R[ie]);var ce=arguments.length-2;if(ce===1)re.children=ee;else if(1<ce){for(var xe=Array(ce),et=0;et<ce;et++)xe[et]=arguments[et+2];re.children=xe}if(m&&m.defaultProps)for(ie in ce=m.defaultProps,ce)re[ie]===void 0&&(re[ie]=ce[ie]);return{$$typeof:o,type:m,key:oe,ref:pe,props:re,_owner:me.current}}function Ye(m,R){return{$$typeof:o,type:m.type,key:R,ref:m.ref,props:m.props,_owner:m._owner}}function Le(m){return typeof m=="object"&&m!==null&&m.$$typeof===o}function Fe(m){var R={"=":"=0",":":"=2"};return"$"+m.replace(/[=:]/g,function(ee){return R[ee]})}var ne=/\/+/g;function Ie(m,R){return typeof m=="object"&&m!==null&&m.key!=null?Fe(""+m.key):R.toString(36)}function Pe(m,R,ee,ie,re){var oe=typeof m;(oe==="undefined"||oe==="boolean")&&(m=null);var pe=!1;if(m===null)pe=!0;else switch(oe){case"string":case"number":pe=!0;break;case"object":switch(m.$$typeof){case o:case c:pe=!0}}if(pe)return pe=m,re=re(pe),m=ie===""?"."+Ie(pe,0):ie,X(re)?(ee="",m!=null&&(ee=m.replace(ne,"$&/")+"/"),Pe(re,R,ee,"",function(et){return et})):re!=null&&(Le(re)&&(re=Ye(re,ee+(!re.key||pe&&pe.key===re.key?"":(""+re.key).replace(ne,"$&/")+"/")+m)),R.push(re)),1;if(pe=0,ie=ie===""?".":ie+":",X(m))for(var ce=0;ce<m.length;ce++){oe=m[ce];var xe=ie+Ie(oe,ce);pe+=Pe(oe,R,ee,xe,re)}else if(xe=q(m),typeof xe=="function")for(m=xe.call(m),ce=0;!(oe=m.next()).done;)oe=oe.value,xe=ie+Ie(oe,ce++),pe+=Pe(oe,R,ee,xe,re);else if(oe==="object")throw R=String(m),Error("Objects are not valid as a React child (found: "+(R==="[object Object]"?"object with keys {"+Object.keys(m).join(", ")+"}":R)+"). If you meant to render a collection of children, use an array instead.");return pe}function Qe(m,R,ee){if(m==null)return m;var ie=[],re=0;return Pe(m,ie,"","",function(oe){return R.call(ee,oe,re++)}),ie}function ue(m){if(m._status===-1){var R=m._result;R=R(),R.then(function(ee){(m._status===0||m._status===-1)&&(m._status=1,m._result=ee)},function(ee){(m._status===0||m._status===-1)&&(m._status=2,m._result=ee)}),m._status===-1&&(m._status=0,m._result=R)}if(m._status===1)return m._result.default;throw m._result}var ge={current:null},M={transition:null},H={ReactCurrentDispatcher:ge,ReactCurrentBatchConfig:M,ReactCurrentOwner:me};function U(){throw Error("act(...) is not supported in production builds of React.")}return te.Children={map:Qe,forEach:function(m,R,ee){Qe(m,function(){R.apply(this,arguments)},ee)},count:function(m){var R=0;return Qe(m,function(){R++}),R},toArray:function(m){return Qe(m,function(R){return R})||[]},only:function(m){if(!Le(m))throw Error("React.Children.only expected to receive a single React element child.");return m}},te.Component=C,te.Fragment=l,te.Profiler=h,te.PureComponent=Q,te.StrictMode=u,te.Suspense=k,te.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=H,te.act=U,te.cloneElement=function(m,R,ee){if(m==null)throw Error("React.cloneElement(...): The argument must be a React element, but you passed "+m+".");var ie=W({},m.props),re=m.key,oe=m.ref,pe=m._owner;if(R!=null){if(R.ref!==void 0&&(oe=R.ref,pe=me.current),R.key!==void 0&&(re=""+R.key),m.type&&m.type.defaultProps)var ce=m.type.defaultProps;for(xe in R)fe.call(R,xe)&&!Ae.hasOwnProperty(xe)&&(ie[xe]=R[xe]===void 0&&ce!==void 0?ce[xe]:R[xe])}var xe=arguments.length-2;if(xe===1)ie.children=ee;else if(1<xe){ce=Array(xe);for(var et=0;et<xe;et++)ce[et]=arguments[et+2];ie.children=ce}return{$$typeof:o,type:m.type,key:re,ref:oe,props:ie,_owner:pe}},te.createContext=function(m){return m={$$typeof:S,_currentValue:m,_currentValue2:m,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null},m.Provider={$$typeof:g,_context:m},m.Consumer=m},te.createElement=le,te.createFactory=function(m){var R=le.bind(null,m);return R.type=m,R},te.createRef=function(){return{current:null}},te.forwardRef=function(m){return{$$typeof:y,render:m}},te.isValidElement=Le,te.lazy=function(m){return{$$typeof:T,_payload:{_status:-1,_result:m},_init:ue}},te.memo=function(m,R){return{$$typeof:A,type:m,compare:R===void 0?null:R}},te.startTransition=function(m){var R=M.transition;M.transition={};try{m()}finally{M.transition=R}},te.unstable_act=U,te.useCallback=function(m,R){return ge.current.useCallback(m,R)},te.useContext=function(m){return ge.current.useContext(m)},te.useDebugValue=function(){},te.useDeferredValue=function(m){return ge.current.useDeferredValue(m)},te.useEffect=function(m,R){return ge.current.useEffect(m,R)},te.useId=function(){return ge.current.useId()},te.useImperativeHandle=function(m,R,ee){return ge.current.useImperativeHandle(m,R,ee)},te.useInsertionEffect=function(m,R){return ge.current.useInsertionEffect(m,R)},te.useLayoutEffect=function(m,R){return ge.current.useLayoutEffect(m,R)},te.useMemo=function(m,R){return ge.current.useMemo(m,R)},te.useReducer=function(m,R,ee){return ge.current.useReducer(m,R,ee)},te.useRef=function(m){return ge.current.useRef(m)},te.useState=function(m){return ge.current.useState(m)},te.useSyncExternalStore=function(m,R,ee){return ge.current.useSyncExternalStore(m,R,ee)},te.useTransition=function(){return ge.current.useTransition()},te.version="18.3.1",te}var dd;function ls(){return dd||(dd=1,Xo.exports=mh()),Xo.exports}/**
 * @license React
 * react-jsx-runtime.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var ud;function gh(){if(ud)return Pi;ud=1;var o=ls(),c=Symbol.for("react.element"),l=Symbol.for("react.fragment"),u=Object.prototype.hasOwnProperty,h=o.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,g={key:!0,ref:!0,__self:!0,__source:!0};function S(y,k,A){var T,E={},q=null,z=null;A!==void 0&&(q=""+A),k.key!==void 0&&(q=""+k.key),k.ref!==void 0&&(z=k.ref);for(T in k)u.call(k,T)&&!g.hasOwnProperty(T)&&(E[T]=k[T]);if(y&&y.defaultProps)for(T in k=y.defaultProps,k)E[T]===void 0&&(E[T]=k[T]);return{$$typeof:c,type:y,key:q,ref:z,props:E,_owner:h.current}}return Pi.Fragment=l,Pi.jsx=S,Pi.jsxs=S,Pi}var pd;function vh(){return pd||(pd=1,Ko.exports=gh()),Ko.exports}var b=vh(),D=ls();const an=fh(D),Td=hh({__proto__:null,default:an},[D]);var Ka={},Zo={exports:{}},Ze={},es={exports:{}},ts={};/**
 * @license React
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var hd;function wh(){return hd||(hd=1,function(o){function c(M,H){var U=M.length;M.push(H);e:for(;0<U;){var m=U-1>>>1,R=M[m];if(0<h(R,H))M[m]=H,M[U]=R,U=m;else break e}}function l(M){return M.length===0?null:M[0]}function u(M){if(M.length===0)return null;var H=M[0],U=M.pop();if(U!==H){M[0]=U;e:for(var m=0,R=M.length,ee=R>>>1;m<ee;){var ie=2*(m+1)-1,re=M[ie],oe=ie+1,pe=M[oe];if(0>h(re,U))oe<R&&0>h(pe,re)?(M[m]=pe,M[oe]=U,m=oe):(M[m]=re,M[ie]=U,m=ie);else if(oe<R&&0>h(pe,U))M[m]=pe,M[oe]=U,m=oe;else break e}}return H}function h(M,H){var U=M.sortIndex-H.sortIndex;return U!==0?U:M.id-H.id}if(typeof performance=="object"&&typeof performance.now=="function"){var g=performance;o.unstable_now=function(){return g.now()}}else{var S=Date,y=S.now();o.unstable_now=function(){return S.now()-y}}var k=[],A=[],T=1,E=null,q=3,z=!1,W=!1,P=!1,C=typeof setTimeout=="function"?setTimeout:null,Y=typeof clearTimeout=="function"?clearTimeout:null,Q=typeof setImmediate<"u"?setImmediate:null;typeof navigator<"u"&&navigator.scheduling!==void 0&&navigator.scheduling.isInputPending!==void 0&&navigator.scheduling.isInputPending.bind(navigator.scheduling);function K(M){for(var H=l(A);H!==null;){if(H.callback===null)u(A);else if(H.startTime<=M)u(A),H.sortIndex=H.expirationTime,c(k,H);else break;H=l(A)}}function X(M){if(P=!1,K(M),!W)if(l(k)!==null)W=!0,ue(fe);else{var H=l(A);H!==null&&ge(X,H.startTime-M)}}function fe(M,H){W=!1,P&&(P=!1,Y(le),le=-1),z=!0;var U=q;try{for(K(H),E=l(k);E!==null&&(!(E.expirationTime>H)||M&&!Fe());){var m=E.callback;if(typeof m=="function"){E.callback=null,q=E.priorityLevel;var R=m(E.expirationTime<=H);H=o.unstable_now(),typeof R=="function"?E.callback=R:E===l(k)&&u(k),K(H)}else u(k);E=l(k)}if(E!==null)var ee=!0;else{var ie=l(A);ie!==null&&ge(X,ie.startTime-H),ee=!1}return ee}finally{E=null,q=U,z=!1}}var me=!1,Ae=null,le=-1,Ye=5,Le=-1;function Fe(){return!(o.unstable_now()-Le<Ye)}function ne(){if(Ae!==null){var M=o.unstable_now();Le=M;var H=!0;try{H=Ae(!0,M)}finally{H?Ie():(me=!1,Ae=null)}}else me=!1}var Ie;if(typeof Q=="function")Ie=function(){Q(ne)};else if(typeof MessageChannel<"u"){var Pe=new MessageChannel,Qe=Pe.port2;Pe.port1.onmessage=ne,Ie=function(){Qe.postMessage(null)}}else Ie=function(){C(ne,0)};function ue(M){Ae=M,me||(me=!0,Ie())}function ge(M,H){le=C(function(){M(o.unstable_now())},H)}o.unstable_IdlePriority=5,o.unstable_ImmediatePriority=1,o.unstable_LowPriority=4,o.unstable_NormalPriority=3,o.unstable_Profiling=null,o.unstable_UserBlockingPriority=2,o.unstable_cancelCallback=function(M){M.callback=null},o.unstable_continueExecution=function(){W||z||(W=!0,ue(fe))},o.unstable_forceFrameRate=function(M){0>M||125<M?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):Ye=0<M?Math.floor(1e3/M):5},o.unstable_getCurrentPriorityLevel=function(){return q},o.unstable_getFirstCallbackNode=function(){return l(k)},o.unstable_next=function(M){switch(q){case 1:case 2:case 3:var H=3;break;default:H=q}var U=q;q=H;try{return M()}finally{q=U}},o.unstable_pauseExecution=function(){},o.unstable_requestPaint=function(){},o.unstable_runWithPriority=function(M,H){switch(M){case 1:case 2:case 3:case 4:case 5:break;default:M=3}var U=q;q=M;try{return H()}finally{q=U}},o.unstable_scheduleCallback=function(M,H,U){var m=o.unstable_now();switch(typeof U=="object"&&U!==null?(U=U.delay,U=typeof U=="number"&&0<U?m+U:m):U=m,M){case 1:var R=-1;break;case 2:R=250;break;case 5:R=1073741823;break;case 4:R=1e4;break;default:R=5e3}return R=U+R,M={id:T++,callback:H,priorityLevel:M,startTime:U,expirationTime:R,sortIndex:-1},U>m?(M.sortIndex=U,c(A,M),l(k)===null&&M===l(A)&&(P?(Y(le),le=-1):P=!0,ge(X,U-m))):(M.sortIndex=R,c(k,M),W||z||(W=!0,ue(fe))),M},o.unstable_shouldYield=Fe,o.unstable_wrapCallback=function(M){var H=q;return function(){var U=q;q=H;try{return M.apply(this,arguments)}finally{q=U}}}}(ts)),ts}var fd;function yh(){return fd||(fd=1,es.exports=wh()),es.exports}/**
 * @license React
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var md;function xh(){if(md)return Ze;md=1;var o=ls(),c=yh();function l(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var u=new Set,h={};function g(e,t){S(e,t),S(e+"Capture",t)}function S(e,t){for(h[e]=t,e=0;e<t.length;e++)u.add(t[e])}var y=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),k=Object.prototype.hasOwnProperty,A=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,T={},E={};function q(e){return k.call(E,e)?!0:k.call(T,e)?!1:A.test(e)?E[e]=!0:(T[e]=!0,!1)}function z(e,t,n,i){if(n!==null&&n.type===0)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return i?!1:n!==null?!n.acceptsBooleans:(e=e.toLowerCase().slice(0,5),e!=="data-"&&e!=="aria-");default:return!1}}function W(e,t,n,i){if(t===null||typeof t>"u"||z(e,t,n,i))return!0;if(i)return!1;if(n!==null)switch(n.type){case 3:return!t;case 4:return t===!1;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}function P(e,t,n,i,a,r,s){this.acceptsBooleans=t===2||t===3||t===4,this.attributeName=i,this.attributeNamespace=a,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=r,this.removeEmptyString=s}var C={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(e){C[e]=new P(e,0,!1,e,null,!1,!1)}),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(e){var t=e[0];C[t]=new P(t,1,!1,e[1],null,!1,!1)}),["contentEditable","draggable","spellCheck","value"].forEach(function(e){C[e]=new P(e,2,!1,e.toLowerCase(),null,!1,!1)}),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(e){C[e]=new P(e,2,!1,e,null,!1,!1)}),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(e){C[e]=new P(e,3,!1,e.toLowerCase(),null,!1,!1)}),["checked","multiple","muted","selected"].forEach(function(e){C[e]=new P(e,3,!0,e,null,!1,!1)}),["capture","download"].forEach(function(e){C[e]=new P(e,4,!1,e,null,!1,!1)}),["cols","rows","size","span"].forEach(function(e){C[e]=new P(e,6,!1,e,null,!1,!1)}),["rowSpan","start"].forEach(function(e){C[e]=new P(e,5,!1,e.toLowerCase(),null,!1,!1)});var Y=/[\-:]([a-z])/g;function Q(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(e){var t=e.replace(Y,Q);C[t]=new P(t,1,!1,e,null,!1,!1)}),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(e){var t=e.replace(Y,Q);C[t]=new P(t,1,!1,e,"http://www.w3.org/1999/xlink",!1,!1)}),["xml:base","xml:lang","xml:space"].forEach(function(e){var t=e.replace(Y,Q);C[t]=new P(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1,!1)}),["tabIndex","crossOrigin"].forEach(function(e){C[e]=new P(e,1,!1,e.toLowerCase(),null,!1,!1)}),C.xlinkHref=new P("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1),["src","href","action","formAction"].forEach(function(e){C[e]=new P(e,1,!1,e.toLowerCase(),null,!0,!0)});function K(e,t,n,i){var a=C.hasOwnProperty(t)?C[t]:null;(a!==null?a.type!==0:i||!(2<t.length)||t[0]!=="o"&&t[0]!=="O"||t[1]!=="n"&&t[1]!=="N")&&(W(t,n,a,i)&&(n=null),i||a===null?q(t)&&(n===null?e.removeAttribute(t):e.setAttribute(t,""+n)):a.mustUseProperty?e[a.propertyName]=n===null?a.type===3?!1:"":n:(t=a.attributeName,i=a.attributeNamespace,n===null?e.removeAttribute(t):(a=a.type,n=a===3||a===4&&n===!0?"":""+n,i?e.setAttributeNS(i,t,n):e.setAttribute(t,n))))}var X=o.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,fe=Symbol.for("react.element"),me=Symbol.for("react.portal"),Ae=Symbol.for("react.fragment"),le=Symbol.for("react.strict_mode"),Ye=Symbol.for("react.profiler"),Le=Symbol.for("react.provider"),Fe=Symbol.for("react.context"),ne=Symbol.for("react.forward_ref"),Ie=Symbol.for("react.suspense"),Pe=Symbol.for("react.suspense_list"),Qe=Symbol.for("react.memo"),ue=Symbol.for("react.lazy"),ge=Symbol.for("react.offscreen"),M=Symbol.iterator;function H(e){return e===null||typeof e!="object"?null:(e=M&&e[M]||e["@@iterator"],typeof e=="function"?e:null)}var U=Object.assign,m;function R(e){if(m===void 0)try{throw Error()}catch(n){var t=n.stack.trim().match(/\n( *(at )?)/);m=t&&t[1]||""}return`
`+m+e}var ee=!1;function ie(e,t){if(!e||ee)return"";ee=!0;var n=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(t,[])}catch(x){var i=x}Reflect.construct(e,[],t)}else{try{t.call()}catch(x){i=x}e.call(t.prototype)}else{try{throw Error()}catch(x){i=x}e()}}catch(x){if(x&&i&&typeof x.stack=="string"){for(var a=x.stack.split(`
`),r=i.stack.split(`
`),s=a.length-1,d=r.length-1;1<=s&&0<=d&&a[s]!==r[d];)d--;for(;1<=s&&0<=d;s--,d--)if(a[s]!==r[d]){if(s!==1||d!==1)do if(s--,d--,0>d||a[s]!==r[d]){var p=`
`+a[s].replace(" at new "," at ");return e.displayName&&p.includes("<anonymous>")&&(p=p.replace("<anonymous>",e.displayName)),p}while(1<=s&&0<=d);break}}}finally{ee=!1,Error.prepareStackTrace=n}return(e=e?e.displayName||e.name:"")?R(e):""}function re(e){switch(e.tag){case 5:return R(e.type);case 16:return R("Lazy");case 13:return R("Suspense");case 19:return R("SuspenseList");case 0:case 2:case 15:return e=ie(e.type,!1),e;case 11:return e=ie(e.type.render,!1),e;case 1:return e=ie(e.type,!0),e;default:return""}}function oe(e){if(e==null)return null;if(typeof e=="function")return e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case Ae:return"Fragment";case me:return"Portal";case Ye:return"Profiler";case le:return"StrictMode";case Ie:return"Suspense";case Pe:return"SuspenseList"}if(typeof e=="object")switch(e.$$typeof){case Fe:return(e.displayName||"Context")+".Consumer";case Le:return(e._context.displayName||"Context")+".Provider";case ne:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case Qe:return t=e.displayName||null,t!==null?t:oe(e.type)||"Memo";case ue:t=e._payload,e=e._init;try{return oe(e(t))}catch{}}return null}function pe(e){var t=e.type;switch(e.tag){case 24:return"Cache";case 9:return(t.displayName||"Context")+".Consumer";case 10:return(t._context.displayName||"Context")+".Provider";case 18:return"DehydratedFragment";case 11:return e=t.render,e=e.displayName||e.name||"",t.displayName||(e!==""?"ForwardRef("+e+")":"ForwardRef");case 7:return"Fragment";case 5:return t;case 4:return"Portal";case 3:return"Root";case 6:return"Text";case 16:return oe(t);case 8:return t===le?"StrictMode":"Mode";case 22:return"Offscreen";case 12:return"Profiler";case 21:return"Scope";case 13:return"Suspense";case 19:return"SuspenseList";case 25:return"TracingMarker";case 1:case 0:case 17:case 2:case 14:case 15:if(typeof t=="function")return t.displayName||t.name||null;if(typeof t=="string")return t}return null}function ce(e){switch(typeof e){case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function xe(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function et(e){var t=xe(e)?"checked":"value",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),i=""+e[t];if(!e.hasOwnProperty(t)&&typeof n<"u"&&typeof n.get=="function"&&typeof n.set=="function"){var a=n.get,r=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return a.call(this)},set:function(s){i=""+s,r.call(this,s)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return i},setValue:function(s){i=""+s},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function ji(e){e._valueTracker||(e._valueTracker=et(e))}function ms(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),i="";return e&&(i=xe(e)?e.checked?"true":"false":e.value),e=i,e!==n?(t.setValue(e),!0):!1}function Wi(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}function ir(e,t){var n=t.checked;return U({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:n??e._wrapperState.initialChecked})}function gs(e,t){var n=t.defaultValue==null?"":t.defaultValue,i=t.checked!=null?t.checked:t.defaultChecked;n=ce(t.value!=null?t.value:n),e._wrapperState={initialChecked:i,initialValue:n,controlled:t.type==="checkbox"||t.type==="radio"?t.checked!=null:t.value!=null}}function vs(e,t){t=t.checked,t!=null&&K(e,"checked",t,!1)}function ar(e,t){vs(e,t);var n=ce(t.value),i=t.type;if(n!=null)i==="number"?(n===0&&e.value===""||e.value!=n)&&(e.value=""+n):e.value!==""+n&&(e.value=""+n);else if(i==="submit"||i==="reset"){e.removeAttribute("value");return}t.hasOwnProperty("value")?rr(e,t.type,n):t.hasOwnProperty("defaultValue")&&rr(e,t.type,ce(t.defaultValue)),t.checked==null&&t.defaultChecked!=null&&(e.defaultChecked=!!t.defaultChecked)}function ws(e,t,n){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var i=t.type;if(!(i!=="submit"&&i!=="reset"||t.value!==void 0&&t.value!==null))return;t=""+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}n=e.name,n!==""&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,n!==""&&(e.name=n)}function rr(e,t,n){(t!=="number"||Wi(e.ownerDocument)!==e)&&(n==null?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+n&&(e.defaultValue=""+n))}var $n=Array.isArray;function yn(e,t,n,i){if(e=e.options,t){t={};for(var a=0;a<n.length;a++)t["$"+n[a]]=!0;for(n=0;n<e.length;n++)a=t.hasOwnProperty("$"+e[n].value),e[n].selected!==a&&(e[n].selected=a),a&&i&&(e[n].defaultSelected=!0)}else{for(n=""+ce(n),t=null,a=0;a<e.length;a++){if(e[a].value===n){e[a].selected=!0,i&&(e[a].defaultSelected=!0);return}t!==null||e[a].disabled||(t=e[a])}t!==null&&(t.selected=!0)}}function or(e,t){if(t.dangerouslySetInnerHTML!=null)throw Error(l(91));return U({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function ys(e,t){var n=t.value;if(n==null){if(n=t.children,t=t.defaultValue,n!=null){if(t!=null)throw Error(l(92));if($n(n)){if(1<n.length)throw Error(l(93));n=n[0]}t=n}t==null&&(t=""),n=t}e._wrapperState={initialValue:ce(n)}}function xs(e,t){var n=ce(t.value),i=ce(t.defaultValue);n!=null&&(n=""+n,n!==e.value&&(e.value=n),t.defaultValue==null&&e.defaultValue!==n&&(e.defaultValue=n)),i!=null&&(e.defaultValue=""+i)}function Ss(e){var t=e.textContent;t===e._wrapperState.initialValue&&t!==""&&t!==null&&(e.value=t)}function bs(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function sr(e,t){return e==null||e==="http://www.w3.org/1999/xhtml"?bs(t):e==="http://www.w3.org/2000/svg"&&t==="foreignObject"?"http://www.w3.org/1999/xhtml":e}var Fi,ks=function(e){return typeof MSApp<"u"&&MSApp.execUnsafeLocalFunction?function(t,n,i,a){MSApp.execUnsafeLocalFunction(function(){return e(t,n,i,a)})}:e}(function(e,t){if(e.namespaceURI!=="http://www.w3.org/2000/svg"||"innerHTML"in e)e.innerHTML=t;else{for(Fi=Fi||document.createElement("div"),Fi.innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=Fi.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}});function Kn(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&n.nodeType===3){n.nodeValue=t;return}}e.textContent=t}var Xn={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},vu=["Webkit","ms","Moz","O"];Object.keys(Xn).forEach(function(e){vu.forEach(function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Xn[t]=Xn[e]})});function As(e,t,n){return t==null||typeof t=="boolean"||t===""?"":n||typeof t!="number"||t===0||Xn.hasOwnProperty(e)&&Xn[e]?(""+t).trim():t+"px"}function Cs(e,t){e=e.style;for(var n in t)if(t.hasOwnProperty(n)){var i=n.indexOf("--")===0,a=As(n,t[n],i);n==="float"&&(n="cssFloat"),i?e.setProperty(n,a):e[n]=a}}var wu=U({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function lr(e,t){if(t){if(wu[e]&&(t.children!=null||t.dangerouslySetInnerHTML!=null))throw Error(l(137,e));if(t.dangerouslySetInnerHTML!=null){if(t.children!=null)throw Error(l(60));if(typeof t.dangerouslySetInnerHTML!="object"||!("__html"in t.dangerouslySetInnerHTML))throw Error(l(61))}if(t.style!=null&&typeof t.style!="object")throw Error(l(62))}}function cr(e,t){if(e.indexOf("-")===-1)return typeof t.is=="string";switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var dr=null;function ur(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var pr=null,xn=null,Sn=null;function Ts(e){if(e=xi(e)){if(typeof pr!="function")throw Error(l(280));var t=e.stateNode;t&&(t=pa(t),pr(e.stateNode,e.type,t))}}function Es(e){xn?Sn?Sn.push(e):Sn=[e]:xn=e}function Ds(){if(xn){var e=xn,t=Sn;if(Sn=xn=null,Ts(e),t)for(e=0;e<t.length;e++)Ts(t[e])}}function Ns(e,t){return e(t)}function Rs(){}var hr=!1;function Is(e,t,n){if(hr)return e(t,n);hr=!0;try{return Ns(e,t,n)}finally{hr=!1,(xn!==null||Sn!==null)&&(Rs(),Ds())}}function Zn(e,t){var n=e.stateNode;if(n===null)return null;var i=pa(n);if(i===null)return null;n=i[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(i=!i.disabled)||(e=e.type,i=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!i;break e;default:e=!1}if(e)return null;if(n&&typeof n!="function")throw Error(l(231,t,typeof n));return n}var fr=!1;if(y)try{var ei={};Object.defineProperty(ei,"passive",{get:function(){fr=!0}}),window.addEventListener("test",ei,ei),window.removeEventListener("test",ei,ei)}catch{fr=!1}function yu(e,t,n,i,a,r,s,d,p){var x=Array.prototype.slice.call(arguments,3);try{t.apply(n,x)}catch(I){this.onError(I)}}var ti=!1,Qi=null,Gi=!1,mr=null,xu={onError:function(e){ti=!0,Qi=e}};function Su(e,t,n,i,a,r,s,d,p){ti=!1,Qi=null,yu.apply(xu,arguments)}function bu(e,t,n,i,a,r,s,d,p){if(Su.apply(this,arguments),ti){if(ti){var x=Qi;ti=!1,Qi=null}else throw Error(l(198));Gi||(Gi=!0,mr=x)}}function rn(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,(t.flags&4098)!==0&&(n=t.return),e=t.return;while(e)}return t.tag===3?n:null}function Os(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function qs(e){if(rn(e)!==e)throw Error(l(188))}function ku(e){var t=e.alternate;if(!t){if(t=rn(e),t===null)throw Error(l(188));return t!==e?null:e}for(var n=e,i=t;;){var a=n.return;if(a===null)break;var r=a.alternate;if(r===null){if(i=a.return,i!==null){n=i;continue}break}if(a.child===r.child){for(r=a.child;r;){if(r===n)return qs(a),e;if(r===i)return qs(a),t;r=r.sibling}throw Error(l(188))}if(n.return!==i.return)n=a,i=r;else{for(var s=!1,d=a.child;d;){if(d===n){s=!0,n=a,i=r;break}if(d===i){s=!0,i=a,n=r;break}d=d.sibling}if(!s){for(d=r.child;d;){if(d===n){s=!0,n=r,i=a;break}if(d===i){s=!0,i=r,n=a;break}d=d.sibling}if(!s)throw Error(l(189))}}if(n.alternate!==i)throw Error(l(190))}if(n.tag!==3)throw Error(l(188));return n.stateNode.current===n?e:t}function _s(e){return e=ku(e),e!==null?Ps(e):null}function Ps(e){if(e.tag===5||e.tag===6)return e;for(e=e.child;e!==null;){var t=Ps(e);if(t!==null)return t;e=e.sibling}return null}var zs=c.unstable_scheduleCallback,Ms=c.unstable_cancelCallback,Au=c.unstable_shouldYield,Cu=c.unstable_requestPaint,Te=c.unstable_now,Tu=c.unstable_getCurrentPriorityLevel,gr=c.unstable_ImmediatePriority,Ls=c.unstable_UserBlockingPriority,Hi=c.unstable_NormalPriority,Eu=c.unstable_LowPriority,Us=c.unstable_IdlePriority,Yi=null,yt=null;function Du(e){if(yt&&typeof yt.onCommitFiberRoot=="function")try{yt.onCommitFiberRoot(Yi,e,void 0,(e.current.flags&128)===128)}catch{}}var ut=Math.clz32?Math.clz32:Iu,Nu=Math.log,Ru=Math.LN2;function Iu(e){return e>>>=0,e===0?32:31-(Nu(e)/Ru|0)|0}var Vi=64,Ji=4194304;function ni(e){switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return e&4194240;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return e&130023424;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return e}}function $i(e,t){var n=e.pendingLanes;if(n===0)return 0;var i=0,a=e.suspendedLanes,r=e.pingedLanes,s=n&268435455;if(s!==0){var d=s&~a;d!==0?i=ni(d):(r&=s,r!==0&&(i=ni(r)))}else s=n&~a,s!==0?i=ni(s):r!==0&&(i=ni(r));if(i===0)return 0;if(t!==0&&t!==i&&(t&a)===0&&(a=i&-i,r=t&-t,a>=r||a===16&&(r&4194240)!==0))return t;if((i&4)!==0&&(i|=n&16),t=e.entangledLanes,t!==0)for(e=e.entanglements,t&=i;0<t;)n=31-ut(t),a=1<<n,i|=e[n],t&=~a;return i}function Ou(e,t){switch(e){case 1:case 2:case 4:return t+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return-1;case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function qu(e,t){for(var n=e.suspendedLanes,i=e.pingedLanes,a=e.expirationTimes,r=e.pendingLanes;0<r;){var s=31-ut(r),d=1<<s,p=a[s];p===-1?((d&n)===0||(d&i)!==0)&&(a[s]=Ou(d,t)):p<=t&&(e.expiredLanes|=d),r&=~d}}function vr(e){return e=e.pendingLanes&-1073741825,e!==0?e:e&1073741824?1073741824:0}function Bs(){var e=Vi;return Vi<<=1,(Vi&4194240)===0&&(Vi=64),e}function wr(e){for(var t=[],n=0;31>n;n++)t.push(e);return t}function ii(e,t,n){e.pendingLanes|=t,t!==536870912&&(e.suspendedLanes=0,e.pingedLanes=0),e=e.eventTimes,t=31-ut(t),e[t]=n}function _u(e,t){var n=e.pendingLanes&~t;e.pendingLanes=t,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=t,e.mutableReadLanes&=t,e.entangledLanes&=t,t=e.entanglements;var i=e.eventTimes;for(e=e.expirationTimes;0<n;){var a=31-ut(n),r=1<<a;t[a]=0,i[a]=-1,e[a]=-1,n&=~r}}function yr(e,t){var n=e.entangledLanes|=t;for(e=e.entanglements;n;){var i=31-ut(n),a=1<<i;a&t|e[i]&t&&(e[i]|=t),n&=~a}}var de=0;function js(e){return e&=-e,1<e?4<e?(e&268435455)!==0?16:536870912:4:1}var Ws,xr,Fs,Qs,Gs,Sr=!1,Ki=[],zt=null,Mt=null,Lt=null,ai=new Map,ri=new Map,Ut=[],Pu="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function Hs(e,t){switch(e){case"focusin":case"focusout":zt=null;break;case"dragenter":case"dragleave":Mt=null;break;case"mouseover":case"mouseout":Lt=null;break;case"pointerover":case"pointerout":ai.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":ri.delete(t.pointerId)}}function oi(e,t,n,i,a,r){return e===null||e.nativeEvent!==r?(e={blockedOn:t,domEventName:n,eventSystemFlags:i,nativeEvent:r,targetContainers:[a]},t!==null&&(t=xi(t),t!==null&&xr(t)),e):(e.eventSystemFlags|=i,t=e.targetContainers,a!==null&&t.indexOf(a)===-1&&t.push(a),e)}function zu(e,t,n,i,a){switch(t){case"focusin":return zt=oi(zt,e,t,n,i,a),!0;case"dragenter":return Mt=oi(Mt,e,t,n,i,a),!0;case"mouseover":return Lt=oi(Lt,e,t,n,i,a),!0;case"pointerover":var r=a.pointerId;return ai.set(r,oi(ai.get(r)||null,e,t,n,i,a)),!0;case"gotpointercapture":return r=a.pointerId,ri.set(r,oi(ri.get(r)||null,e,t,n,i,a)),!0}return!1}function Ys(e){var t=on(e.target);if(t!==null){var n=rn(t);if(n!==null){if(t=n.tag,t===13){if(t=Os(n),t!==null){e.blockedOn=t,Gs(e.priority,function(){Fs(n)});return}}else if(t===3&&n.stateNode.current.memoizedState.isDehydrated){e.blockedOn=n.tag===3?n.stateNode.containerInfo:null;return}}}e.blockedOn=null}function Xi(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var n=kr(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(n===null){n=e.nativeEvent;var i=new n.constructor(n.type,n);dr=i,n.target.dispatchEvent(i),dr=null}else return t=xi(n),t!==null&&xr(t),e.blockedOn=n,!1;t.shift()}return!0}function Vs(e,t,n){Xi(e)&&n.delete(t)}function Mu(){Sr=!1,zt!==null&&Xi(zt)&&(zt=null),Mt!==null&&Xi(Mt)&&(Mt=null),Lt!==null&&Xi(Lt)&&(Lt=null),ai.forEach(Vs),ri.forEach(Vs)}function si(e,t){e.blockedOn===t&&(e.blockedOn=null,Sr||(Sr=!0,c.unstable_scheduleCallback(c.unstable_NormalPriority,Mu)))}function li(e){function t(a){return si(a,e)}if(0<Ki.length){si(Ki[0],e);for(var n=1;n<Ki.length;n++){var i=Ki[n];i.blockedOn===e&&(i.blockedOn=null)}}for(zt!==null&&si(zt,e),Mt!==null&&si(Mt,e),Lt!==null&&si(Lt,e),ai.forEach(t),ri.forEach(t),n=0;n<Ut.length;n++)i=Ut[n],i.blockedOn===e&&(i.blockedOn=null);for(;0<Ut.length&&(n=Ut[0],n.blockedOn===null);)Ys(n),n.blockedOn===null&&Ut.shift()}var bn=X.ReactCurrentBatchConfig,Zi=!0;function Lu(e,t,n,i){var a=de,r=bn.transition;bn.transition=null;try{de=1,br(e,t,n,i)}finally{de=a,bn.transition=r}}function Uu(e,t,n,i){var a=de,r=bn.transition;bn.transition=null;try{de=4,br(e,t,n,i)}finally{de=a,bn.transition=r}}function br(e,t,n,i){if(Zi){var a=kr(e,t,n,i);if(a===null)Br(e,t,i,ea,n),Hs(e,i);else if(zu(a,e,t,n,i))i.stopPropagation();else if(Hs(e,i),t&4&&-1<Pu.indexOf(e)){for(;a!==null;){var r=xi(a);if(r!==null&&Ws(r),r=kr(e,t,n,i),r===null&&Br(e,t,i,ea,n),r===a)break;a=r}a!==null&&i.stopPropagation()}else Br(e,t,i,null,n)}}var ea=null;function kr(e,t,n,i){if(ea=null,e=ur(i),e=on(e),e!==null)if(t=rn(e),t===null)e=null;else if(n=t.tag,n===13){if(e=Os(t),e!==null)return e;e=null}else if(n===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null);return ea=e,null}function Js(e){switch(e){case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 1;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"toggle":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 4;case"message":switch(Tu()){case gr:return 1;case Ls:return 4;case Hi:case Eu:return 16;case Us:return 536870912;default:return 16}default:return 16}}var Bt=null,Ar=null,ta=null;function $s(){if(ta)return ta;var e,t=Ar,n=t.length,i,a="value"in Bt?Bt.value:Bt.textContent,r=a.length;for(e=0;e<n&&t[e]===a[e];e++);var s=n-e;for(i=1;i<=s&&t[n-i]===a[r-i];i++);return ta=a.slice(e,1<i?1-i:void 0)}function na(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function ia(){return!0}function Ks(){return!1}function tt(e){function t(n,i,a,r,s){this._reactName=n,this._targetInst=a,this.type=i,this.nativeEvent=r,this.target=s,this.currentTarget=null;for(var d in e)e.hasOwnProperty(d)&&(n=e[d],this[d]=n?n(r):r[d]);return this.isDefaultPrevented=(r.defaultPrevented!=null?r.defaultPrevented:r.returnValue===!1)?ia:Ks,this.isPropagationStopped=Ks,this}return U(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var n=this.nativeEvent;n&&(n.preventDefault?n.preventDefault():typeof n.returnValue!="unknown"&&(n.returnValue=!1),this.isDefaultPrevented=ia)},stopPropagation:function(){var n=this.nativeEvent;n&&(n.stopPropagation?n.stopPropagation():typeof n.cancelBubble!="unknown"&&(n.cancelBubble=!0),this.isPropagationStopped=ia)},persist:function(){},isPersistent:ia}),t}var kn={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},Cr=tt(kn),ci=U({},kn,{view:0,detail:0}),Bu=tt(ci),Tr,Er,di,aa=U({},ci,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:Nr,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==di&&(di&&e.type==="mousemove"?(Tr=e.screenX-di.screenX,Er=e.screenY-di.screenY):Er=Tr=0,di=e),Tr)},movementY:function(e){return"movementY"in e?e.movementY:Er}}),Xs=tt(aa),ju=U({},aa,{dataTransfer:0}),Wu=tt(ju),Fu=U({},ci,{relatedTarget:0}),Dr=tt(Fu),Qu=U({},kn,{animationName:0,elapsedTime:0,pseudoElement:0}),Gu=tt(Qu),Hu=U({},kn,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),Yu=tt(Hu),Vu=U({},kn,{data:0}),Zs=tt(Vu),Ju={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},$u={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},Ku={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Xu(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=Ku[e])?!!t[e]:!1}function Nr(){return Xu}var Zu=U({},ci,{key:function(e){if(e.key){var t=Ju[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=na(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?$u[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:Nr,charCode:function(e){return e.type==="keypress"?na(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?na(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),ep=tt(Zu),tp=U({},aa,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),el=tt(tp),np=U({},ci,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:Nr}),ip=tt(np),ap=U({},kn,{propertyName:0,elapsedTime:0,pseudoElement:0}),rp=tt(ap),op=U({},aa,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),sp=tt(op),lp=[9,13,27,32],Rr=y&&"CompositionEvent"in window,ui=null;y&&"documentMode"in document&&(ui=document.documentMode);var cp=y&&"TextEvent"in window&&!ui,tl=y&&(!Rr||ui&&8<ui&&11>=ui),nl=" ",il=!1;function al(e,t){switch(e){case"keyup":return lp.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function rl(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var An=!1;function dp(e,t){switch(e){case"compositionend":return rl(t);case"keypress":return t.which!==32?null:(il=!0,nl);case"textInput":return e=t.data,e===nl&&il?null:e;default:return null}}function up(e,t){if(An)return e==="compositionend"||!Rr&&al(e,t)?(e=$s(),ta=Ar=Bt=null,An=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return tl&&t.locale!=="ko"?null:t.data;default:return null}}var pp={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function ol(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!pp[e.type]:t==="textarea"}function sl(e,t,n,i){Es(i),t=ca(t,"onChange"),0<t.length&&(n=new Cr("onChange","change",null,n,i),e.push({event:n,listeners:t}))}var pi=null,hi=null;function hp(e){Cl(e,0)}function ra(e){var t=Nn(e);if(ms(t))return e}function fp(e,t){if(e==="change")return t}var ll=!1;if(y){var Ir;if(y){var Or="oninput"in document;if(!Or){var cl=document.createElement("div");cl.setAttribute("oninput","return;"),Or=typeof cl.oninput=="function"}Ir=Or}else Ir=!1;ll=Ir&&(!document.documentMode||9<document.documentMode)}function dl(){pi&&(pi.detachEvent("onpropertychange",ul),hi=pi=null)}function ul(e){if(e.propertyName==="value"&&ra(hi)){var t=[];sl(t,hi,e,ur(e)),Is(hp,t)}}function mp(e,t,n){e==="focusin"?(dl(),pi=t,hi=n,pi.attachEvent("onpropertychange",ul)):e==="focusout"&&dl()}function gp(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return ra(hi)}function vp(e,t){if(e==="click")return ra(t)}function wp(e,t){if(e==="input"||e==="change")return ra(t)}function yp(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var pt=typeof Object.is=="function"?Object.is:yp;function fi(e,t){if(pt(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var n=Object.keys(e),i=Object.keys(t);if(n.length!==i.length)return!1;for(i=0;i<n.length;i++){var a=n[i];if(!k.call(t,a)||!pt(e[a],t[a]))return!1}return!0}function pl(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function hl(e,t){var n=pl(e);e=0;for(var i;n;){if(n.nodeType===3){if(i=e+n.textContent.length,e<=t&&i>=t)return{node:n,offset:t-e};e=i}e:{for(;n;){if(n.nextSibling){n=n.nextSibling;break e}n=n.parentNode}n=void 0}n=pl(n)}}function fl(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?fl(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function ml(){for(var e=window,t=Wi();t instanceof e.HTMLIFrameElement;){try{var n=typeof t.contentWindow.location.href=="string"}catch{n=!1}if(n)e=t.contentWindow;else break;t=Wi(e.document)}return t}function qr(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}function xp(e){var t=ml(),n=e.focusedElem,i=e.selectionRange;if(t!==n&&n&&n.ownerDocument&&fl(n.ownerDocument.documentElement,n)){if(i!==null&&qr(n)){if(t=i.start,e=i.end,e===void 0&&(e=t),"selectionStart"in n)n.selectionStart=t,n.selectionEnd=Math.min(e,n.value.length);else if(e=(t=n.ownerDocument||document)&&t.defaultView||window,e.getSelection){e=e.getSelection();var a=n.textContent.length,r=Math.min(i.start,a);i=i.end===void 0?r:Math.min(i.end,a),!e.extend&&r>i&&(a=i,i=r,r=a),a=hl(n,r);var s=hl(n,i);a&&s&&(e.rangeCount!==1||e.anchorNode!==a.node||e.anchorOffset!==a.offset||e.focusNode!==s.node||e.focusOffset!==s.offset)&&(t=t.createRange(),t.setStart(a.node,a.offset),e.removeAllRanges(),r>i?(e.addRange(t),e.extend(s.node,s.offset)):(t.setEnd(s.node,s.offset),e.addRange(t)))}}for(t=[],e=n;e=e.parentNode;)e.nodeType===1&&t.push({element:e,left:e.scrollLeft,top:e.scrollTop});for(typeof n.focus=="function"&&n.focus(),n=0;n<t.length;n++)e=t[n],e.element.scrollLeft=e.left,e.element.scrollTop=e.top}}var Sp=y&&"documentMode"in document&&11>=document.documentMode,Cn=null,_r=null,mi=null,Pr=!1;function gl(e,t,n){var i=n.window===n?n.document:n.nodeType===9?n:n.ownerDocument;Pr||Cn==null||Cn!==Wi(i)||(i=Cn,"selectionStart"in i&&qr(i)?i={start:i.selectionStart,end:i.selectionEnd}:(i=(i.ownerDocument&&i.ownerDocument.defaultView||window).getSelection(),i={anchorNode:i.anchorNode,anchorOffset:i.anchorOffset,focusNode:i.focusNode,focusOffset:i.focusOffset}),mi&&fi(mi,i)||(mi=i,i=ca(_r,"onSelect"),0<i.length&&(t=new Cr("onSelect","select",null,t,n),e.push({event:t,listeners:i}),t.target=Cn)))}function oa(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var Tn={animationend:oa("Animation","AnimationEnd"),animationiteration:oa("Animation","AnimationIteration"),animationstart:oa("Animation","AnimationStart"),transitionend:oa("Transition","TransitionEnd")},zr={},vl={};y&&(vl=document.createElement("div").style,"AnimationEvent"in window||(delete Tn.animationend.animation,delete Tn.animationiteration.animation,delete Tn.animationstart.animation),"TransitionEvent"in window||delete Tn.transitionend.transition);function sa(e){if(zr[e])return zr[e];if(!Tn[e])return e;var t=Tn[e],n;for(n in t)if(t.hasOwnProperty(n)&&n in vl)return zr[e]=t[n];return e}var wl=sa("animationend"),yl=sa("animationiteration"),xl=sa("animationstart"),Sl=sa("transitionend"),bl=new Map,kl="abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");function jt(e,t){bl.set(e,t),g(t,[e])}for(var Mr=0;Mr<kl.length;Mr++){var Lr=kl[Mr],bp=Lr.toLowerCase(),kp=Lr[0].toUpperCase()+Lr.slice(1);jt(bp,"on"+kp)}jt(wl,"onAnimationEnd"),jt(yl,"onAnimationIteration"),jt(xl,"onAnimationStart"),jt("dblclick","onDoubleClick"),jt("focusin","onFocus"),jt("focusout","onBlur"),jt(Sl,"onTransitionEnd"),S("onMouseEnter",["mouseout","mouseover"]),S("onMouseLeave",["mouseout","mouseover"]),S("onPointerEnter",["pointerout","pointerover"]),S("onPointerLeave",["pointerout","pointerover"]),g("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),g("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),g("onBeforeInput",["compositionend","keypress","textInput","paste"]),g("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),g("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),g("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var gi="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Ap=new Set("cancel close invalid load scroll toggle".split(" ").concat(gi));function Al(e,t,n){var i=e.type||"unknown-event";e.currentTarget=n,bu(i,t,void 0,e),e.currentTarget=null}function Cl(e,t){t=(t&4)!==0;for(var n=0;n<e.length;n++){var i=e[n],a=i.event;i=i.listeners;e:{var r=void 0;if(t)for(var s=i.length-1;0<=s;s--){var d=i[s],p=d.instance,x=d.currentTarget;if(d=d.listener,p!==r&&a.isPropagationStopped())break e;Al(a,d,x),r=p}else for(s=0;s<i.length;s++){if(d=i[s],p=d.instance,x=d.currentTarget,d=d.listener,p!==r&&a.isPropagationStopped())break e;Al(a,d,x),r=p}}}if(Gi)throw e=mr,Gi=!1,mr=null,e}function ve(e,t){var n=t[Hr];n===void 0&&(n=t[Hr]=new Set);var i=e+"__bubble";n.has(i)||(Tl(t,e,2,!1),n.add(i))}function Ur(e,t,n){var i=0;t&&(i|=4),Tl(n,e,i,t)}var la="_reactListening"+Math.random().toString(36).slice(2);function vi(e){if(!e[la]){e[la]=!0,u.forEach(function(n){n!=="selectionchange"&&(Ap.has(n)||Ur(n,!1,e),Ur(n,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[la]||(t[la]=!0,Ur("selectionchange",!1,t))}}function Tl(e,t,n,i){switch(Js(t)){case 1:var a=Lu;break;case 4:a=Uu;break;default:a=br}n=a.bind(null,t,n,e),a=void 0,!fr||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(a=!0),i?a!==void 0?e.addEventListener(t,n,{capture:!0,passive:a}):e.addEventListener(t,n,!0):a!==void 0?e.addEventListener(t,n,{passive:a}):e.addEventListener(t,n,!1)}function Br(e,t,n,i,a){var r=i;if((t&1)===0&&(t&2)===0&&i!==null)e:for(;;){if(i===null)return;var s=i.tag;if(s===3||s===4){var d=i.stateNode.containerInfo;if(d===a||d.nodeType===8&&d.parentNode===a)break;if(s===4)for(s=i.return;s!==null;){var p=s.tag;if((p===3||p===4)&&(p=s.stateNode.containerInfo,p===a||p.nodeType===8&&p.parentNode===a))return;s=s.return}for(;d!==null;){if(s=on(d),s===null)return;if(p=s.tag,p===5||p===6){i=r=s;continue e}d=d.parentNode}}i=i.return}Is(function(){var x=r,I=ur(n),O=[];e:{var N=bl.get(e);if(N!==void 0){var L=Cr,j=e;switch(e){case"keypress":if(na(n)===0)break e;case"keydown":case"keyup":L=ep;break;case"focusin":j="focus",L=Dr;break;case"focusout":j="blur",L=Dr;break;case"beforeblur":case"afterblur":L=Dr;break;case"click":if(n.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":L=Xs;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":L=Wu;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":L=ip;break;case wl:case yl:case xl:L=Gu;break;case Sl:L=rp;break;case"scroll":L=Bu;break;case"wheel":L=sp;break;case"copy":case"cut":case"paste":L=Yu;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":L=el}var F=(t&4)!==0,Ee=!F&&e==="scroll",v=F?N!==null?N+"Capture":null:N;F=[];for(var f=x,w;f!==null;){w=f;var _=w.stateNode;if(w.tag===5&&_!==null&&(w=_,v!==null&&(_=Zn(f,v),_!=null&&F.push(wi(f,_,w)))),Ee)break;f=f.return}0<F.length&&(N=new L(N,j,null,n,I),O.push({event:N,listeners:F}))}}if((t&7)===0){e:{if(N=e==="mouseover"||e==="pointerover",L=e==="mouseout"||e==="pointerout",N&&n!==dr&&(j=n.relatedTarget||n.fromElement)&&(on(j)||j[At]))break e;if((L||N)&&(N=I.window===I?I:(N=I.ownerDocument)?N.defaultView||N.parentWindow:window,L?(j=n.relatedTarget||n.toElement,L=x,j=j?on(j):null,j!==null&&(Ee=rn(j),j!==Ee||j.tag!==5&&j.tag!==6)&&(j=null)):(L=null,j=x),L!==j)){if(F=Xs,_="onMouseLeave",v="onMouseEnter",f="mouse",(e==="pointerout"||e==="pointerover")&&(F=el,_="onPointerLeave",v="onPointerEnter",f="pointer"),Ee=L==null?N:Nn(L),w=j==null?N:Nn(j),N=new F(_,f+"leave",L,n,I),N.target=Ee,N.relatedTarget=w,_=null,on(I)===x&&(F=new F(v,f+"enter",j,n,I),F.target=w,F.relatedTarget=Ee,_=F),Ee=_,L&&j)t:{for(F=L,v=j,f=0,w=F;w;w=En(w))f++;for(w=0,_=v;_;_=En(_))w++;for(;0<f-w;)F=En(F),f--;for(;0<w-f;)v=En(v),w--;for(;f--;){if(F===v||v!==null&&F===v.alternate)break t;F=En(F),v=En(v)}F=null}else F=null;L!==null&&El(O,N,L,F,!1),j!==null&&Ee!==null&&El(O,Ee,j,F,!0)}}e:{if(N=x?Nn(x):window,L=N.nodeName&&N.nodeName.toLowerCase(),L==="select"||L==="input"&&N.type==="file")var G=fp;else if(ol(N))if(ll)G=wp;else{G=gp;var V=mp}else(L=N.nodeName)&&L.toLowerCase()==="input"&&(N.type==="checkbox"||N.type==="radio")&&(G=vp);if(G&&(G=G(e,x))){sl(O,G,n,I);break e}V&&V(e,N,x),e==="focusout"&&(V=N._wrapperState)&&V.controlled&&N.type==="number"&&rr(N,"number",N.value)}switch(V=x?Nn(x):window,e){case"focusin":(ol(V)||V.contentEditable==="true")&&(Cn=V,_r=x,mi=null);break;case"focusout":mi=_r=Cn=null;break;case"mousedown":Pr=!0;break;case"contextmenu":case"mouseup":case"dragend":Pr=!1,gl(O,n,I);break;case"selectionchange":if(Sp)break;case"keydown":case"keyup":gl(O,n,I)}var J;if(Rr)e:{switch(e){case"compositionstart":var $="onCompositionStart";break e;case"compositionend":$="onCompositionEnd";break e;case"compositionupdate":$="onCompositionUpdate";break e}$=void 0}else An?al(e,n)&&($="onCompositionEnd"):e==="keydown"&&n.keyCode===229&&($="onCompositionStart");$&&(tl&&n.locale!=="ko"&&(An||$!=="onCompositionStart"?$==="onCompositionEnd"&&An&&(J=$s()):(Bt=I,Ar="value"in Bt?Bt.value:Bt.textContent,An=!0)),V=ca(x,$),0<V.length&&($=new Zs($,e,null,n,I),O.push({event:$,listeners:V}),J?$.data=J:(J=rl(n),J!==null&&($.data=J)))),(J=cp?dp(e,n):up(e,n))&&(x=ca(x,"onBeforeInput"),0<x.length&&(I=new Zs("onBeforeInput","beforeinput",null,n,I),O.push({event:I,listeners:x}),I.data=J))}Cl(O,t)})}function wi(e,t,n){return{instance:e,listener:t,currentTarget:n}}function ca(e,t){for(var n=t+"Capture",i=[];e!==null;){var a=e,r=a.stateNode;a.tag===5&&r!==null&&(a=r,r=Zn(e,n),r!=null&&i.unshift(wi(e,r,a)),r=Zn(e,t),r!=null&&i.push(wi(e,r,a))),e=e.return}return i}function En(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5);return e||null}function El(e,t,n,i,a){for(var r=t._reactName,s=[];n!==null&&n!==i;){var d=n,p=d.alternate,x=d.stateNode;if(p!==null&&p===i)break;d.tag===5&&x!==null&&(d=x,a?(p=Zn(n,r),p!=null&&s.unshift(wi(n,p,d))):a||(p=Zn(n,r),p!=null&&s.push(wi(n,p,d)))),n=n.return}s.length!==0&&e.push({event:t,listeners:s})}var Cp=/\r\n?/g,Tp=/\u0000|\uFFFD/g;function Dl(e){return(typeof e=="string"?e:""+e).replace(Cp,`
`).replace(Tp,"")}function da(e,t,n){if(t=Dl(t),Dl(e)!==t&&n)throw Error(l(425))}function ua(){}var jr=null,Wr=null;function Fr(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var Qr=typeof setTimeout=="function"?setTimeout:void 0,Ep=typeof clearTimeout=="function"?clearTimeout:void 0,Nl=typeof Promise=="function"?Promise:void 0,Dp=typeof queueMicrotask=="function"?queueMicrotask:typeof Nl<"u"?function(e){return Nl.resolve(null).then(e).catch(Np)}:Qr;function Np(e){setTimeout(function(){throw e})}function Gr(e,t){var n=t,i=0;do{var a=n.nextSibling;if(e.removeChild(n),a&&a.nodeType===8)if(n=a.data,n==="/$"){if(i===0){e.removeChild(a),li(t);return}i--}else n!=="$"&&n!=="$?"&&n!=="$!"||i++;n=a}while(n);li(t)}function Wt(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?")break;if(t==="/$")return null}}return e}function Rl(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var n=e.data;if(n==="$"||n==="$!"||n==="$?"){if(t===0)return e;t--}else n==="/$"&&t++}e=e.previousSibling}return null}var Dn=Math.random().toString(36).slice(2),xt="__reactFiber$"+Dn,yi="__reactProps$"+Dn,At="__reactContainer$"+Dn,Hr="__reactEvents$"+Dn,Rp="__reactListeners$"+Dn,Ip="__reactHandles$"+Dn;function on(e){var t=e[xt];if(t)return t;for(var n=e.parentNode;n;){if(t=n[At]||n[xt]){if(n=t.alternate,t.child!==null||n!==null&&n.child!==null)for(e=Rl(e);e!==null;){if(n=e[xt])return n;e=Rl(e)}return t}e=n,n=e.parentNode}return null}function xi(e){return e=e[xt]||e[At],!e||e.tag!==5&&e.tag!==6&&e.tag!==13&&e.tag!==3?null:e}function Nn(e){if(e.tag===5||e.tag===6)return e.stateNode;throw Error(l(33))}function pa(e){return e[yi]||null}var Yr=[],Rn=-1;function Ft(e){return{current:e}}function we(e){0>Rn||(e.current=Yr[Rn],Yr[Rn]=null,Rn--)}function he(e,t){Rn++,Yr[Rn]=e.current,e.current=t}var Qt={},Ue=Ft(Qt),Ve=Ft(!1),sn=Qt;function In(e,t){var n=e.type.contextTypes;if(!n)return Qt;var i=e.stateNode;if(i&&i.__reactInternalMemoizedUnmaskedChildContext===t)return i.__reactInternalMemoizedMaskedChildContext;var a={},r;for(r in n)a[r]=t[r];return i&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=a),a}function Je(e){return e=e.childContextTypes,e!=null}function ha(){we(Ve),we(Ue)}function Il(e,t,n){if(Ue.current!==Qt)throw Error(l(168));he(Ue,t),he(Ve,n)}function Ol(e,t,n){var i=e.stateNode;if(t=t.childContextTypes,typeof i.getChildContext!="function")return n;i=i.getChildContext();for(var a in i)if(!(a in t))throw Error(l(108,pe(e)||"Unknown",a));return U({},n,i)}function fa(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||Qt,sn=Ue.current,he(Ue,e),he(Ve,Ve.current),!0}function ql(e,t,n){var i=e.stateNode;if(!i)throw Error(l(169));n?(e=Ol(e,t,sn),i.__reactInternalMemoizedMergedChildContext=e,we(Ve),we(Ue),he(Ue,e)):we(Ve),he(Ve,n)}var Ct=null,ma=!1,Vr=!1;function _l(e){Ct===null?Ct=[e]:Ct.push(e)}function Op(e){ma=!0,_l(e)}function Gt(){if(!Vr&&Ct!==null){Vr=!0;var e=0,t=de;try{var n=Ct;for(de=1;e<n.length;e++){var i=n[e];do i=i(!0);while(i!==null)}Ct=null,ma=!1}catch(a){throw Ct!==null&&(Ct=Ct.slice(e+1)),zs(gr,Gt),a}finally{de=t,Vr=!1}}return null}var On=[],qn=0,ga=null,va=0,rt=[],ot=0,ln=null,Tt=1,Et="";function cn(e,t){On[qn++]=va,On[qn++]=ga,ga=e,va=t}function Pl(e,t,n){rt[ot++]=Tt,rt[ot++]=Et,rt[ot++]=ln,ln=e;var i=Tt;e=Et;var a=32-ut(i)-1;i&=~(1<<a),n+=1;var r=32-ut(t)+a;if(30<r){var s=a-a%5;r=(i&(1<<s)-1).toString(32),i>>=s,a-=s,Tt=1<<32-ut(t)+a|n<<a|i,Et=r+e}else Tt=1<<r|n<<a|i,Et=e}function Jr(e){e.return!==null&&(cn(e,1),Pl(e,1,0))}function $r(e){for(;e===ga;)ga=On[--qn],On[qn]=null,va=On[--qn],On[qn]=null;for(;e===ln;)ln=rt[--ot],rt[ot]=null,Et=rt[--ot],rt[ot]=null,Tt=rt[--ot],rt[ot]=null}var nt=null,it=null,Se=!1,ht=null;function zl(e,t){var n=dt(5,null,null,0);n.elementType="DELETED",n.stateNode=t,n.return=e,t=e.deletions,t===null?(e.deletions=[n],e.flags|=16):t.push(n)}function Ml(e,t){switch(e.tag){case 5:var n=e.type;return t=t.nodeType!==1||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t,t!==null?(e.stateNode=t,nt=e,it=Wt(t.firstChild),!0):!1;case 6:return t=e.pendingProps===""||t.nodeType!==3?null:t,t!==null?(e.stateNode=t,nt=e,it=null,!0):!1;case 13:return t=t.nodeType!==8?null:t,t!==null?(n=ln!==null?{id:Tt,overflow:Et}:null,e.memoizedState={dehydrated:t,treeContext:n,retryLane:1073741824},n=dt(18,null,null,0),n.stateNode=t,n.return=e,e.child=n,nt=e,it=null,!0):!1;default:return!1}}function Kr(e){return(e.mode&1)!==0&&(e.flags&128)===0}function Xr(e){if(Se){var t=it;if(t){var n=t;if(!Ml(e,t)){if(Kr(e))throw Error(l(418));t=Wt(n.nextSibling);var i=nt;t&&Ml(e,t)?zl(i,n):(e.flags=e.flags&-4097|2,Se=!1,nt=e)}}else{if(Kr(e))throw Error(l(418));e.flags=e.flags&-4097|2,Se=!1,nt=e}}}function Ll(e){for(e=e.return;e!==null&&e.tag!==5&&e.tag!==3&&e.tag!==13;)e=e.return;nt=e}function wa(e){if(e!==nt)return!1;if(!Se)return Ll(e),Se=!0,!1;var t;if((t=e.tag!==3)&&!(t=e.tag!==5)&&(t=e.type,t=t!=="head"&&t!=="body"&&!Fr(e.type,e.memoizedProps)),t&&(t=it)){if(Kr(e))throw Ul(),Error(l(418));for(;t;)zl(e,t),t=Wt(t.nextSibling)}if(Ll(e),e.tag===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(l(317));e:{for(e=e.nextSibling,t=0;e;){if(e.nodeType===8){var n=e.data;if(n==="/$"){if(t===0){it=Wt(e.nextSibling);break e}t--}else n!=="$"&&n!=="$!"&&n!=="$?"||t++}e=e.nextSibling}it=null}}else it=nt?Wt(e.stateNode.nextSibling):null;return!0}function Ul(){for(var e=it;e;)e=Wt(e.nextSibling)}function _n(){it=nt=null,Se=!1}function Zr(e){ht===null?ht=[e]:ht.push(e)}var qp=X.ReactCurrentBatchConfig;function Si(e,t,n){if(e=n.ref,e!==null&&typeof e!="function"&&typeof e!="object"){if(n._owner){if(n=n._owner,n){if(n.tag!==1)throw Error(l(309));var i=n.stateNode}if(!i)throw Error(l(147,e));var a=i,r=""+e;return t!==null&&t.ref!==null&&typeof t.ref=="function"&&t.ref._stringRef===r?t.ref:(t=function(s){var d=a.refs;s===null?delete d[r]:d[r]=s},t._stringRef=r,t)}if(typeof e!="string")throw Error(l(284));if(!n._owner)throw Error(l(290,e))}return e}function ya(e,t){throw e=Object.prototype.toString.call(t),Error(l(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e))}function Bl(e){var t=e._init;return t(e._payload)}function jl(e){function t(v,f){if(e){var w=v.deletions;w===null?(v.deletions=[f],v.flags|=16):w.push(f)}}function n(v,f){if(!e)return null;for(;f!==null;)t(v,f),f=f.sibling;return null}function i(v,f){for(v=new Map;f!==null;)f.key!==null?v.set(f.key,f):v.set(f.index,f),f=f.sibling;return v}function a(v,f){return v=Zt(v,f),v.index=0,v.sibling=null,v}function r(v,f,w){return v.index=w,e?(w=v.alternate,w!==null?(w=w.index,w<f?(v.flags|=2,f):w):(v.flags|=2,f)):(v.flags|=1048576,f)}function s(v){return e&&v.alternate===null&&(v.flags|=2),v}function d(v,f,w,_){return f===null||f.tag!==6?(f=Go(w,v.mode,_),f.return=v,f):(f=a(f,w),f.return=v,f)}function p(v,f,w,_){var G=w.type;return G===Ae?I(v,f,w.props.children,_,w.key):f!==null&&(f.elementType===G||typeof G=="object"&&G!==null&&G.$$typeof===ue&&Bl(G)===f.type)?(_=a(f,w.props),_.ref=Si(v,f,w),_.return=v,_):(_=Fa(w.type,w.key,w.props,null,v.mode,_),_.ref=Si(v,f,w),_.return=v,_)}function x(v,f,w,_){return f===null||f.tag!==4||f.stateNode.containerInfo!==w.containerInfo||f.stateNode.implementation!==w.implementation?(f=Ho(w,v.mode,_),f.return=v,f):(f=a(f,w.children||[]),f.return=v,f)}function I(v,f,w,_,G){return f===null||f.tag!==7?(f=vn(w,v.mode,_,G),f.return=v,f):(f=a(f,w),f.return=v,f)}function O(v,f,w){if(typeof f=="string"&&f!==""||typeof f=="number")return f=Go(""+f,v.mode,w),f.return=v,f;if(typeof f=="object"&&f!==null){switch(f.$$typeof){case fe:return w=Fa(f.type,f.key,f.props,null,v.mode,w),w.ref=Si(v,null,f),w.return=v,w;case me:return f=Ho(f,v.mode,w),f.return=v,f;case ue:var _=f._init;return O(v,_(f._payload),w)}if($n(f)||H(f))return f=vn(f,v.mode,w,null),f.return=v,f;ya(v,f)}return null}function N(v,f,w,_){var G=f!==null?f.key:null;if(typeof w=="string"&&w!==""||typeof w=="number")return G!==null?null:d(v,f,""+w,_);if(typeof w=="object"&&w!==null){switch(w.$$typeof){case fe:return w.key===G?p(v,f,w,_):null;case me:return w.key===G?x(v,f,w,_):null;case ue:return G=w._init,N(v,f,G(w._payload),_)}if($n(w)||H(w))return G!==null?null:I(v,f,w,_,null);ya(v,w)}return null}function L(v,f,w,_,G){if(typeof _=="string"&&_!==""||typeof _=="number")return v=v.get(w)||null,d(f,v,""+_,G);if(typeof _=="object"&&_!==null){switch(_.$$typeof){case fe:return v=v.get(_.key===null?w:_.key)||null,p(f,v,_,G);case me:return v=v.get(_.key===null?w:_.key)||null,x(f,v,_,G);case ue:var V=_._init;return L(v,f,w,V(_._payload),G)}if($n(_)||H(_))return v=v.get(w)||null,I(f,v,_,G,null);ya(f,_)}return null}function j(v,f,w,_){for(var G=null,V=null,J=f,$=f=0,_e=null;J!==null&&$<w.length;$++){J.index>$?(_e=J,J=null):_e=J.sibling;var se=N(v,J,w[$],_);if(se===null){J===null&&(J=_e);break}e&&J&&se.alternate===null&&t(v,J),f=r(se,f,$),V===null?G=se:V.sibling=se,V=se,J=_e}if($===w.length)return n(v,J),Se&&cn(v,$),G;if(J===null){for(;$<w.length;$++)J=O(v,w[$],_),J!==null&&(f=r(J,f,$),V===null?G=J:V.sibling=J,V=J);return Se&&cn(v,$),G}for(J=i(v,J);$<w.length;$++)_e=L(J,v,$,w[$],_),_e!==null&&(e&&_e.alternate!==null&&J.delete(_e.key===null?$:_e.key),f=r(_e,f,$),V===null?G=_e:V.sibling=_e,V=_e);return e&&J.forEach(function(en){return t(v,en)}),Se&&cn(v,$),G}function F(v,f,w,_){var G=H(w);if(typeof G!="function")throw Error(l(150));if(w=G.call(w),w==null)throw Error(l(151));for(var V=G=null,J=f,$=f=0,_e=null,se=w.next();J!==null&&!se.done;$++,se=w.next()){J.index>$?(_e=J,J=null):_e=J.sibling;var en=N(v,J,se.value,_);if(en===null){J===null&&(J=_e);break}e&&J&&en.alternate===null&&t(v,J),f=r(en,f,$),V===null?G=en:V.sibling=en,V=en,J=_e}if(se.done)return n(v,J),Se&&cn(v,$),G;if(J===null){for(;!se.done;$++,se=w.next())se=O(v,se.value,_),se!==null&&(f=r(se,f,$),V===null?G=se:V.sibling=se,V=se);return Se&&cn(v,$),G}for(J=i(v,J);!se.done;$++,se=w.next())se=L(J,v,$,se.value,_),se!==null&&(e&&se.alternate!==null&&J.delete(se.key===null?$:se.key),f=r(se,f,$),V===null?G=se:V.sibling=se,V=se);return e&&J.forEach(function(ph){return t(v,ph)}),Se&&cn(v,$),G}function Ee(v,f,w,_){if(typeof w=="object"&&w!==null&&w.type===Ae&&w.key===null&&(w=w.props.children),typeof w=="object"&&w!==null){switch(w.$$typeof){case fe:e:{for(var G=w.key,V=f;V!==null;){if(V.key===G){if(G=w.type,G===Ae){if(V.tag===7){n(v,V.sibling),f=a(V,w.props.children),f.return=v,v=f;break e}}else if(V.elementType===G||typeof G=="object"&&G!==null&&G.$$typeof===ue&&Bl(G)===V.type){n(v,V.sibling),f=a(V,w.props),f.ref=Si(v,V,w),f.return=v,v=f;break e}n(v,V);break}else t(v,V);V=V.sibling}w.type===Ae?(f=vn(w.props.children,v.mode,_,w.key),f.return=v,v=f):(_=Fa(w.type,w.key,w.props,null,v.mode,_),_.ref=Si(v,f,w),_.return=v,v=_)}return s(v);case me:e:{for(V=w.key;f!==null;){if(f.key===V)if(f.tag===4&&f.stateNode.containerInfo===w.containerInfo&&f.stateNode.implementation===w.implementation){n(v,f.sibling),f=a(f,w.children||[]),f.return=v,v=f;break e}else{n(v,f);break}else t(v,f);f=f.sibling}f=Ho(w,v.mode,_),f.return=v,v=f}return s(v);case ue:return V=w._init,Ee(v,f,V(w._payload),_)}if($n(w))return j(v,f,w,_);if(H(w))return F(v,f,w,_);ya(v,w)}return typeof w=="string"&&w!==""||typeof w=="number"?(w=""+w,f!==null&&f.tag===6?(n(v,f.sibling),f=a(f,w),f.return=v,v=f):(n(v,f),f=Go(w,v.mode,_),f.return=v,v=f),s(v)):n(v,f)}return Ee}var Pn=jl(!0),Wl=jl(!1),xa=Ft(null),Sa=null,zn=null,eo=null;function to(){eo=zn=Sa=null}function no(e){var t=xa.current;we(xa),e._currentValue=t}function io(e,t,n){for(;e!==null;){var i=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,i!==null&&(i.childLanes|=t)):i!==null&&(i.childLanes&t)!==t&&(i.childLanes|=t),e===n)break;e=e.return}}function Mn(e,t){Sa=e,eo=zn=null,e=e.dependencies,e!==null&&e.firstContext!==null&&((e.lanes&t)!==0&&($e=!0),e.firstContext=null)}function st(e){var t=e._currentValue;if(eo!==e)if(e={context:e,memoizedValue:t,next:null},zn===null){if(Sa===null)throw Error(l(308));zn=e,Sa.dependencies={lanes:0,firstContext:e}}else zn=zn.next=e;return t}var dn=null;function ao(e){dn===null?dn=[e]:dn.push(e)}function Fl(e,t,n,i){var a=t.interleaved;return a===null?(n.next=n,ao(t)):(n.next=a.next,a.next=n),t.interleaved=n,Dt(e,i)}function Dt(e,t){e.lanes|=t;var n=e.alternate;for(n!==null&&(n.lanes|=t),n=e,e=e.return;e!==null;)e.childLanes|=t,n=e.alternate,n!==null&&(n.childLanes|=t),n=e,e=e.return;return n.tag===3?n.stateNode:null}var Ht=!1;function ro(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function Ql(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function Nt(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function Yt(e,t,n){var i=e.updateQueue;if(i===null)return null;if(i=i.shared,(ae&2)!==0){var a=i.pending;return a===null?t.next=t:(t.next=a.next,a.next=t),i.pending=t,Dt(e,n)}return a=i.interleaved,a===null?(t.next=t,ao(i)):(t.next=a.next,a.next=t),i.interleaved=t,Dt(e,n)}function ba(e,t,n){if(t=t.updateQueue,t!==null&&(t=t.shared,(n&4194240)!==0)){var i=t.lanes;i&=e.pendingLanes,n|=i,t.lanes=n,yr(e,n)}}function Gl(e,t){var n=e.updateQueue,i=e.alternate;if(i!==null&&(i=i.updateQueue,n===i)){var a=null,r=null;if(n=n.firstBaseUpdate,n!==null){do{var s={eventTime:n.eventTime,lane:n.lane,tag:n.tag,payload:n.payload,callback:n.callback,next:null};r===null?a=r=s:r=r.next=s,n=n.next}while(n!==null);r===null?a=r=t:r=r.next=t}else a=r=t;n={baseState:i.baseState,firstBaseUpdate:a,lastBaseUpdate:r,shared:i.shared,effects:i.effects},e.updateQueue=n;return}e=n.lastBaseUpdate,e===null?n.firstBaseUpdate=t:e.next=t,n.lastBaseUpdate=t}function ka(e,t,n,i){var a=e.updateQueue;Ht=!1;var r=a.firstBaseUpdate,s=a.lastBaseUpdate,d=a.shared.pending;if(d!==null){a.shared.pending=null;var p=d,x=p.next;p.next=null,s===null?r=x:s.next=x,s=p;var I=e.alternate;I!==null&&(I=I.updateQueue,d=I.lastBaseUpdate,d!==s&&(d===null?I.firstBaseUpdate=x:d.next=x,I.lastBaseUpdate=p))}if(r!==null){var O=a.baseState;s=0,I=x=p=null,d=r;do{var N=d.lane,L=d.eventTime;if((i&N)===N){I!==null&&(I=I.next={eventTime:L,lane:0,tag:d.tag,payload:d.payload,callback:d.callback,next:null});e:{var j=e,F=d;switch(N=t,L=n,F.tag){case 1:if(j=F.payload,typeof j=="function"){O=j.call(L,O,N);break e}O=j;break e;case 3:j.flags=j.flags&-65537|128;case 0:if(j=F.payload,N=typeof j=="function"?j.call(L,O,N):j,N==null)break e;O=U({},O,N);break e;case 2:Ht=!0}}d.callback!==null&&d.lane!==0&&(e.flags|=64,N=a.effects,N===null?a.effects=[d]:N.push(d))}else L={eventTime:L,lane:N,tag:d.tag,payload:d.payload,callback:d.callback,next:null},I===null?(x=I=L,p=O):I=I.next=L,s|=N;if(d=d.next,d===null){if(d=a.shared.pending,d===null)break;N=d,d=N.next,N.next=null,a.lastBaseUpdate=N,a.shared.pending=null}}while(!0);if(I===null&&(p=O),a.baseState=p,a.firstBaseUpdate=x,a.lastBaseUpdate=I,t=a.shared.interleaved,t!==null){a=t;do s|=a.lane,a=a.next;while(a!==t)}else r===null&&(a.shared.lanes=0);hn|=s,e.lanes=s,e.memoizedState=O}}function Hl(e,t,n){if(e=t.effects,t.effects=null,e!==null)for(t=0;t<e.length;t++){var i=e[t],a=i.callback;if(a!==null){if(i.callback=null,i=n,typeof a!="function")throw Error(l(191,a));a.call(i)}}}var bi={},St=Ft(bi),ki=Ft(bi),Ai=Ft(bi);function un(e){if(e===bi)throw Error(l(174));return e}function oo(e,t){switch(he(Ai,t),he(ki,e),he(St,bi),e=t.nodeType,e){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:sr(null,"");break;default:e=e===8?t.parentNode:t,t=e.namespaceURI||null,e=e.tagName,t=sr(t,e)}we(St),he(St,t)}function Ln(){we(St),we(ki),we(Ai)}function Yl(e){un(Ai.current);var t=un(St.current),n=sr(t,e.type);t!==n&&(he(ki,e),he(St,n))}function so(e){ki.current===e&&(we(St),we(ki))}var be=Ft(0);function Aa(e){for(var t=e;t!==null;){if(t.tag===13){var n=t.memoizedState;if(n!==null&&(n=n.dehydrated,n===null||n.data==="$?"||n.data==="$!"))return t}else if(t.tag===19&&t.memoizedProps.revealOrder!==void 0){if((t.flags&128)!==0)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var lo=[];function co(){for(var e=0;e<lo.length;e++)lo[e]._workInProgressVersionPrimary=null;lo.length=0}var Ca=X.ReactCurrentDispatcher,uo=X.ReactCurrentBatchConfig,pn=0,ke=null,Ne=null,Oe=null,Ta=!1,Ci=!1,Ti=0,_p=0;function Be(){throw Error(l(321))}function po(e,t){if(t===null)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!pt(e[n],t[n]))return!1;return!0}function ho(e,t,n,i,a,r){if(pn=r,ke=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,Ca.current=e===null||e.memoizedState===null?Lp:Up,e=n(i,a),Ci){r=0;do{if(Ci=!1,Ti=0,25<=r)throw Error(l(301));r+=1,Oe=Ne=null,t.updateQueue=null,Ca.current=Bp,e=n(i,a)}while(Ci)}if(Ca.current=Na,t=Ne!==null&&Ne.next!==null,pn=0,Oe=Ne=ke=null,Ta=!1,t)throw Error(l(300));return e}function fo(){var e=Ti!==0;return Ti=0,e}function bt(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return Oe===null?ke.memoizedState=Oe=e:Oe=Oe.next=e,Oe}function lt(){if(Ne===null){var e=ke.alternate;e=e!==null?e.memoizedState:null}else e=Ne.next;var t=Oe===null?ke.memoizedState:Oe.next;if(t!==null)Oe=t,Ne=e;else{if(e===null)throw Error(l(310));Ne=e,e={memoizedState:Ne.memoizedState,baseState:Ne.baseState,baseQueue:Ne.baseQueue,queue:Ne.queue,next:null},Oe===null?ke.memoizedState=Oe=e:Oe=Oe.next=e}return Oe}function Ei(e,t){return typeof t=="function"?t(e):t}function mo(e){var t=lt(),n=t.queue;if(n===null)throw Error(l(311));n.lastRenderedReducer=e;var i=Ne,a=i.baseQueue,r=n.pending;if(r!==null){if(a!==null){var s=a.next;a.next=r.next,r.next=s}i.baseQueue=a=r,n.pending=null}if(a!==null){r=a.next,i=i.baseState;var d=s=null,p=null,x=r;do{var I=x.lane;if((pn&I)===I)p!==null&&(p=p.next={lane:0,action:x.action,hasEagerState:x.hasEagerState,eagerState:x.eagerState,next:null}),i=x.hasEagerState?x.eagerState:e(i,x.action);else{var O={lane:I,action:x.action,hasEagerState:x.hasEagerState,eagerState:x.eagerState,next:null};p===null?(d=p=O,s=i):p=p.next=O,ke.lanes|=I,hn|=I}x=x.next}while(x!==null&&x!==r);p===null?s=i:p.next=d,pt(i,t.memoizedState)||($e=!0),t.memoizedState=i,t.baseState=s,t.baseQueue=p,n.lastRenderedState=i}if(e=n.interleaved,e!==null){a=e;do r=a.lane,ke.lanes|=r,hn|=r,a=a.next;while(a!==e)}else a===null&&(n.lanes=0);return[t.memoizedState,n.dispatch]}function go(e){var t=lt(),n=t.queue;if(n===null)throw Error(l(311));n.lastRenderedReducer=e;var i=n.dispatch,a=n.pending,r=t.memoizedState;if(a!==null){n.pending=null;var s=a=a.next;do r=e(r,s.action),s=s.next;while(s!==a);pt(r,t.memoizedState)||($e=!0),t.memoizedState=r,t.baseQueue===null&&(t.baseState=r),n.lastRenderedState=r}return[r,i]}function Vl(){}function Jl(e,t){var n=ke,i=lt(),a=t(),r=!pt(i.memoizedState,a);if(r&&(i.memoizedState=a,$e=!0),i=i.queue,vo(Xl.bind(null,n,i,e),[e]),i.getSnapshot!==t||r||Oe!==null&&Oe.memoizedState.tag&1){if(n.flags|=2048,Di(9,Kl.bind(null,n,i,a,t),void 0,null),qe===null)throw Error(l(349));(pn&30)!==0||$l(n,t,a)}return a}function $l(e,t,n){e.flags|=16384,e={getSnapshot:t,value:n},t=ke.updateQueue,t===null?(t={lastEffect:null,stores:null},ke.updateQueue=t,t.stores=[e]):(n=t.stores,n===null?t.stores=[e]:n.push(e))}function Kl(e,t,n,i){t.value=n,t.getSnapshot=i,Zl(t)&&ec(e)}function Xl(e,t,n){return n(function(){Zl(t)&&ec(e)})}function Zl(e){var t=e.getSnapshot;e=e.value;try{var n=t();return!pt(e,n)}catch{return!0}}function ec(e){var t=Dt(e,1);t!==null&&vt(t,e,1,-1)}function tc(e){var t=bt();return typeof e=="function"&&(e=e()),t.memoizedState=t.baseState=e,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:Ei,lastRenderedState:e},t.queue=e,e=e.dispatch=Mp.bind(null,ke,e),[t.memoizedState,e]}function Di(e,t,n,i){return e={tag:e,create:t,destroy:n,deps:i,next:null},t=ke.updateQueue,t===null?(t={lastEffect:null,stores:null},ke.updateQueue=t,t.lastEffect=e.next=e):(n=t.lastEffect,n===null?t.lastEffect=e.next=e:(i=n.next,n.next=e,e.next=i,t.lastEffect=e)),e}function nc(){return lt().memoizedState}function Ea(e,t,n,i){var a=bt();ke.flags|=e,a.memoizedState=Di(1|t,n,void 0,i===void 0?null:i)}function Da(e,t,n,i){var a=lt();i=i===void 0?null:i;var r=void 0;if(Ne!==null){var s=Ne.memoizedState;if(r=s.destroy,i!==null&&po(i,s.deps)){a.memoizedState=Di(t,n,r,i);return}}ke.flags|=e,a.memoizedState=Di(1|t,n,r,i)}function ic(e,t){return Ea(8390656,8,e,t)}function vo(e,t){return Da(2048,8,e,t)}function ac(e,t){return Da(4,2,e,t)}function rc(e,t){return Da(4,4,e,t)}function oc(e,t){if(typeof t=="function")return e=e(),t(e),function(){t(null)};if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function sc(e,t,n){return n=n!=null?n.concat([e]):null,Da(4,4,oc.bind(null,t,e),n)}function wo(){}function lc(e,t){var n=lt();t=t===void 0?null:t;var i=n.memoizedState;return i!==null&&t!==null&&po(t,i[1])?i[0]:(n.memoizedState=[e,t],e)}function cc(e,t){var n=lt();t=t===void 0?null:t;var i=n.memoizedState;return i!==null&&t!==null&&po(t,i[1])?i[0]:(e=e(),n.memoizedState=[e,t],e)}function dc(e,t,n){return(pn&21)===0?(e.baseState&&(e.baseState=!1,$e=!0),e.memoizedState=n):(pt(n,t)||(n=Bs(),ke.lanes|=n,hn|=n,e.baseState=!0),t)}function Pp(e,t){var n=de;de=n!==0&&4>n?n:4,e(!0);var i=uo.transition;uo.transition={};try{e(!1),t()}finally{de=n,uo.transition=i}}function uc(){return lt().memoizedState}function zp(e,t,n){var i=Kt(e);if(n={lane:i,action:n,hasEagerState:!1,eagerState:null,next:null},pc(e))hc(t,n);else if(n=Fl(e,t,n,i),n!==null){var a=He();vt(n,e,i,a),fc(n,t,i)}}function Mp(e,t,n){var i=Kt(e),a={lane:i,action:n,hasEagerState:!1,eagerState:null,next:null};if(pc(e))hc(t,a);else{var r=e.alternate;if(e.lanes===0&&(r===null||r.lanes===0)&&(r=t.lastRenderedReducer,r!==null))try{var s=t.lastRenderedState,d=r(s,n);if(a.hasEagerState=!0,a.eagerState=d,pt(d,s)){var p=t.interleaved;p===null?(a.next=a,ao(t)):(a.next=p.next,p.next=a),t.interleaved=a;return}}catch{}finally{}n=Fl(e,t,a,i),n!==null&&(a=He(),vt(n,e,i,a),fc(n,t,i))}}function pc(e){var t=e.alternate;return e===ke||t!==null&&t===ke}function hc(e,t){Ci=Ta=!0;var n=e.pending;n===null?t.next=t:(t.next=n.next,n.next=t),e.pending=t}function fc(e,t,n){if((n&4194240)!==0){var i=t.lanes;i&=e.pendingLanes,n|=i,t.lanes=n,yr(e,n)}}var Na={readContext:st,useCallback:Be,useContext:Be,useEffect:Be,useImperativeHandle:Be,useInsertionEffect:Be,useLayoutEffect:Be,useMemo:Be,useReducer:Be,useRef:Be,useState:Be,useDebugValue:Be,useDeferredValue:Be,useTransition:Be,useMutableSource:Be,useSyncExternalStore:Be,useId:Be,unstable_isNewReconciler:!1},Lp={readContext:st,useCallback:function(e,t){return bt().memoizedState=[e,t===void 0?null:t],e},useContext:st,useEffect:ic,useImperativeHandle:function(e,t,n){return n=n!=null?n.concat([e]):null,Ea(4194308,4,oc.bind(null,t,e),n)},useLayoutEffect:function(e,t){return Ea(4194308,4,e,t)},useInsertionEffect:function(e,t){return Ea(4,2,e,t)},useMemo:function(e,t){var n=bt();return t=t===void 0?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var i=bt();return t=n!==void 0?n(t):t,i.memoizedState=i.baseState=t,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:t},i.queue=e,e=e.dispatch=zp.bind(null,ke,e),[i.memoizedState,e]},useRef:function(e){var t=bt();return e={current:e},t.memoizedState=e},useState:tc,useDebugValue:wo,useDeferredValue:function(e){return bt().memoizedState=e},useTransition:function(){var e=tc(!1),t=e[0];return e=Pp.bind(null,e[1]),bt().memoizedState=e,[t,e]},useMutableSource:function(){},useSyncExternalStore:function(e,t,n){var i=ke,a=bt();if(Se){if(n===void 0)throw Error(l(407));n=n()}else{if(n=t(),qe===null)throw Error(l(349));(pn&30)!==0||$l(i,t,n)}a.memoizedState=n;var r={value:n,getSnapshot:t};return a.queue=r,ic(Xl.bind(null,i,r,e),[e]),i.flags|=2048,Di(9,Kl.bind(null,i,r,n,t),void 0,null),n},useId:function(){var e=bt(),t=qe.identifierPrefix;if(Se){var n=Et,i=Tt;n=(i&~(1<<32-ut(i)-1)).toString(32)+n,t=":"+t+"R"+n,n=Ti++,0<n&&(t+="H"+n.toString(32)),t+=":"}else n=_p++,t=":"+t+"r"+n.toString(32)+":";return e.memoizedState=t},unstable_isNewReconciler:!1},Up={readContext:st,useCallback:lc,useContext:st,useEffect:vo,useImperativeHandle:sc,useInsertionEffect:ac,useLayoutEffect:rc,useMemo:cc,useReducer:mo,useRef:nc,useState:function(){return mo(Ei)},useDebugValue:wo,useDeferredValue:function(e){var t=lt();return dc(t,Ne.memoizedState,e)},useTransition:function(){var e=mo(Ei)[0],t=lt().memoizedState;return[e,t]},useMutableSource:Vl,useSyncExternalStore:Jl,useId:uc,unstable_isNewReconciler:!1},Bp={readContext:st,useCallback:lc,useContext:st,useEffect:vo,useImperativeHandle:sc,useInsertionEffect:ac,useLayoutEffect:rc,useMemo:cc,useReducer:go,useRef:nc,useState:function(){return go(Ei)},useDebugValue:wo,useDeferredValue:function(e){var t=lt();return Ne===null?t.memoizedState=e:dc(t,Ne.memoizedState,e)},useTransition:function(){var e=go(Ei)[0],t=lt().memoizedState;return[e,t]},useMutableSource:Vl,useSyncExternalStore:Jl,useId:uc,unstable_isNewReconciler:!1};function ft(e,t){if(e&&e.defaultProps){t=U({},t),e=e.defaultProps;for(var n in e)t[n]===void 0&&(t[n]=e[n]);return t}return t}function yo(e,t,n,i){t=e.memoizedState,n=n(i,t),n=n==null?t:U({},t,n),e.memoizedState=n,e.lanes===0&&(e.updateQueue.baseState=n)}var Ra={isMounted:function(e){return(e=e._reactInternals)?rn(e)===e:!1},enqueueSetState:function(e,t,n){e=e._reactInternals;var i=He(),a=Kt(e),r=Nt(i,a);r.payload=t,n!=null&&(r.callback=n),t=Yt(e,r,a),t!==null&&(vt(t,e,a,i),ba(t,e,a))},enqueueReplaceState:function(e,t,n){e=e._reactInternals;var i=He(),a=Kt(e),r=Nt(i,a);r.tag=1,r.payload=t,n!=null&&(r.callback=n),t=Yt(e,r,a),t!==null&&(vt(t,e,a,i),ba(t,e,a))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var n=He(),i=Kt(e),a=Nt(n,i);a.tag=2,t!=null&&(a.callback=t),t=Yt(e,a,i),t!==null&&(vt(t,e,i,n),ba(t,e,i))}};function mc(e,t,n,i,a,r,s){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(i,r,s):t.prototype&&t.prototype.isPureReactComponent?!fi(n,i)||!fi(a,r):!0}function gc(e,t,n){var i=!1,a=Qt,r=t.contextType;return typeof r=="object"&&r!==null?r=st(r):(a=Je(t)?sn:Ue.current,i=t.contextTypes,r=(i=i!=null)?In(e,a):Qt),t=new t(n,r),e.memoizedState=t.state!==null&&t.state!==void 0?t.state:null,t.updater=Ra,e.stateNode=t,t._reactInternals=e,i&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=a,e.__reactInternalMemoizedMaskedChildContext=r),t}function vc(e,t,n,i){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(n,i),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(n,i),t.state!==e&&Ra.enqueueReplaceState(t,t.state,null)}function xo(e,t,n,i){var a=e.stateNode;a.props=n,a.state=e.memoizedState,a.refs={},ro(e);var r=t.contextType;typeof r=="object"&&r!==null?a.context=st(r):(r=Je(t)?sn:Ue.current,a.context=In(e,r)),a.state=e.memoizedState,r=t.getDerivedStateFromProps,typeof r=="function"&&(yo(e,t,r,n),a.state=e.memoizedState),typeof t.getDerivedStateFromProps=="function"||typeof a.getSnapshotBeforeUpdate=="function"||typeof a.UNSAFE_componentWillMount!="function"&&typeof a.componentWillMount!="function"||(t=a.state,typeof a.componentWillMount=="function"&&a.componentWillMount(),typeof a.UNSAFE_componentWillMount=="function"&&a.UNSAFE_componentWillMount(),t!==a.state&&Ra.enqueueReplaceState(a,a.state,null),ka(e,n,a,i),a.state=e.memoizedState),typeof a.componentDidMount=="function"&&(e.flags|=4194308)}function Un(e,t){try{var n="",i=t;do n+=re(i),i=i.return;while(i);var a=n}catch(r){a=`
Error generating stack: `+r.message+`
`+r.stack}return{value:e,source:t,stack:a,digest:null}}function So(e,t,n){return{value:e,source:null,stack:n??null,digest:t??null}}function bo(e,t){try{console.error(t.value)}catch(n){setTimeout(function(){throw n})}}var jp=typeof WeakMap=="function"?WeakMap:Map;function wc(e,t,n){n=Nt(-1,n),n.tag=3,n.payload={element:null};var i=t.value;return n.callback=function(){Ma||(Ma=!0,Mo=i),bo(e,t)},n}function yc(e,t,n){n=Nt(-1,n),n.tag=3;var i=e.type.getDerivedStateFromError;if(typeof i=="function"){var a=t.value;n.payload=function(){return i(a)},n.callback=function(){bo(e,t)}}var r=e.stateNode;return r!==null&&typeof r.componentDidCatch=="function"&&(n.callback=function(){bo(e,t),typeof i!="function"&&(Jt===null?Jt=new Set([this]):Jt.add(this));var s=t.stack;this.componentDidCatch(t.value,{componentStack:s!==null?s:""})}),n}function xc(e,t,n){var i=e.pingCache;if(i===null){i=e.pingCache=new jp;var a=new Set;i.set(t,a)}else a=i.get(t),a===void 0&&(a=new Set,i.set(t,a));a.has(n)||(a.add(n),e=th.bind(null,e,t,n),t.then(e,e))}function Sc(e){do{var t;if((t=e.tag===13)&&(t=e.memoizedState,t=t!==null?t.dehydrated!==null:!0),t)return e;e=e.return}while(e!==null);return null}function bc(e,t,n,i,a){return(e.mode&1)===0?(e===t?e.flags|=65536:(e.flags|=128,n.flags|=131072,n.flags&=-52805,n.tag===1&&(n.alternate===null?n.tag=17:(t=Nt(-1,1),t.tag=2,Yt(n,t,1))),n.lanes|=1),e):(e.flags|=65536,e.lanes=a,e)}var Wp=X.ReactCurrentOwner,$e=!1;function Ge(e,t,n,i){t.child=e===null?Wl(t,null,n,i):Pn(t,e.child,n,i)}function kc(e,t,n,i,a){n=n.render;var r=t.ref;return Mn(t,a),i=ho(e,t,n,i,r,a),n=fo(),e!==null&&!$e?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~a,Rt(e,t,a)):(Se&&n&&Jr(t),t.flags|=1,Ge(e,t,i,a),t.child)}function Ac(e,t,n,i,a){if(e===null){var r=n.type;return typeof r=="function"&&!Qo(r)&&r.defaultProps===void 0&&n.compare===null&&n.defaultProps===void 0?(t.tag=15,t.type=r,Cc(e,t,r,i,a)):(e=Fa(n.type,null,i,t,t.mode,a),e.ref=t.ref,e.return=t,t.child=e)}if(r=e.child,(e.lanes&a)===0){var s=r.memoizedProps;if(n=n.compare,n=n!==null?n:fi,n(s,i)&&e.ref===t.ref)return Rt(e,t,a)}return t.flags|=1,e=Zt(r,i),e.ref=t.ref,e.return=t,t.child=e}function Cc(e,t,n,i,a){if(e!==null){var r=e.memoizedProps;if(fi(r,i)&&e.ref===t.ref)if($e=!1,t.pendingProps=i=r,(e.lanes&a)!==0)(e.flags&131072)!==0&&($e=!0);else return t.lanes=e.lanes,Rt(e,t,a)}return ko(e,t,n,i,a)}function Tc(e,t,n){var i=t.pendingProps,a=i.children,r=e!==null?e.memoizedState:null;if(i.mode==="hidden")if((t.mode&1)===0)t.memoizedState={baseLanes:0,cachePool:null,transitions:null},he(jn,at),at|=n;else{if((n&1073741824)===0)return e=r!==null?r.baseLanes|n:n,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e,cachePool:null,transitions:null},t.updateQueue=null,he(jn,at),at|=e,null;t.memoizedState={baseLanes:0,cachePool:null,transitions:null},i=r!==null?r.baseLanes:n,he(jn,at),at|=i}else r!==null?(i=r.baseLanes|n,t.memoizedState=null):i=n,he(jn,at),at|=i;return Ge(e,t,a,n),t.child}function Ec(e,t){var n=t.ref;(e===null&&n!==null||e!==null&&e.ref!==n)&&(t.flags|=512,t.flags|=2097152)}function ko(e,t,n,i,a){var r=Je(n)?sn:Ue.current;return r=In(t,r),Mn(t,a),n=ho(e,t,n,i,r,a),i=fo(),e!==null&&!$e?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~a,Rt(e,t,a)):(Se&&i&&Jr(t),t.flags|=1,Ge(e,t,n,a),t.child)}function Dc(e,t,n,i,a){if(Je(n)){var r=!0;fa(t)}else r=!1;if(Mn(t,a),t.stateNode===null)Oa(e,t),gc(t,n,i),xo(t,n,i,a),i=!0;else if(e===null){var s=t.stateNode,d=t.memoizedProps;s.props=d;var p=s.context,x=n.contextType;typeof x=="object"&&x!==null?x=st(x):(x=Je(n)?sn:Ue.current,x=In(t,x));var I=n.getDerivedStateFromProps,O=typeof I=="function"||typeof s.getSnapshotBeforeUpdate=="function";O||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(d!==i||p!==x)&&vc(t,s,i,x),Ht=!1;var N=t.memoizedState;s.state=N,ka(t,i,s,a),p=t.memoizedState,d!==i||N!==p||Ve.current||Ht?(typeof I=="function"&&(yo(t,n,I,i),p=t.memoizedState),(d=Ht||mc(t,n,d,i,N,p,x))?(O||typeof s.UNSAFE_componentWillMount!="function"&&typeof s.componentWillMount!="function"||(typeof s.componentWillMount=="function"&&s.componentWillMount(),typeof s.UNSAFE_componentWillMount=="function"&&s.UNSAFE_componentWillMount()),typeof s.componentDidMount=="function"&&(t.flags|=4194308)):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=i,t.memoizedState=p),s.props=i,s.state=p,s.context=x,i=d):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),i=!1)}else{s=t.stateNode,Ql(e,t),d=t.memoizedProps,x=t.type===t.elementType?d:ft(t.type,d),s.props=x,O=t.pendingProps,N=s.context,p=n.contextType,typeof p=="object"&&p!==null?p=st(p):(p=Je(n)?sn:Ue.current,p=In(t,p));var L=n.getDerivedStateFromProps;(I=typeof L=="function"||typeof s.getSnapshotBeforeUpdate=="function")||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(d!==O||N!==p)&&vc(t,s,i,p),Ht=!1,N=t.memoizedState,s.state=N,ka(t,i,s,a);var j=t.memoizedState;d!==O||N!==j||Ve.current||Ht?(typeof L=="function"&&(yo(t,n,L,i),j=t.memoizedState),(x=Ht||mc(t,n,x,i,N,j,p)||!1)?(I||typeof s.UNSAFE_componentWillUpdate!="function"&&typeof s.componentWillUpdate!="function"||(typeof s.componentWillUpdate=="function"&&s.componentWillUpdate(i,j,p),typeof s.UNSAFE_componentWillUpdate=="function"&&s.UNSAFE_componentWillUpdate(i,j,p)),typeof s.componentDidUpdate=="function"&&(t.flags|=4),typeof s.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof s.componentDidUpdate!="function"||d===e.memoizedProps&&N===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||d===e.memoizedProps&&N===e.memoizedState||(t.flags|=1024),t.memoizedProps=i,t.memoizedState=j),s.props=i,s.state=j,s.context=p,i=x):(typeof s.componentDidUpdate!="function"||d===e.memoizedProps&&N===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||d===e.memoizedProps&&N===e.memoizedState||(t.flags|=1024),i=!1)}return Ao(e,t,n,i,r,a)}function Ao(e,t,n,i,a,r){Ec(e,t);var s=(t.flags&128)!==0;if(!i&&!s)return a&&ql(t,n,!1),Rt(e,t,r);i=t.stateNode,Wp.current=t;var d=s&&typeof n.getDerivedStateFromError!="function"?null:i.render();return t.flags|=1,e!==null&&s?(t.child=Pn(t,e.child,null,r),t.child=Pn(t,null,d,r)):Ge(e,t,d,r),t.memoizedState=i.state,a&&ql(t,n,!0),t.child}function Nc(e){var t=e.stateNode;t.pendingContext?Il(e,t.pendingContext,t.pendingContext!==t.context):t.context&&Il(e,t.context,!1),oo(e,t.containerInfo)}function Rc(e,t,n,i,a){return _n(),Zr(a),t.flags|=256,Ge(e,t,n,i),t.child}var Co={dehydrated:null,treeContext:null,retryLane:0};function To(e){return{baseLanes:e,cachePool:null,transitions:null}}function Ic(e,t,n){var i=t.pendingProps,a=be.current,r=!1,s=(t.flags&128)!==0,d;if((d=s)||(d=e!==null&&e.memoizedState===null?!1:(a&2)!==0),d?(r=!0,t.flags&=-129):(e===null||e.memoizedState!==null)&&(a|=1),he(be,a&1),e===null)return Xr(t),e=t.memoizedState,e!==null&&(e=e.dehydrated,e!==null)?((t.mode&1)===0?t.lanes=1:e.data==="$!"?t.lanes=8:t.lanes=1073741824,null):(s=i.children,e=i.fallback,r?(i=t.mode,r=t.child,s={mode:"hidden",children:s},(i&1)===0&&r!==null?(r.childLanes=0,r.pendingProps=s):r=Qa(s,i,0,null),e=vn(e,i,n,null),r.return=t,e.return=t,r.sibling=e,t.child=r,t.child.memoizedState=To(n),t.memoizedState=Co,e):Eo(t,s));if(a=e.memoizedState,a!==null&&(d=a.dehydrated,d!==null))return Fp(e,t,s,i,d,a,n);if(r){r=i.fallback,s=t.mode,a=e.child,d=a.sibling;var p={mode:"hidden",children:i.children};return(s&1)===0&&t.child!==a?(i=t.child,i.childLanes=0,i.pendingProps=p,t.deletions=null):(i=Zt(a,p),i.subtreeFlags=a.subtreeFlags&14680064),d!==null?r=Zt(d,r):(r=vn(r,s,n,null),r.flags|=2),r.return=t,i.return=t,i.sibling=r,t.child=i,i=r,r=t.child,s=e.child.memoizedState,s=s===null?To(n):{baseLanes:s.baseLanes|n,cachePool:null,transitions:s.transitions},r.memoizedState=s,r.childLanes=e.childLanes&~n,t.memoizedState=Co,i}return r=e.child,e=r.sibling,i=Zt(r,{mode:"visible",children:i.children}),(t.mode&1)===0&&(i.lanes=n),i.return=t,i.sibling=null,e!==null&&(n=t.deletions,n===null?(t.deletions=[e],t.flags|=16):n.push(e)),t.child=i,t.memoizedState=null,i}function Eo(e,t){return t=Qa({mode:"visible",children:t},e.mode,0,null),t.return=e,e.child=t}function Ia(e,t,n,i){return i!==null&&Zr(i),Pn(t,e.child,null,n),e=Eo(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function Fp(e,t,n,i,a,r,s){if(n)return t.flags&256?(t.flags&=-257,i=So(Error(l(422))),Ia(e,t,s,i)):t.memoizedState!==null?(t.child=e.child,t.flags|=128,null):(r=i.fallback,a=t.mode,i=Qa({mode:"visible",children:i.children},a,0,null),r=vn(r,a,s,null),r.flags|=2,i.return=t,r.return=t,i.sibling=r,t.child=i,(t.mode&1)!==0&&Pn(t,e.child,null,s),t.child.memoizedState=To(s),t.memoizedState=Co,r);if((t.mode&1)===0)return Ia(e,t,s,null);if(a.data==="$!"){if(i=a.nextSibling&&a.nextSibling.dataset,i)var d=i.dgst;return i=d,r=Error(l(419)),i=So(r,i,void 0),Ia(e,t,s,i)}if(d=(s&e.childLanes)!==0,$e||d){if(i=qe,i!==null){switch(s&-s){case 4:a=2;break;case 16:a=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:a=32;break;case 536870912:a=268435456;break;default:a=0}a=(a&(i.suspendedLanes|s))!==0?0:a,a!==0&&a!==r.retryLane&&(r.retryLane=a,Dt(e,a),vt(i,e,a,-1))}return Fo(),i=So(Error(l(421))),Ia(e,t,s,i)}return a.data==="$?"?(t.flags|=128,t.child=e.child,t=nh.bind(null,e),a._reactRetry=t,null):(e=r.treeContext,it=Wt(a.nextSibling),nt=t,Se=!0,ht=null,e!==null&&(rt[ot++]=Tt,rt[ot++]=Et,rt[ot++]=ln,Tt=e.id,Et=e.overflow,ln=t),t=Eo(t,i.children),t.flags|=4096,t)}function Oc(e,t,n){e.lanes|=t;var i=e.alternate;i!==null&&(i.lanes|=t),io(e.return,t,n)}function Do(e,t,n,i,a){var r=e.memoizedState;r===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:i,tail:n,tailMode:a}:(r.isBackwards=t,r.rendering=null,r.renderingStartTime=0,r.last=i,r.tail=n,r.tailMode=a)}function qc(e,t,n){var i=t.pendingProps,a=i.revealOrder,r=i.tail;if(Ge(e,t,i.children,n),i=be.current,(i&2)!==0)i=i&1|2,t.flags|=128;else{if(e!==null&&(e.flags&128)!==0)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&Oc(e,n,t);else if(e.tag===19)Oc(e,n,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}i&=1}if(he(be,i),(t.mode&1)===0)t.memoizedState=null;else switch(a){case"forwards":for(n=t.child,a=null;n!==null;)e=n.alternate,e!==null&&Aa(e)===null&&(a=n),n=n.sibling;n=a,n===null?(a=t.child,t.child=null):(a=n.sibling,n.sibling=null),Do(t,!1,a,n,r);break;case"backwards":for(n=null,a=t.child,t.child=null;a!==null;){if(e=a.alternate,e!==null&&Aa(e)===null){t.child=a;break}e=a.sibling,a.sibling=n,n=a,a=e}Do(t,!0,n,null,r);break;case"together":Do(t,!1,null,null,void 0);break;default:t.memoizedState=null}return t.child}function Oa(e,t){(t.mode&1)===0&&e!==null&&(e.alternate=null,t.alternate=null,t.flags|=2)}function Rt(e,t,n){if(e!==null&&(t.dependencies=e.dependencies),hn|=t.lanes,(n&t.childLanes)===0)return null;if(e!==null&&t.child!==e.child)throw Error(l(153));if(t.child!==null){for(e=t.child,n=Zt(e,e.pendingProps),t.child=n,n.return=t;e.sibling!==null;)e=e.sibling,n=n.sibling=Zt(e,e.pendingProps),n.return=t;n.sibling=null}return t.child}function Qp(e,t,n){switch(t.tag){case 3:Nc(t),_n();break;case 5:Yl(t);break;case 1:Je(t.type)&&fa(t);break;case 4:oo(t,t.stateNode.containerInfo);break;case 10:var i=t.type._context,a=t.memoizedProps.value;he(xa,i._currentValue),i._currentValue=a;break;case 13:if(i=t.memoizedState,i!==null)return i.dehydrated!==null?(he(be,be.current&1),t.flags|=128,null):(n&t.child.childLanes)!==0?Ic(e,t,n):(he(be,be.current&1),e=Rt(e,t,n),e!==null?e.sibling:null);he(be,be.current&1);break;case 19:if(i=(n&t.childLanes)!==0,(e.flags&128)!==0){if(i)return qc(e,t,n);t.flags|=128}if(a=t.memoizedState,a!==null&&(a.rendering=null,a.tail=null,a.lastEffect=null),he(be,be.current),i)break;return null;case 22:case 23:return t.lanes=0,Tc(e,t,n)}return Rt(e,t,n)}var _c,No,Pc,zc;_c=function(e,t){for(var n=t.child;n!==null;){if(n.tag===5||n.tag===6)e.appendChild(n.stateNode);else if(n.tag!==4&&n.child!==null){n.child.return=n,n=n.child;continue}if(n===t)break;for(;n.sibling===null;){if(n.return===null||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}},No=function(){},Pc=function(e,t,n,i){var a=e.memoizedProps;if(a!==i){e=t.stateNode,un(St.current);var r=null;switch(n){case"input":a=ir(e,a),i=ir(e,i),r=[];break;case"select":a=U({},a,{value:void 0}),i=U({},i,{value:void 0}),r=[];break;case"textarea":a=or(e,a),i=or(e,i),r=[];break;default:typeof a.onClick!="function"&&typeof i.onClick=="function"&&(e.onclick=ua)}lr(n,i);var s;n=null;for(x in a)if(!i.hasOwnProperty(x)&&a.hasOwnProperty(x)&&a[x]!=null)if(x==="style"){var d=a[x];for(s in d)d.hasOwnProperty(s)&&(n||(n={}),n[s]="")}else x!=="dangerouslySetInnerHTML"&&x!=="children"&&x!=="suppressContentEditableWarning"&&x!=="suppressHydrationWarning"&&x!=="autoFocus"&&(h.hasOwnProperty(x)?r||(r=[]):(r=r||[]).push(x,null));for(x in i){var p=i[x];if(d=a!=null?a[x]:void 0,i.hasOwnProperty(x)&&p!==d&&(p!=null||d!=null))if(x==="style")if(d){for(s in d)!d.hasOwnProperty(s)||p&&p.hasOwnProperty(s)||(n||(n={}),n[s]="");for(s in p)p.hasOwnProperty(s)&&d[s]!==p[s]&&(n||(n={}),n[s]=p[s])}else n||(r||(r=[]),r.push(x,n)),n=p;else x==="dangerouslySetInnerHTML"?(p=p?p.__html:void 0,d=d?d.__html:void 0,p!=null&&d!==p&&(r=r||[]).push(x,p)):x==="children"?typeof p!="string"&&typeof p!="number"||(r=r||[]).push(x,""+p):x!=="suppressContentEditableWarning"&&x!=="suppressHydrationWarning"&&(h.hasOwnProperty(x)?(p!=null&&x==="onScroll"&&ve("scroll",e),r||d===p||(r=[])):(r=r||[]).push(x,p))}n&&(r=r||[]).push("style",n);var x=r;(t.updateQueue=x)&&(t.flags|=4)}},zc=function(e,t,n,i){n!==i&&(t.flags|=4)};function Ni(e,t){if(!Se)switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;t!==null;)t.alternate!==null&&(n=t),t=t.sibling;n===null?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var i=null;n!==null;)n.alternate!==null&&(i=n),n=n.sibling;i===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:i.sibling=null}}function je(e){var t=e.alternate!==null&&e.alternate.child===e.child,n=0,i=0;if(t)for(var a=e.child;a!==null;)n|=a.lanes|a.childLanes,i|=a.subtreeFlags&14680064,i|=a.flags&14680064,a.return=e,a=a.sibling;else for(a=e.child;a!==null;)n|=a.lanes|a.childLanes,i|=a.subtreeFlags,i|=a.flags,a.return=e,a=a.sibling;return e.subtreeFlags|=i,e.childLanes=n,t}function Gp(e,t,n){var i=t.pendingProps;switch($r(t),t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return je(t),null;case 1:return Je(t.type)&&ha(),je(t),null;case 3:return i=t.stateNode,Ln(),we(Ve),we(Ue),co(),i.pendingContext&&(i.context=i.pendingContext,i.pendingContext=null),(e===null||e.child===null)&&(wa(t)?t.flags|=4:e===null||e.memoizedState.isDehydrated&&(t.flags&256)===0||(t.flags|=1024,ht!==null&&(Bo(ht),ht=null))),No(e,t),je(t),null;case 5:so(t);var a=un(Ai.current);if(n=t.type,e!==null&&t.stateNode!=null)Pc(e,t,n,i,a),e.ref!==t.ref&&(t.flags|=512,t.flags|=2097152);else{if(!i){if(t.stateNode===null)throw Error(l(166));return je(t),null}if(e=un(St.current),wa(t)){i=t.stateNode,n=t.type;var r=t.memoizedProps;switch(i[xt]=t,i[yi]=r,e=(t.mode&1)!==0,n){case"dialog":ve("cancel",i),ve("close",i);break;case"iframe":case"object":case"embed":ve("load",i);break;case"video":case"audio":for(a=0;a<gi.length;a++)ve(gi[a],i);break;case"source":ve("error",i);break;case"img":case"image":case"link":ve("error",i),ve("load",i);break;case"details":ve("toggle",i);break;case"input":gs(i,r),ve("invalid",i);break;case"select":i._wrapperState={wasMultiple:!!r.multiple},ve("invalid",i);break;case"textarea":ys(i,r),ve("invalid",i)}lr(n,r),a=null;for(var s in r)if(r.hasOwnProperty(s)){var d=r[s];s==="children"?typeof d=="string"?i.textContent!==d&&(r.suppressHydrationWarning!==!0&&da(i.textContent,d,e),a=["children",d]):typeof d=="number"&&i.textContent!==""+d&&(r.suppressHydrationWarning!==!0&&da(i.textContent,d,e),a=["children",""+d]):h.hasOwnProperty(s)&&d!=null&&s==="onScroll"&&ve("scroll",i)}switch(n){case"input":ji(i),ws(i,r,!0);break;case"textarea":ji(i),Ss(i);break;case"select":case"option":break;default:typeof r.onClick=="function"&&(i.onclick=ua)}i=a,t.updateQueue=i,i!==null&&(t.flags|=4)}else{s=a.nodeType===9?a:a.ownerDocument,e==="http://www.w3.org/1999/xhtml"&&(e=bs(n)),e==="http://www.w3.org/1999/xhtml"?n==="script"?(e=s.createElement("div"),e.innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):typeof i.is=="string"?e=s.createElement(n,{is:i.is}):(e=s.createElement(n),n==="select"&&(s=e,i.multiple?s.multiple=!0:i.size&&(s.size=i.size))):e=s.createElementNS(e,n),e[xt]=t,e[yi]=i,_c(e,t,!1,!1),t.stateNode=e;e:{switch(s=cr(n,i),n){case"dialog":ve("cancel",e),ve("close",e),a=i;break;case"iframe":case"object":case"embed":ve("load",e),a=i;break;case"video":case"audio":for(a=0;a<gi.length;a++)ve(gi[a],e);a=i;break;case"source":ve("error",e),a=i;break;case"img":case"image":case"link":ve("error",e),ve("load",e),a=i;break;case"details":ve("toggle",e),a=i;break;case"input":gs(e,i),a=ir(e,i),ve("invalid",e);break;case"option":a=i;break;case"select":e._wrapperState={wasMultiple:!!i.multiple},a=U({},i,{value:void 0}),ve("invalid",e);break;case"textarea":ys(e,i),a=or(e,i),ve("invalid",e);break;default:a=i}lr(n,a),d=a;for(r in d)if(d.hasOwnProperty(r)){var p=d[r];r==="style"?Cs(e,p):r==="dangerouslySetInnerHTML"?(p=p?p.__html:void 0,p!=null&&ks(e,p)):r==="children"?typeof p=="string"?(n!=="textarea"||p!=="")&&Kn(e,p):typeof p=="number"&&Kn(e,""+p):r!=="suppressContentEditableWarning"&&r!=="suppressHydrationWarning"&&r!=="autoFocus"&&(h.hasOwnProperty(r)?p!=null&&r==="onScroll"&&ve("scroll",e):p!=null&&K(e,r,p,s))}switch(n){case"input":ji(e),ws(e,i,!1);break;case"textarea":ji(e),Ss(e);break;case"option":i.value!=null&&e.setAttribute("value",""+ce(i.value));break;case"select":e.multiple=!!i.multiple,r=i.value,r!=null?yn(e,!!i.multiple,r,!1):i.defaultValue!=null&&yn(e,!!i.multiple,i.defaultValue,!0);break;default:typeof a.onClick=="function"&&(e.onclick=ua)}switch(n){case"button":case"input":case"select":case"textarea":i=!!i.autoFocus;break e;case"img":i=!0;break e;default:i=!1}}i&&(t.flags|=4)}t.ref!==null&&(t.flags|=512,t.flags|=2097152)}return je(t),null;case 6:if(e&&t.stateNode!=null)zc(e,t,e.memoizedProps,i);else{if(typeof i!="string"&&t.stateNode===null)throw Error(l(166));if(n=un(Ai.current),un(St.current),wa(t)){if(i=t.stateNode,n=t.memoizedProps,i[xt]=t,(r=i.nodeValue!==n)&&(e=nt,e!==null))switch(e.tag){case 3:da(i.nodeValue,n,(e.mode&1)!==0);break;case 5:e.memoizedProps.suppressHydrationWarning!==!0&&da(i.nodeValue,n,(e.mode&1)!==0)}r&&(t.flags|=4)}else i=(n.nodeType===9?n:n.ownerDocument).createTextNode(i),i[xt]=t,t.stateNode=i}return je(t),null;case 13:if(we(be),i=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(Se&&it!==null&&(t.mode&1)!==0&&(t.flags&128)===0)Ul(),_n(),t.flags|=98560,r=!1;else if(r=wa(t),i!==null&&i.dehydrated!==null){if(e===null){if(!r)throw Error(l(318));if(r=t.memoizedState,r=r!==null?r.dehydrated:null,!r)throw Error(l(317));r[xt]=t}else _n(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;je(t),r=!1}else ht!==null&&(Bo(ht),ht=null),r=!0;if(!r)return t.flags&65536?t:null}return(t.flags&128)!==0?(t.lanes=n,t):(i=i!==null,i!==(e!==null&&e.memoizedState!==null)&&i&&(t.child.flags|=8192,(t.mode&1)!==0&&(e===null||(be.current&1)!==0?Re===0&&(Re=3):Fo())),t.updateQueue!==null&&(t.flags|=4),je(t),null);case 4:return Ln(),No(e,t),e===null&&vi(t.stateNode.containerInfo),je(t),null;case 10:return no(t.type._context),je(t),null;case 17:return Je(t.type)&&ha(),je(t),null;case 19:if(we(be),r=t.memoizedState,r===null)return je(t),null;if(i=(t.flags&128)!==0,s=r.rendering,s===null)if(i)Ni(r,!1);else{if(Re!==0||e!==null&&(e.flags&128)!==0)for(e=t.child;e!==null;){if(s=Aa(e),s!==null){for(t.flags|=128,Ni(r,!1),i=s.updateQueue,i!==null&&(t.updateQueue=i,t.flags|=4),t.subtreeFlags=0,i=n,n=t.child;n!==null;)r=n,e=i,r.flags&=14680066,s=r.alternate,s===null?(r.childLanes=0,r.lanes=e,r.child=null,r.subtreeFlags=0,r.memoizedProps=null,r.memoizedState=null,r.updateQueue=null,r.dependencies=null,r.stateNode=null):(r.childLanes=s.childLanes,r.lanes=s.lanes,r.child=s.child,r.subtreeFlags=0,r.deletions=null,r.memoizedProps=s.memoizedProps,r.memoizedState=s.memoizedState,r.updateQueue=s.updateQueue,r.type=s.type,e=s.dependencies,r.dependencies=e===null?null:{lanes:e.lanes,firstContext:e.firstContext}),n=n.sibling;return he(be,be.current&1|2),t.child}e=e.sibling}r.tail!==null&&Te()>Wn&&(t.flags|=128,i=!0,Ni(r,!1),t.lanes=4194304)}else{if(!i)if(e=Aa(s),e!==null){if(t.flags|=128,i=!0,n=e.updateQueue,n!==null&&(t.updateQueue=n,t.flags|=4),Ni(r,!0),r.tail===null&&r.tailMode==="hidden"&&!s.alternate&&!Se)return je(t),null}else 2*Te()-r.renderingStartTime>Wn&&n!==1073741824&&(t.flags|=128,i=!0,Ni(r,!1),t.lanes=4194304);r.isBackwards?(s.sibling=t.child,t.child=s):(n=r.last,n!==null?n.sibling=s:t.child=s,r.last=s)}return r.tail!==null?(t=r.tail,r.rendering=t,r.tail=t.sibling,r.renderingStartTime=Te(),t.sibling=null,n=be.current,he(be,i?n&1|2:n&1),t):(je(t),null);case 22:case 23:return Wo(),i=t.memoizedState!==null,e!==null&&e.memoizedState!==null!==i&&(t.flags|=8192),i&&(t.mode&1)!==0?(at&1073741824)!==0&&(je(t),t.subtreeFlags&6&&(t.flags|=8192)):je(t),null;case 24:return null;case 25:return null}throw Error(l(156,t.tag))}function Hp(e,t){switch($r(t),t.tag){case 1:return Je(t.type)&&ha(),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return Ln(),we(Ve),we(Ue),co(),e=t.flags,(e&65536)!==0&&(e&128)===0?(t.flags=e&-65537|128,t):null;case 5:return so(t),null;case 13:if(we(be),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(l(340));_n()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return we(be),null;case 4:return Ln(),null;case 10:return no(t.type._context),null;case 22:case 23:return Wo(),null;case 24:return null;default:return null}}var qa=!1,We=!1,Yp=typeof WeakSet=="function"?WeakSet:Set,B=null;function Bn(e,t){var n=e.ref;if(n!==null)if(typeof n=="function")try{n(null)}catch(i){Ce(e,t,i)}else n.current=null}function Ro(e,t,n){try{n()}catch(i){Ce(e,t,i)}}var Mc=!1;function Vp(e,t){if(jr=Zi,e=ml(),qr(e)){if("selectionStart"in e)var n={start:e.selectionStart,end:e.selectionEnd};else e:{n=(n=e.ownerDocument)&&n.defaultView||window;var i=n.getSelection&&n.getSelection();if(i&&i.rangeCount!==0){n=i.anchorNode;var a=i.anchorOffset,r=i.focusNode;i=i.focusOffset;try{n.nodeType,r.nodeType}catch{n=null;break e}var s=0,d=-1,p=-1,x=0,I=0,O=e,N=null;t:for(;;){for(var L;O!==n||a!==0&&O.nodeType!==3||(d=s+a),O!==r||i!==0&&O.nodeType!==3||(p=s+i),O.nodeType===3&&(s+=O.nodeValue.length),(L=O.firstChild)!==null;)N=O,O=L;for(;;){if(O===e)break t;if(N===n&&++x===a&&(d=s),N===r&&++I===i&&(p=s),(L=O.nextSibling)!==null)break;O=N,N=O.parentNode}O=L}n=d===-1||p===-1?null:{start:d,end:p}}else n=null}n=n||{start:0,end:0}}else n=null;for(Wr={focusedElem:e,selectionRange:n},Zi=!1,B=t;B!==null;)if(t=B,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,B=e;else for(;B!==null;){t=B;try{var j=t.alternate;if((t.flags&1024)!==0)switch(t.tag){case 0:case 11:case 15:break;case 1:if(j!==null){var F=j.memoizedProps,Ee=j.memoizedState,v=t.stateNode,f=v.getSnapshotBeforeUpdate(t.elementType===t.type?F:ft(t.type,F),Ee);v.__reactInternalSnapshotBeforeUpdate=f}break;case 3:var w=t.stateNode.containerInfo;w.nodeType===1?w.textContent="":w.nodeType===9&&w.documentElement&&w.removeChild(w.documentElement);break;case 5:case 6:case 4:case 17:break;default:throw Error(l(163))}}catch(_){Ce(t,t.return,_)}if(e=t.sibling,e!==null){e.return=t.return,B=e;break}B=t.return}return j=Mc,Mc=!1,j}function Ri(e,t,n){var i=t.updateQueue;if(i=i!==null?i.lastEffect:null,i!==null){var a=i=i.next;do{if((a.tag&e)===e){var r=a.destroy;a.destroy=void 0,r!==void 0&&Ro(t,n,r)}a=a.next}while(a!==i)}}function _a(e,t){if(t=t.updateQueue,t=t!==null?t.lastEffect:null,t!==null){var n=t=t.next;do{if((n.tag&e)===e){var i=n.create;n.destroy=i()}n=n.next}while(n!==t)}}function Io(e){var t=e.ref;if(t!==null){var n=e.stateNode;switch(e.tag){case 5:e=n;break;default:e=n}typeof t=="function"?t(e):t.current=e}}function Lc(e){var t=e.alternate;t!==null&&(e.alternate=null,Lc(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&(delete t[xt],delete t[yi],delete t[Hr],delete t[Rp],delete t[Ip])),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}function Uc(e){return e.tag===5||e.tag===3||e.tag===4}function Bc(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||Uc(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function Oo(e,t,n){var i=e.tag;if(i===5||i===6)e=e.stateNode,t?n.nodeType===8?n.parentNode.insertBefore(e,t):n.insertBefore(e,t):(n.nodeType===8?(t=n.parentNode,t.insertBefore(e,n)):(t=n,t.appendChild(e)),n=n._reactRootContainer,n!=null||t.onclick!==null||(t.onclick=ua));else if(i!==4&&(e=e.child,e!==null))for(Oo(e,t,n),e=e.sibling;e!==null;)Oo(e,t,n),e=e.sibling}function qo(e,t,n){var i=e.tag;if(i===5||i===6)e=e.stateNode,t?n.insertBefore(e,t):n.appendChild(e);else if(i!==4&&(e=e.child,e!==null))for(qo(e,t,n),e=e.sibling;e!==null;)qo(e,t,n),e=e.sibling}var ze=null,mt=!1;function Vt(e,t,n){for(n=n.child;n!==null;)jc(e,t,n),n=n.sibling}function jc(e,t,n){if(yt&&typeof yt.onCommitFiberUnmount=="function")try{yt.onCommitFiberUnmount(Yi,n)}catch{}switch(n.tag){case 5:We||Bn(n,t);case 6:var i=ze,a=mt;ze=null,Vt(e,t,n),ze=i,mt=a,ze!==null&&(mt?(e=ze,n=n.stateNode,e.nodeType===8?e.parentNode.removeChild(n):e.removeChild(n)):ze.removeChild(n.stateNode));break;case 18:ze!==null&&(mt?(e=ze,n=n.stateNode,e.nodeType===8?Gr(e.parentNode,n):e.nodeType===1&&Gr(e,n),li(e)):Gr(ze,n.stateNode));break;case 4:i=ze,a=mt,ze=n.stateNode.containerInfo,mt=!0,Vt(e,t,n),ze=i,mt=a;break;case 0:case 11:case 14:case 15:if(!We&&(i=n.updateQueue,i!==null&&(i=i.lastEffect,i!==null))){a=i=i.next;do{var r=a,s=r.destroy;r=r.tag,s!==void 0&&((r&2)!==0||(r&4)!==0)&&Ro(n,t,s),a=a.next}while(a!==i)}Vt(e,t,n);break;case 1:if(!We&&(Bn(n,t),i=n.stateNode,typeof i.componentWillUnmount=="function"))try{i.props=n.memoizedProps,i.state=n.memoizedState,i.componentWillUnmount()}catch(d){Ce(n,t,d)}Vt(e,t,n);break;case 21:Vt(e,t,n);break;case 22:n.mode&1?(We=(i=We)||n.memoizedState!==null,Vt(e,t,n),We=i):Vt(e,t,n);break;default:Vt(e,t,n)}}function Wc(e){var t=e.updateQueue;if(t!==null){e.updateQueue=null;var n=e.stateNode;n===null&&(n=e.stateNode=new Yp),t.forEach(function(i){var a=ih.bind(null,e,i);n.has(i)||(n.add(i),i.then(a,a))})}}function gt(e,t){var n=t.deletions;if(n!==null)for(var i=0;i<n.length;i++){var a=n[i];try{var r=e,s=t,d=s;e:for(;d!==null;){switch(d.tag){case 5:ze=d.stateNode,mt=!1;break e;case 3:ze=d.stateNode.containerInfo,mt=!0;break e;case 4:ze=d.stateNode.containerInfo,mt=!0;break e}d=d.return}if(ze===null)throw Error(l(160));jc(r,s,a),ze=null,mt=!1;var p=a.alternate;p!==null&&(p.return=null),a.return=null}catch(x){Ce(a,t,x)}}if(t.subtreeFlags&12854)for(t=t.child;t!==null;)Fc(t,e),t=t.sibling}function Fc(e,t){var n=e.alternate,i=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:if(gt(t,e),kt(e),i&4){try{Ri(3,e,e.return),_a(3,e)}catch(F){Ce(e,e.return,F)}try{Ri(5,e,e.return)}catch(F){Ce(e,e.return,F)}}break;case 1:gt(t,e),kt(e),i&512&&n!==null&&Bn(n,n.return);break;case 5:if(gt(t,e),kt(e),i&512&&n!==null&&Bn(n,n.return),e.flags&32){var a=e.stateNode;try{Kn(a,"")}catch(F){Ce(e,e.return,F)}}if(i&4&&(a=e.stateNode,a!=null)){var r=e.memoizedProps,s=n!==null?n.memoizedProps:r,d=e.type,p=e.updateQueue;if(e.updateQueue=null,p!==null)try{d==="input"&&r.type==="radio"&&r.name!=null&&vs(a,r),cr(d,s);var x=cr(d,r);for(s=0;s<p.length;s+=2){var I=p[s],O=p[s+1];I==="style"?Cs(a,O):I==="dangerouslySetInnerHTML"?ks(a,O):I==="children"?Kn(a,O):K(a,I,O,x)}switch(d){case"input":ar(a,r);break;case"textarea":xs(a,r);break;case"select":var N=a._wrapperState.wasMultiple;a._wrapperState.wasMultiple=!!r.multiple;var L=r.value;L!=null?yn(a,!!r.multiple,L,!1):N!==!!r.multiple&&(r.defaultValue!=null?yn(a,!!r.multiple,r.defaultValue,!0):yn(a,!!r.multiple,r.multiple?[]:"",!1))}a[yi]=r}catch(F){Ce(e,e.return,F)}}break;case 6:if(gt(t,e),kt(e),i&4){if(e.stateNode===null)throw Error(l(162));a=e.stateNode,r=e.memoizedProps;try{a.nodeValue=r}catch(F){Ce(e,e.return,F)}}break;case 3:if(gt(t,e),kt(e),i&4&&n!==null&&n.memoizedState.isDehydrated)try{li(t.containerInfo)}catch(F){Ce(e,e.return,F)}break;case 4:gt(t,e),kt(e);break;case 13:gt(t,e),kt(e),a=e.child,a.flags&8192&&(r=a.memoizedState!==null,a.stateNode.isHidden=r,!r||a.alternate!==null&&a.alternate.memoizedState!==null||(zo=Te())),i&4&&Wc(e);break;case 22:if(I=n!==null&&n.memoizedState!==null,e.mode&1?(We=(x=We)||I,gt(t,e),We=x):gt(t,e),kt(e),i&8192){if(x=e.memoizedState!==null,(e.stateNode.isHidden=x)&&!I&&(e.mode&1)!==0)for(B=e,I=e.child;I!==null;){for(O=B=I;B!==null;){switch(N=B,L=N.child,N.tag){case 0:case 11:case 14:case 15:Ri(4,N,N.return);break;case 1:Bn(N,N.return);var j=N.stateNode;if(typeof j.componentWillUnmount=="function"){i=N,n=N.return;try{t=i,j.props=t.memoizedProps,j.state=t.memoizedState,j.componentWillUnmount()}catch(F){Ce(i,n,F)}}break;case 5:Bn(N,N.return);break;case 22:if(N.memoizedState!==null){Hc(O);continue}}L!==null?(L.return=N,B=L):Hc(O)}I=I.sibling}e:for(I=null,O=e;;){if(O.tag===5){if(I===null){I=O;try{a=O.stateNode,x?(r=a.style,typeof r.setProperty=="function"?r.setProperty("display","none","important"):r.display="none"):(d=O.stateNode,p=O.memoizedProps.style,s=p!=null&&p.hasOwnProperty("display")?p.display:null,d.style.display=As("display",s))}catch(F){Ce(e,e.return,F)}}}else if(O.tag===6){if(I===null)try{O.stateNode.nodeValue=x?"":O.memoizedProps}catch(F){Ce(e,e.return,F)}}else if((O.tag!==22&&O.tag!==23||O.memoizedState===null||O===e)&&O.child!==null){O.child.return=O,O=O.child;continue}if(O===e)break e;for(;O.sibling===null;){if(O.return===null||O.return===e)break e;I===O&&(I=null),O=O.return}I===O&&(I=null),O.sibling.return=O.return,O=O.sibling}}break;case 19:gt(t,e),kt(e),i&4&&Wc(e);break;case 21:break;default:gt(t,e),kt(e)}}function kt(e){var t=e.flags;if(t&2){try{e:{for(var n=e.return;n!==null;){if(Uc(n)){var i=n;break e}n=n.return}throw Error(l(160))}switch(i.tag){case 5:var a=i.stateNode;i.flags&32&&(Kn(a,""),i.flags&=-33);var r=Bc(e);qo(e,r,a);break;case 3:case 4:var s=i.stateNode.containerInfo,d=Bc(e);Oo(e,d,s);break;default:throw Error(l(161))}}catch(p){Ce(e,e.return,p)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function Jp(e,t,n){B=e,Qc(e)}function Qc(e,t,n){for(var i=(e.mode&1)!==0;B!==null;){var a=B,r=a.child;if(a.tag===22&&i){var s=a.memoizedState!==null||qa;if(!s){var d=a.alternate,p=d!==null&&d.memoizedState!==null||We;d=qa;var x=We;if(qa=s,(We=p)&&!x)for(B=a;B!==null;)s=B,p=s.child,s.tag===22&&s.memoizedState!==null?Yc(a):p!==null?(p.return=s,B=p):Yc(a);for(;r!==null;)B=r,Qc(r),r=r.sibling;B=a,qa=d,We=x}Gc(e)}else(a.subtreeFlags&8772)!==0&&r!==null?(r.return=a,B=r):Gc(e)}}function Gc(e){for(;B!==null;){var t=B;if((t.flags&8772)!==0){var n=t.alternate;try{if((t.flags&8772)!==0)switch(t.tag){case 0:case 11:case 15:We||_a(5,t);break;case 1:var i=t.stateNode;if(t.flags&4&&!We)if(n===null)i.componentDidMount();else{var a=t.elementType===t.type?n.memoizedProps:ft(t.type,n.memoizedProps);i.componentDidUpdate(a,n.memoizedState,i.__reactInternalSnapshotBeforeUpdate)}var r=t.updateQueue;r!==null&&Hl(t,r,i);break;case 3:var s=t.updateQueue;if(s!==null){if(n=null,t.child!==null)switch(t.child.tag){case 5:n=t.child.stateNode;break;case 1:n=t.child.stateNode}Hl(t,s,n)}break;case 5:var d=t.stateNode;if(n===null&&t.flags&4){n=d;var p=t.memoizedProps;switch(t.type){case"button":case"input":case"select":case"textarea":p.autoFocus&&n.focus();break;case"img":p.src&&(n.src=p.src)}}break;case 6:break;case 4:break;case 12:break;case 13:if(t.memoizedState===null){var x=t.alternate;if(x!==null){var I=x.memoizedState;if(I!==null){var O=I.dehydrated;O!==null&&li(O)}}}break;case 19:case 17:case 21:case 22:case 23:case 25:break;default:throw Error(l(163))}We||t.flags&512&&Io(t)}catch(N){Ce(t,t.return,N)}}if(t===e){B=null;break}if(n=t.sibling,n!==null){n.return=t.return,B=n;break}B=t.return}}function Hc(e){for(;B!==null;){var t=B;if(t===e){B=null;break}var n=t.sibling;if(n!==null){n.return=t.return,B=n;break}B=t.return}}function Yc(e){for(;B!==null;){var t=B;try{switch(t.tag){case 0:case 11:case 15:var n=t.return;try{_a(4,t)}catch(p){Ce(t,n,p)}break;case 1:var i=t.stateNode;if(typeof i.componentDidMount=="function"){var a=t.return;try{i.componentDidMount()}catch(p){Ce(t,a,p)}}var r=t.return;try{Io(t)}catch(p){Ce(t,r,p)}break;case 5:var s=t.return;try{Io(t)}catch(p){Ce(t,s,p)}}}catch(p){Ce(t,t.return,p)}if(t===e){B=null;break}var d=t.sibling;if(d!==null){d.return=t.return,B=d;break}B=t.return}}var $p=Math.ceil,Pa=X.ReactCurrentDispatcher,_o=X.ReactCurrentOwner,ct=X.ReactCurrentBatchConfig,ae=0,qe=null,De=null,Me=0,at=0,jn=Ft(0),Re=0,Ii=null,hn=0,za=0,Po=0,Oi=null,Ke=null,zo=0,Wn=1/0,It=null,Ma=!1,Mo=null,Jt=null,La=!1,$t=null,Ua=0,qi=0,Lo=null,Ba=-1,ja=0;function He(){return(ae&6)!==0?Te():Ba!==-1?Ba:Ba=Te()}function Kt(e){return(e.mode&1)===0?1:(ae&2)!==0&&Me!==0?Me&-Me:qp.transition!==null?(ja===0&&(ja=Bs()),ja):(e=de,e!==0||(e=window.event,e=e===void 0?16:Js(e.type)),e)}function vt(e,t,n,i){if(50<qi)throw qi=0,Lo=null,Error(l(185));ii(e,n,i),((ae&2)===0||e!==qe)&&(e===qe&&((ae&2)===0&&(za|=n),Re===4&&Xt(e,Me)),Xe(e,i),n===1&&ae===0&&(t.mode&1)===0&&(Wn=Te()+500,ma&&Gt()))}function Xe(e,t){var n=e.callbackNode;qu(e,t);var i=$i(e,e===qe?Me:0);if(i===0)n!==null&&Ms(n),e.callbackNode=null,e.callbackPriority=0;else if(t=i&-i,e.callbackPriority!==t){if(n!=null&&Ms(n),t===1)e.tag===0?Op(Jc.bind(null,e)):_l(Jc.bind(null,e)),Dp(function(){(ae&6)===0&&Gt()}),n=null;else{switch(js(i)){case 1:n=gr;break;case 4:n=Ls;break;case 16:n=Hi;break;case 536870912:n=Us;break;default:n=Hi}n=id(n,Vc.bind(null,e))}e.callbackPriority=t,e.callbackNode=n}}function Vc(e,t){if(Ba=-1,ja=0,(ae&6)!==0)throw Error(l(327));var n=e.callbackNode;if(Fn()&&e.callbackNode!==n)return null;var i=$i(e,e===qe?Me:0);if(i===0)return null;if((i&30)!==0||(i&e.expiredLanes)!==0||t)t=Wa(e,i);else{t=i;var a=ae;ae|=2;var r=Kc();(qe!==e||Me!==t)&&(It=null,Wn=Te()+500,mn(e,t));do try{Zp();break}catch(d){$c(e,d)}while(!0);to(),Pa.current=r,ae=a,De!==null?t=0:(qe=null,Me=0,t=Re)}if(t!==0){if(t===2&&(a=vr(e),a!==0&&(i=a,t=Uo(e,a))),t===1)throw n=Ii,mn(e,0),Xt(e,i),Xe(e,Te()),n;if(t===6)Xt(e,i);else{if(a=e.current.alternate,(i&30)===0&&!Kp(a)&&(t=Wa(e,i),t===2&&(r=vr(e),r!==0&&(i=r,t=Uo(e,r))),t===1))throw n=Ii,mn(e,0),Xt(e,i),Xe(e,Te()),n;switch(e.finishedWork=a,e.finishedLanes=i,t){case 0:case 1:throw Error(l(345));case 2:gn(e,Ke,It);break;case 3:if(Xt(e,i),(i&130023424)===i&&(t=zo+500-Te(),10<t)){if($i(e,0)!==0)break;if(a=e.suspendedLanes,(a&i)!==i){He(),e.pingedLanes|=e.suspendedLanes&a;break}e.timeoutHandle=Qr(gn.bind(null,e,Ke,It),t);break}gn(e,Ke,It);break;case 4:if(Xt(e,i),(i&4194240)===i)break;for(t=e.eventTimes,a=-1;0<i;){var s=31-ut(i);r=1<<s,s=t[s],s>a&&(a=s),i&=~r}if(i=a,i=Te()-i,i=(120>i?120:480>i?480:1080>i?1080:1920>i?1920:3e3>i?3e3:4320>i?4320:1960*$p(i/1960))-i,10<i){e.timeoutHandle=Qr(gn.bind(null,e,Ke,It),i);break}gn(e,Ke,It);break;case 5:gn(e,Ke,It);break;default:throw Error(l(329))}}}return Xe(e,Te()),e.callbackNode===n?Vc.bind(null,e):null}function Uo(e,t){var n=Oi;return e.current.memoizedState.isDehydrated&&(mn(e,t).flags|=256),e=Wa(e,t),e!==2&&(t=Ke,Ke=n,t!==null&&Bo(t)),e}function Bo(e){Ke===null?Ke=e:Ke.push.apply(Ke,e)}function Kp(e){for(var t=e;;){if(t.flags&16384){var n=t.updateQueue;if(n!==null&&(n=n.stores,n!==null))for(var i=0;i<n.length;i++){var a=n[i],r=a.getSnapshot;a=a.value;try{if(!pt(r(),a))return!1}catch{return!1}}}if(n=t.child,t.subtreeFlags&16384&&n!==null)n.return=t,t=n;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function Xt(e,t){for(t&=~Po,t&=~za,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var n=31-ut(t),i=1<<n;e[n]=-1,t&=~i}}function Jc(e){if((ae&6)!==0)throw Error(l(327));Fn();var t=$i(e,0);if((t&1)===0)return Xe(e,Te()),null;var n=Wa(e,t);if(e.tag!==0&&n===2){var i=vr(e);i!==0&&(t=i,n=Uo(e,i))}if(n===1)throw n=Ii,mn(e,0),Xt(e,t),Xe(e,Te()),n;if(n===6)throw Error(l(345));return e.finishedWork=e.current.alternate,e.finishedLanes=t,gn(e,Ke,It),Xe(e,Te()),null}function jo(e,t){var n=ae;ae|=1;try{return e(t)}finally{ae=n,ae===0&&(Wn=Te()+500,ma&&Gt())}}function fn(e){$t!==null&&$t.tag===0&&(ae&6)===0&&Fn();var t=ae;ae|=1;var n=ct.transition,i=de;try{if(ct.transition=null,de=1,e)return e()}finally{de=i,ct.transition=n,ae=t,(ae&6)===0&&Gt()}}function Wo(){at=jn.current,we(jn)}function mn(e,t){e.finishedWork=null,e.finishedLanes=0;var n=e.timeoutHandle;if(n!==-1&&(e.timeoutHandle=-1,Ep(n)),De!==null)for(n=De.return;n!==null;){var i=n;switch($r(i),i.tag){case 1:i=i.type.childContextTypes,i!=null&&ha();break;case 3:Ln(),we(Ve),we(Ue),co();break;case 5:so(i);break;case 4:Ln();break;case 13:we(be);break;case 19:we(be);break;case 10:no(i.type._context);break;case 22:case 23:Wo()}n=n.return}if(qe=e,De=e=Zt(e.current,null),Me=at=t,Re=0,Ii=null,Po=za=hn=0,Ke=Oi=null,dn!==null){for(t=0;t<dn.length;t++)if(n=dn[t],i=n.interleaved,i!==null){n.interleaved=null;var a=i.next,r=n.pending;if(r!==null){var s=r.next;r.next=a,i.next=s}n.pending=i}dn=null}return e}function $c(e,t){do{var n=De;try{if(to(),Ca.current=Na,Ta){for(var i=ke.memoizedState;i!==null;){var a=i.queue;a!==null&&(a.pending=null),i=i.next}Ta=!1}if(pn=0,Oe=Ne=ke=null,Ci=!1,Ti=0,_o.current=null,n===null||n.return===null){Re=1,Ii=t,De=null;break}e:{var r=e,s=n.return,d=n,p=t;if(t=Me,d.flags|=32768,p!==null&&typeof p=="object"&&typeof p.then=="function"){var x=p,I=d,O=I.tag;if((I.mode&1)===0&&(O===0||O===11||O===15)){var N=I.alternate;N?(I.updateQueue=N.updateQueue,I.memoizedState=N.memoizedState,I.lanes=N.lanes):(I.updateQueue=null,I.memoizedState=null)}var L=Sc(s);if(L!==null){L.flags&=-257,bc(L,s,d,r,t),L.mode&1&&xc(r,x,t),t=L,p=x;var j=t.updateQueue;if(j===null){var F=new Set;F.add(p),t.updateQueue=F}else j.add(p);break e}else{if((t&1)===0){xc(r,x,t),Fo();break e}p=Error(l(426))}}else if(Se&&d.mode&1){var Ee=Sc(s);if(Ee!==null){(Ee.flags&65536)===0&&(Ee.flags|=256),bc(Ee,s,d,r,t),Zr(Un(p,d));break e}}r=p=Un(p,d),Re!==4&&(Re=2),Oi===null?Oi=[r]:Oi.push(r),r=s;do{switch(r.tag){case 3:r.flags|=65536,t&=-t,r.lanes|=t;var v=wc(r,p,t);Gl(r,v);break e;case 1:d=p;var f=r.type,w=r.stateNode;if((r.flags&128)===0&&(typeof f.getDerivedStateFromError=="function"||w!==null&&typeof w.componentDidCatch=="function"&&(Jt===null||!Jt.has(w)))){r.flags|=65536,t&=-t,r.lanes|=t;var _=yc(r,d,t);Gl(r,_);break e}}r=r.return}while(r!==null)}Zc(n)}catch(G){t=G,De===n&&n!==null&&(De=n=n.return);continue}break}while(!0)}function Kc(){var e=Pa.current;return Pa.current=Na,e===null?Na:e}function Fo(){(Re===0||Re===3||Re===2)&&(Re=4),qe===null||(hn&268435455)===0&&(za&268435455)===0||Xt(qe,Me)}function Wa(e,t){var n=ae;ae|=2;var i=Kc();(qe!==e||Me!==t)&&(It=null,mn(e,t));do try{Xp();break}catch(a){$c(e,a)}while(!0);if(to(),ae=n,Pa.current=i,De!==null)throw Error(l(261));return qe=null,Me=0,Re}function Xp(){for(;De!==null;)Xc(De)}function Zp(){for(;De!==null&&!Au();)Xc(De)}function Xc(e){var t=nd(e.alternate,e,at);e.memoizedProps=e.pendingProps,t===null?Zc(e):De=t,_o.current=null}function Zc(e){var t=e;do{var n=t.alternate;if(e=t.return,(t.flags&32768)===0){if(n=Gp(n,t,at),n!==null){De=n;return}}else{if(n=Hp(n,t),n!==null){n.flags&=32767,De=n;return}if(e!==null)e.flags|=32768,e.subtreeFlags=0,e.deletions=null;else{Re=6,De=null;return}}if(t=t.sibling,t!==null){De=t;return}De=t=e}while(t!==null);Re===0&&(Re=5)}function gn(e,t,n){var i=de,a=ct.transition;try{ct.transition=null,de=1,eh(e,t,n,i)}finally{ct.transition=a,de=i}return null}function eh(e,t,n,i){do Fn();while($t!==null);if((ae&6)!==0)throw Error(l(327));n=e.finishedWork;var a=e.finishedLanes;if(n===null)return null;if(e.finishedWork=null,e.finishedLanes=0,n===e.current)throw Error(l(177));e.callbackNode=null,e.callbackPriority=0;var r=n.lanes|n.childLanes;if(_u(e,r),e===qe&&(De=qe=null,Me=0),(n.subtreeFlags&2064)===0&&(n.flags&2064)===0||La||(La=!0,id(Hi,function(){return Fn(),null})),r=(n.flags&15990)!==0,(n.subtreeFlags&15990)!==0||r){r=ct.transition,ct.transition=null;var s=de;de=1;var d=ae;ae|=4,_o.current=null,Vp(e,n),Fc(n,e),xp(Wr),Zi=!!jr,Wr=jr=null,e.current=n,Jp(n),Cu(),ae=d,de=s,ct.transition=r}else e.current=n;if(La&&(La=!1,$t=e,Ua=a),r=e.pendingLanes,r===0&&(Jt=null),Du(n.stateNode),Xe(e,Te()),t!==null)for(i=e.onRecoverableError,n=0;n<t.length;n++)a=t[n],i(a.value,{componentStack:a.stack,digest:a.digest});if(Ma)throw Ma=!1,e=Mo,Mo=null,e;return(Ua&1)!==0&&e.tag!==0&&Fn(),r=e.pendingLanes,(r&1)!==0?e===Lo?qi++:(qi=0,Lo=e):qi=0,Gt(),null}function Fn(){if($t!==null){var e=js(Ua),t=ct.transition,n=de;try{if(ct.transition=null,de=16>e?16:e,$t===null)var i=!1;else{if(e=$t,$t=null,Ua=0,(ae&6)!==0)throw Error(l(331));var a=ae;for(ae|=4,B=e.current;B!==null;){var r=B,s=r.child;if((B.flags&16)!==0){var d=r.deletions;if(d!==null){for(var p=0;p<d.length;p++){var x=d[p];for(B=x;B!==null;){var I=B;switch(I.tag){case 0:case 11:case 15:Ri(8,I,r)}var O=I.child;if(O!==null)O.return=I,B=O;else for(;B!==null;){I=B;var N=I.sibling,L=I.return;if(Lc(I),I===x){B=null;break}if(N!==null){N.return=L,B=N;break}B=L}}}var j=r.alternate;if(j!==null){var F=j.child;if(F!==null){j.child=null;do{var Ee=F.sibling;F.sibling=null,F=Ee}while(F!==null)}}B=r}}if((r.subtreeFlags&2064)!==0&&s!==null)s.return=r,B=s;else e:for(;B!==null;){if(r=B,(r.flags&2048)!==0)switch(r.tag){case 0:case 11:case 15:Ri(9,r,r.return)}var v=r.sibling;if(v!==null){v.return=r.return,B=v;break e}B=r.return}}var f=e.current;for(B=f;B!==null;){s=B;var w=s.child;if((s.subtreeFlags&2064)!==0&&w!==null)w.return=s,B=w;else e:for(s=f;B!==null;){if(d=B,(d.flags&2048)!==0)try{switch(d.tag){case 0:case 11:case 15:_a(9,d)}}catch(G){Ce(d,d.return,G)}if(d===s){B=null;break e}var _=d.sibling;if(_!==null){_.return=d.return,B=_;break e}B=d.return}}if(ae=a,Gt(),yt&&typeof yt.onPostCommitFiberRoot=="function")try{yt.onPostCommitFiberRoot(Yi,e)}catch{}i=!0}return i}finally{de=n,ct.transition=t}}return!1}function ed(e,t,n){t=Un(n,t),t=wc(e,t,1),e=Yt(e,t,1),t=He(),e!==null&&(ii(e,1,t),Xe(e,t))}function Ce(e,t,n){if(e.tag===3)ed(e,e,n);else for(;t!==null;){if(t.tag===3){ed(t,e,n);break}else if(t.tag===1){var i=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof i.componentDidCatch=="function"&&(Jt===null||!Jt.has(i))){e=Un(n,e),e=yc(t,e,1),t=Yt(t,e,1),e=He(),t!==null&&(ii(t,1,e),Xe(t,e));break}}t=t.return}}function th(e,t,n){var i=e.pingCache;i!==null&&i.delete(t),t=He(),e.pingedLanes|=e.suspendedLanes&n,qe===e&&(Me&n)===n&&(Re===4||Re===3&&(Me&130023424)===Me&&500>Te()-zo?mn(e,0):Po|=n),Xe(e,t)}function td(e,t){t===0&&((e.mode&1)===0?t=1:(t=Ji,Ji<<=1,(Ji&130023424)===0&&(Ji=4194304)));var n=He();e=Dt(e,t),e!==null&&(ii(e,t,n),Xe(e,n))}function nh(e){var t=e.memoizedState,n=0;t!==null&&(n=t.retryLane),td(e,n)}function ih(e,t){var n=0;switch(e.tag){case 13:var i=e.stateNode,a=e.memoizedState;a!==null&&(n=a.retryLane);break;case 19:i=e.stateNode;break;default:throw Error(l(314))}i!==null&&i.delete(t),td(e,n)}var nd;nd=function(e,t,n){if(e!==null)if(e.memoizedProps!==t.pendingProps||Ve.current)$e=!0;else{if((e.lanes&n)===0&&(t.flags&128)===0)return $e=!1,Qp(e,t,n);$e=(e.flags&131072)!==0}else $e=!1,Se&&(t.flags&1048576)!==0&&Pl(t,va,t.index);switch(t.lanes=0,t.tag){case 2:var i=t.type;Oa(e,t),e=t.pendingProps;var a=In(t,Ue.current);Mn(t,n),a=ho(null,t,i,e,a,n);var r=fo();return t.flags|=1,typeof a=="object"&&a!==null&&typeof a.render=="function"&&a.$$typeof===void 0?(t.tag=1,t.memoizedState=null,t.updateQueue=null,Je(i)?(r=!0,fa(t)):r=!1,t.memoizedState=a.state!==null&&a.state!==void 0?a.state:null,ro(t),a.updater=Ra,t.stateNode=a,a._reactInternals=t,xo(t,i,e,n),t=Ao(null,t,i,!0,r,n)):(t.tag=0,Se&&r&&Jr(t),Ge(null,t,a,n),t=t.child),t;case 16:i=t.elementType;e:{switch(Oa(e,t),e=t.pendingProps,a=i._init,i=a(i._payload),t.type=i,a=t.tag=rh(i),e=ft(i,e),a){case 0:t=ko(null,t,i,e,n);break e;case 1:t=Dc(null,t,i,e,n);break e;case 11:t=kc(null,t,i,e,n);break e;case 14:t=Ac(null,t,i,ft(i.type,e),n);break e}throw Error(l(306,i,""))}return t;case 0:return i=t.type,a=t.pendingProps,a=t.elementType===i?a:ft(i,a),ko(e,t,i,a,n);case 1:return i=t.type,a=t.pendingProps,a=t.elementType===i?a:ft(i,a),Dc(e,t,i,a,n);case 3:e:{if(Nc(t),e===null)throw Error(l(387));i=t.pendingProps,r=t.memoizedState,a=r.element,Ql(e,t),ka(t,i,null,n);var s=t.memoizedState;if(i=s.element,r.isDehydrated)if(r={element:i,isDehydrated:!1,cache:s.cache,pendingSuspenseBoundaries:s.pendingSuspenseBoundaries,transitions:s.transitions},t.updateQueue.baseState=r,t.memoizedState=r,t.flags&256){a=Un(Error(l(423)),t),t=Rc(e,t,i,n,a);break e}else if(i!==a){a=Un(Error(l(424)),t),t=Rc(e,t,i,n,a);break e}else for(it=Wt(t.stateNode.containerInfo.firstChild),nt=t,Se=!0,ht=null,n=Wl(t,null,i,n),t.child=n;n;)n.flags=n.flags&-3|4096,n=n.sibling;else{if(_n(),i===a){t=Rt(e,t,n);break e}Ge(e,t,i,n)}t=t.child}return t;case 5:return Yl(t),e===null&&Xr(t),i=t.type,a=t.pendingProps,r=e!==null?e.memoizedProps:null,s=a.children,Fr(i,a)?s=null:r!==null&&Fr(i,r)&&(t.flags|=32),Ec(e,t),Ge(e,t,s,n),t.child;case 6:return e===null&&Xr(t),null;case 13:return Ic(e,t,n);case 4:return oo(t,t.stateNode.containerInfo),i=t.pendingProps,e===null?t.child=Pn(t,null,i,n):Ge(e,t,i,n),t.child;case 11:return i=t.type,a=t.pendingProps,a=t.elementType===i?a:ft(i,a),kc(e,t,i,a,n);case 7:return Ge(e,t,t.pendingProps,n),t.child;case 8:return Ge(e,t,t.pendingProps.children,n),t.child;case 12:return Ge(e,t,t.pendingProps.children,n),t.child;case 10:e:{if(i=t.type._context,a=t.pendingProps,r=t.memoizedProps,s=a.value,he(xa,i._currentValue),i._currentValue=s,r!==null)if(pt(r.value,s)){if(r.children===a.children&&!Ve.current){t=Rt(e,t,n);break e}}else for(r=t.child,r!==null&&(r.return=t);r!==null;){var d=r.dependencies;if(d!==null){s=r.child;for(var p=d.firstContext;p!==null;){if(p.context===i){if(r.tag===1){p=Nt(-1,n&-n),p.tag=2;var x=r.updateQueue;if(x!==null){x=x.shared;var I=x.pending;I===null?p.next=p:(p.next=I.next,I.next=p),x.pending=p}}r.lanes|=n,p=r.alternate,p!==null&&(p.lanes|=n),io(r.return,n,t),d.lanes|=n;break}p=p.next}}else if(r.tag===10)s=r.type===t.type?null:r.child;else if(r.tag===18){if(s=r.return,s===null)throw Error(l(341));s.lanes|=n,d=s.alternate,d!==null&&(d.lanes|=n),io(s,n,t),s=r.sibling}else s=r.child;if(s!==null)s.return=r;else for(s=r;s!==null;){if(s===t){s=null;break}if(r=s.sibling,r!==null){r.return=s.return,s=r;break}s=s.return}r=s}Ge(e,t,a.children,n),t=t.child}return t;case 9:return a=t.type,i=t.pendingProps.children,Mn(t,n),a=st(a),i=i(a),t.flags|=1,Ge(e,t,i,n),t.child;case 14:return i=t.type,a=ft(i,t.pendingProps),a=ft(i.type,a),Ac(e,t,i,a,n);case 15:return Cc(e,t,t.type,t.pendingProps,n);case 17:return i=t.type,a=t.pendingProps,a=t.elementType===i?a:ft(i,a),Oa(e,t),t.tag=1,Je(i)?(e=!0,fa(t)):e=!1,Mn(t,n),gc(t,i,a),xo(t,i,a,n),Ao(null,t,i,!0,e,n);case 19:return qc(e,t,n);case 22:return Tc(e,t,n)}throw Error(l(156,t.tag))};function id(e,t){return zs(e,t)}function ah(e,t,n,i){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=i,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function dt(e,t,n,i){return new ah(e,t,n,i)}function Qo(e){return e=e.prototype,!(!e||!e.isReactComponent)}function rh(e){if(typeof e=="function")return Qo(e)?1:0;if(e!=null){if(e=e.$$typeof,e===ne)return 11;if(e===Qe)return 14}return 2}function Zt(e,t){var n=e.alternate;return n===null?(n=dt(e.tag,t,e.key,e.mode),n.elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.type=e.type,n.flags=0,n.subtreeFlags=0,n.deletions=null),n.flags=e.flags&14680064,n.childLanes=e.childLanes,n.lanes=e.lanes,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function Fa(e,t,n,i,a,r){var s=2;if(i=e,typeof e=="function")Qo(e)&&(s=1);else if(typeof e=="string")s=5;else e:switch(e){case Ae:return vn(n.children,a,r,t);case le:s=8,a|=8;break;case Ye:return e=dt(12,n,t,a|2),e.elementType=Ye,e.lanes=r,e;case Ie:return e=dt(13,n,t,a),e.elementType=Ie,e.lanes=r,e;case Pe:return e=dt(19,n,t,a),e.elementType=Pe,e.lanes=r,e;case ge:return Qa(n,a,r,t);default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case Le:s=10;break e;case Fe:s=9;break e;case ne:s=11;break e;case Qe:s=14;break e;case ue:s=16,i=null;break e}throw Error(l(130,e==null?e:typeof e,""))}return t=dt(s,n,t,a),t.elementType=e,t.type=i,t.lanes=r,t}function vn(e,t,n,i){return e=dt(7,e,i,t),e.lanes=n,e}function Qa(e,t,n,i){return e=dt(22,e,i,t),e.elementType=ge,e.lanes=n,e.stateNode={isHidden:!1},e}function Go(e,t,n){return e=dt(6,e,null,t),e.lanes=n,e}function Ho(e,t,n){return t=dt(4,e.children!==null?e.children:[],e.key,t),t.lanes=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function oh(e,t,n,i,a){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=wr(0),this.expirationTimes=wr(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=wr(0),this.identifierPrefix=i,this.onRecoverableError=a,this.mutableSourceEagerHydrationData=null}function Yo(e,t,n,i,a,r,s,d,p){return e=new oh(e,t,n,d,p),t===1?(t=1,r===!0&&(t|=8)):t=0,r=dt(3,null,null,t),e.current=r,r.stateNode=e,r.memoizedState={element:i,isDehydrated:n,cache:null,transitions:null,pendingSuspenseBoundaries:null},ro(r),e}function sh(e,t,n){var i=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:me,key:i==null?null:""+i,children:e,containerInfo:t,implementation:n}}function ad(e){if(!e)return Qt;e=e._reactInternals;e:{if(rn(e)!==e||e.tag!==1)throw Error(l(170));var t=e;do{switch(t.tag){case 3:t=t.stateNode.context;break e;case 1:if(Je(t.type)){t=t.stateNode.__reactInternalMemoizedMergedChildContext;break e}}t=t.return}while(t!==null);throw Error(l(171))}if(e.tag===1){var n=e.type;if(Je(n))return Ol(e,n,t)}return t}function rd(e,t,n,i,a,r,s,d,p){return e=Yo(n,i,!0,e,a,r,s,d,p),e.context=ad(null),n=e.current,i=He(),a=Kt(n),r=Nt(i,a),r.callback=t??null,Yt(n,r,a),e.current.lanes=a,ii(e,a,i),Xe(e,i),e}function Ga(e,t,n,i){var a=t.current,r=He(),s=Kt(a);return n=ad(n),t.context===null?t.context=n:t.pendingContext=n,t=Nt(r,s),t.payload={element:e},i=i===void 0?null:i,i!==null&&(t.callback=i),e=Yt(a,t,s),e!==null&&(vt(e,a,s,r),ba(e,a,s)),s}function Ha(e){if(e=e.current,!e.child)return null;switch(e.child.tag){case 5:return e.child.stateNode;default:return e.child.stateNode}}function od(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var n=e.retryLane;e.retryLane=n!==0&&n<t?n:t}}function Vo(e,t){od(e,t),(e=e.alternate)&&od(e,t)}function lh(){return null}var sd=typeof reportError=="function"?reportError:function(e){console.error(e)};function Jo(e){this._internalRoot=e}Ya.prototype.render=Jo.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(l(409));Ga(e,t,null,null)},Ya.prototype.unmount=Jo.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;fn(function(){Ga(null,e,null,null)}),t[At]=null}};function Ya(e){this._internalRoot=e}Ya.prototype.unstable_scheduleHydration=function(e){if(e){var t=Qs();e={blockedOn:null,target:e,priority:t};for(var n=0;n<Ut.length&&t!==0&&t<Ut[n].priority;n++);Ut.splice(n,0,e),n===0&&Ys(e)}};function $o(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function Va(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11&&(e.nodeType!==8||e.nodeValue!==" react-mount-point-unstable "))}function ld(){}function ch(e,t,n,i,a){if(a){if(typeof i=="function"){var r=i;i=function(){var x=Ha(s);r.call(x)}}var s=rd(t,i,e,0,null,!1,!1,"",ld);return e._reactRootContainer=s,e[At]=s.current,vi(e.nodeType===8?e.parentNode:e),fn(),s}for(;a=e.lastChild;)e.removeChild(a);if(typeof i=="function"){var d=i;i=function(){var x=Ha(p);d.call(x)}}var p=Yo(e,0,!1,null,null,!1,!1,"",ld);return e._reactRootContainer=p,e[At]=p.current,vi(e.nodeType===8?e.parentNode:e),fn(function(){Ga(t,p,n,i)}),p}function Ja(e,t,n,i,a){var r=n._reactRootContainer;if(r){var s=r;if(typeof a=="function"){var d=a;a=function(){var p=Ha(s);d.call(p)}}Ga(t,s,e,a)}else s=ch(n,t,e,a,i);return Ha(s)}Ws=function(e){switch(e.tag){case 3:var t=e.stateNode;if(t.current.memoizedState.isDehydrated){var n=ni(t.pendingLanes);n!==0&&(yr(t,n|1),Xe(t,Te()),(ae&6)===0&&(Wn=Te()+500,Gt()))}break;case 13:fn(function(){var i=Dt(e,1);if(i!==null){var a=He();vt(i,e,1,a)}}),Vo(e,1)}},xr=function(e){if(e.tag===13){var t=Dt(e,134217728);if(t!==null){var n=He();vt(t,e,134217728,n)}Vo(e,134217728)}},Fs=function(e){if(e.tag===13){var t=Kt(e),n=Dt(e,t);if(n!==null){var i=He();vt(n,e,t,i)}Vo(e,t)}},Qs=function(){return de},Gs=function(e,t){var n=de;try{return de=e,t()}finally{de=n}},pr=function(e,t,n){switch(t){case"input":if(ar(e,n),t=n.name,n.type==="radio"&&t!=null){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<n.length;t++){var i=n[t];if(i!==e&&i.form===e.form){var a=pa(i);if(!a)throw Error(l(90));ms(i),ar(i,a)}}}break;case"textarea":xs(e,n);break;case"select":t=n.value,t!=null&&yn(e,!!n.multiple,t,!1)}},Ns=jo,Rs=fn;var dh={usingClientEntryPoint:!1,Events:[xi,Nn,pa,Es,Ds,jo]},_i={findFiberByHostInstance:on,bundleType:0,version:"18.3.1",rendererPackageName:"react-dom"},uh={bundleType:_i.bundleType,version:_i.version,rendererPackageName:_i.rendererPackageName,rendererConfig:_i.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:X.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return e=_s(e),e===null?null:e.stateNode},findFiberByHostInstance:_i.findFiberByHostInstance||lh,findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:"18.3.1-next-f1338f8080-20240426"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var $a=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!$a.isDisabled&&$a.supportsFiber)try{Yi=$a.inject(uh),yt=$a}catch{}}return Ze.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=dh,Ze.createPortal=function(e,t){var n=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!$o(t))throw Error(l(200));return sh(e,t,null,n)},Ze.createRoot=function(e,t){if(!$o(e))throw Error(l(299));var n=!1,i="",a=sd;return t!=null&&(t.unstable_strictMode===!0&&(n=!0),t.identifierPrefix!==void 0&&(i=t.identifierPrefix),t.onRecoverableError!==void 0&&(a=t.onRecoverableError)),t=Yo(e,1,!1,null,null,n,!1,i,a),e[At]=t.current,vi(e.nodeType===8?e.parentNode:e),new Jo(t)},Ze.findDOMNode=function(e){if(e==null)return null;if(e.nodeType===1)return e;var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(l(188)):(e=Object.keys(e).join(","),Error(l(268,e)));return e=_s(t),e=e===null?null:e.stateNode,e},Ze.flushSync=function(e){return fn(e)},Ze.hydrate=function(e,t,n){if(!Va(t))throw Error(l(200));return Ja(null,e,t,!0,n)},Ze.hydrateRoot=function(e,t,n){if(!$o(e))throw Error(l(405));var i=n!=null&&n.hydratedSources||null,a=!1,r="",s=sd;if(n!=null&&(n.unstable_strictMode===!0&&(a=!0),n.identifierPrefix!==void 0&&(r=n.identifierPrefix),n.onRecoverableError!==void 0&&(s=n.onRecoverableError)),t=rd(t,null,e,1,n??null,a,!1,r,s),e[At]=t.current,vi(e),i)for(e=0;e<i.length;e++)n=i[e],a=n._getVersion,a=a(n._source),t.mutableSourceEagerHydrationData==null?t.mutableSourceEagerHydrationData=[n,a]:t.mutableSourceEagerHydrationData.push(n,a);return new Ya(t)},Ze.render=function(e,t,n){if(!Va(t))throw Error(l(200));return Ja(null,e,t,!1,n)},Ze.unmountComponentAtNode=function(e){if(!Va(e))throw Error(l(40));return e._reactRootContainer?(fn(function(){Ja(null,null,e,!1,function(){e._reactRootContainer=null,e[At]=null})}),!0):!1},Ze.unstable_batchedUpdates=jo,Ze.unstable_renderSubtreeIntoContainer=function(e,t,n,i){if(!Va(n))throw Error(l(200));if(e==null||e._reactInternals===void 0)throw Error(l(38));return Ja(e,t,n,!1,i)},Ze.version="18.3.1-next-f1338f8080-20240426",Ze}var gd;function Ed(){if(gd)return Zo.exports;gd=1;function o(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(o)}catch(c){console.error(c)}}return o(),Zo.exports=xh(),Zo.exports}var vd;function Sh(){if(vd)return Ka;vd=1;var o=Ed();return Ka.createRoot=o.createRoot,Ka.hydrateRoot=o.hydrateRoot,Ka}var bh=Sh();function Dd(o){var c,l,u="";if(typeof o=="string"||typeof o=="number")u+=o;else if(typeof o=="object")if(Array.isArray(o)){var h=o.length;for(c=0;c<h;c++)o[c]&&(l=Dd(o[c]))&&(u&&(u+=" "),u+=l)}else for(l in o)o[l]&&(u&&(u+=" "),u+=l);return u}function Nd(){for(var o,c,l=0,u="",h=arguments.length;l<h;l++)(o=arguments[l])&&(c=Dd(o))&&(u&&(u+=" "),u+=c);return u}const cs="-",kh=o=>{const c=Ch(o),{conflictingClassGroups:l,conflictingClassGroupModifiers:u}=o;return{getClassGroupId:S=>{const y=S.split(cs);return y[0]===""&&y.length!==1&&y.shift(),Rd(y,c)||Ah(S)},getConflictingClassGroupIds:(S,y)=>{const k=l[S]||[];return y&&u[S]?[...k,...u[S]]:k}}},Rd=(o,c)=>{var S;if(o.length===0)return c.classGroupId;const l=o[0],u=c.nextPart.get(l),h=u?Rd(o.slice(1),u):void 0;if(h)return h;if(c.validators.length===0)return;const g=o.join(cs);return(S=c.validators.find(({validator:y})=>y(g)))==null?void 0:S.classGroupId},wd=/^\[(.+)\]$/,Ah=o=>{if(wd.test(o)){const c=wd.exec(o)[1],l=c==null?void 0:c.substring(0,c.indexOf(":"));if(l)return"arbitrary.."+l}},Ch=o=>{const{theme:c,prefix:l}=o,u={nextPart:new Map,validators:[]};return Eh(Object.entries(o.classGroups),l).forEach(([g,S])=>{as(S,u,g,c)}),u},as=(o,c,l,u)=>{o.forEach(h=>{if(typeof h=="string"){const g=h===""?c:yd(c,h);g.classGroupId=l;return}if(typeof h=="function"){if(Th(h)){as(h(u),c,l,u);return}c.validators.push({validator:h,classGroupId:l});return}Object.entries(h).forEach(([g,S])=>{as(S,yd(c,g),l,u)})})},yd=(o,c)=>{let l=o;return c.split(cs).forEach(u=>{l.nextPart.has(u)||l.nextPart.set(u,{nextPart:new Map,validators:[]}),l=l.nextPart.get(u)}),l},Th=o=>o.isThemeGetter,Eh=(o,c)=>c?o.map(([l,u])=>{const h=u.map(g=>typeof g=="string"?c+g:typeof g=="object"?Object.fromEntries(Object.entries(g).map(([S,y])=>[c+S,y])):g);return[l,h]}):o,Dh=o=>{if(o<1)return{get:()=>{},set:()=>{}};let c=0,l=new Map,u=new Map;const h=(g,S)=>{l.set(g,S),c++,c>o&&(c=0,u=l,l=new Map)};return{get(g){let S=l.get(g);if(S!==void 0)return S;if((S=u.get(g))!==void 0)return h(g,S),S},set(g,S){l.has(g)?l.set(g,S):h(g,S)}}},Id="!",Nh=o=>{const{separator:c,experimentalParseClassName:l}=o,u=c.length===1,h=c[0],g=c.length,S=y=>{const k=[];let A=0,T=0,E;for(let C=0;C<y.length;C++){let Y=y[C];if(A===0){if(Y===h&&(u||y.slice(C,C+g)===c)){k.push(y.slice(T,C)),T=C+g;continue}if(Y==="/"){E=C;continue}}Y==="["?A++:Y==="]"&&A--}const q=k.length===0?y:y.substring(T),z=q.startsWith(Id),W=z?q.substring(1):q,P=E&&E>T?E-T:void 0;return{modifiers:k,hasImportantModifier:z,baseClassName:W,maybePostfixModifierPosition:P}};return l?y=>l({className:y,parseClassName:S}):S},Rh=o=>{if(o.length<=1)return o;const c=[];let l=[];return o.forEach(u=>{u[0]==="["?(c.push(...l.sort(),u),l=[]):l.push(u)}),c.push(...l.sort()),c},Ih=o=>({cache:Dh(o.cacheSize),parseClassName:Nh(o),...kh(o)}),Oh=/\s+/,qh=(o,c)=>{const{parseClassName:l,getClassGroupId:u,getConflictingClassGroupIds:h}=c,g=[],S=o.trim().split(Oh);let y="";for(let k=S.length-1;k>=0;k-=1){const A=S[k],{modifiers:T,hasImportantModifier:E,baseClassName:q,maybePostfixModifierPosition:z}=l(A);let W=!!z,P=u(W?q.substring(0,z):q);if(!P){if(!W){y=A+(y.length>0?" "+y:y);continue}if(P=u(q),!P){y=A+(y.length>0?" "+y:y);continue}W=!1}const C=Rh(T).join(":"),Y=E?C+Id:C,Q=Y+P;if(g.includes(Q))continue;g.push(Q);const K=h(P,W);for(let X=0;X<K.length;++X){const fe=K[X];g.push(Y+fe)}y=A+(y.length>0?" "+y:y)}return y};function _h(){let o=0,c,l,u="";for(;o<arguments.length;)(c=arguments[o++])&&(l=Od(c))&&(u&&(u+=" "),u+=l);return u}const Od=o=>{if(typeof o=="string")return o;let c,l="";for(let u=0;u<o.length;u++)o[u]&&(c=Od(o[u]))&&(l&&(l+=" "),l+=c);return l};function Ph(o,...c){let l,u,h,g=S;function S(k){const A=c.reduce((T,E)=>E(T),o());return l=Ih(A),u=l.cache.get,h=l.cache.set,g=y,y(k)}function y(k){const A=u(k);if(A)return A;const T=qh(k,l);return h(k,T),T}return function(){return g(_h.apply(null,arguments))}}const ye=o=>{const c=l=>l[o]||[];return c.isThemeGetter=!0,c},qd=/^\[(?:([a-z-]+):)?(.+)\]$/i,zh=/^\d+\/\d+$/,Mh=new Set(["px","full","screen"]),Lh=/^(\d+(\.\d+)?)?(xs|sm|md|lg|xl)$/,Uh=/\d+(%|px|r?em|[sdl]?v([hwib]|min|max)|pt|pc|in|cm|mm|cap|ch|ex|r?lh|cq(w|h|i|b|min|max))|\b(calc|min|max|clamp)\(.+\)|^0$/,Bh=/^(rgba?|hsla?|hwb|(ok)?(lab|lch))\(.+\)$/,jh=/^(inset_)?-?((\d+)?\.?(\d+)[a-z]+|0)_-?((\d+)?\.?(\d+)[a-z]+|0)/,Wh=/^(url|image|image-set|cross-fade|element|(repeating-)?(linear|radial|conic)-gradient)\(.+\)$/,Ot=o=>Qn(o)||Mh.has(o)||zh.test(o),tn=o=>Jn(o,"length",$h),Qn=o=>!!o&&!Number.isNaN(Number(o)),ns=o=>Jn(o,"number",Qn),zi=o=>!!o&&Number.isInteger(Number(o)),Fh=o=>o.endsWith("%")&&Qn(o.slice(0,-1)),Z=o=>qd.test(o),nn=o=>Lh.test(o),Qh=new Set(["length","size","percentage"]),Gh=o=>Jn(o,Qh,_d),Hh=o=>Jn(o,"position",_d),Yh=new Set(["image","url"]),Vh=o=>Jn(o,Yh,Xh),Jh=o=>Jn(o,"",Kh),Mi=()=>!0,Jn=(o,c,l)=>{const u=qd.exec(o);return u?u[1]?typeof c=="string"?u[1]===c:c.has(u[1]):l(u[2]):!1},$h=o=>Uh.test(o)&&!Bh.test(o),_d=()=>!1,Kh=o=>jh.test(o),Xh=o=>Wh.test(o),Zh=()=>{const o=ye("colors"),c=ye("spacing"),l=ye("blur"),u=ye("brightness"),h=ye("borderColor"),g=ye("borderRadius"),S=ye("borderSpacing"),y=ye("borderWidth"),k=ye("contrast"),A=ye("grayscale"),T=ye("hueRotate"),E=ye("invert"),q=ye("gap"),z=ye("gradientColorStops"),W=ye("gradientColorStopPositions"),P=ye("inset"),C=ye("margin"),Y=ye("opacity"),Q=ye("padding"),K=ye("saturate"),X=ye("scale"),fe=ye("sepia"),me=ye("skew"),Ae=ye("space"),le=ye("translate"),Ye=()=>["auto","contain","none"],Le=()=>["auto","hidden","clip","visible","scroll"],Fe=()=>["auto",Z,c],ne=()=>[Z,c],Ie=()=>["",Ot,tn],Pe=()=>["auto",Qn,Z],Qe=()=>["bottom","center","left","left-bottom","left-top","right","right-bottom","right-top","top"],ue=()=>["solid","dashed","dotted","double","none"],ge=()=>["normal","multiply","screen","overlay","darken","lighten","color-dodge","color-burn","hard-light","soft-light","difference","exclusion","hue","saturation","color","luminosity"],M=()=>["start","end","center","between","around","evenly","stretch"],H=()=>["","0",Z],U=()=>["auto","avoid","all","avoid-page","page","left","right","column"],m=()=>[Qn,Z];return{cacheSize:500,separator:":",theme:{colors:[Mi],spacing:[Ot,tn],blur:["none","",nn,Z],brightness:m(),borderColor:[o],borderRadius:["none","","full",nn,Z],borderSpacing:ne(),borderWidth:Ie(),contrast:m(),grayscale:H(),hueRotate:m(),invert:H(),gap:ne(),gradientColorStops:[o],gradientColorStopPositions:[Fh,tn],inset:Fe(),margin:Fe(),opacity:m(),padding:ne(),saturate:m(),scale:m(),sepia:H(),skew:m(),space:ne(),translate:ne()},classGroups:{aspect:[{aspect:["auto","square","video",Z]}],container:["container"],columns:[{columns:[nn]}],"break-after":[{"break-after":U()}],"break-before":[{"break-before":U()}],"break-inside":[{"break-inside":["auto","avoid","avoid-page","avoid-column"]}],"box-decoration":[{"box-decoration":["slice","clone"]}],box:[{box:["border","content"]}],display:["block","inline-block","inline","flex","inline-flex","table","inline-table","table-caption","table-cell","table-column","table-column-group","table-footer-group","table-header-group","table-row-group","table-row","flow-root","grid","inline-grid","contents","list-item","hidden"],float:[{float:["right","left","none","start","end"]}],clear:[{clear:["left","right","both","none","start","end"]}],isolation:["isolate","isolation-auto"],"object-fit":[{object:["contain","cover","fill","none","scale-down"]}],"object-position":[{object:[...Qe(),Z]}],overflow:[{overflow:Le()}],"overflow-x":[{"overflow-x":Le()}],"overflow-y":[{"overflow-y":Le()}],overscroll:[{overscroll:Ye()}],"overscroll-x":[{"overscroll-x":Ye()}],"overscroll-y":[{"overscroll-y":Ye()}],position:["static","fixed","absolute","relative","sticky"],inset:[{inset:[P]}],"inset-x":[{"inset-x":[P]}],"inset-y":[{"inset-y":[P]}],start:[{start:[P]}],end:[{end:[P]}],top:[{top:[P]}],right:[{right:[P]}],bottom:[{bottom:[P]}],left:[{left:[P]}],visibility:["visible","invisible","collapse"],z:[{z:["auto",zi,Z]}],basis:[{basis:Fe()}],"flex-direction":[{flex:["row","row-reverse","col","col-reverse"]}],"flex-wrap":[{flex:["wrap","wrap-reverse","nowrap"]}],flex:[{flex:["1","auto","initial","none",Z]}],grow:[{grow:H()}],shrink:[{shrink:H()}],order:[{order:["first","last","none",zi,Z]}],"grid-cols":[{"grid-cols":[Mi]}],"col-start-end":[{col:["auto",{span:["full",zi,Z]},Z]}],"col-start":[{"col-start":Pe()}],"col-end":[{"col-end":Pe()}],"grid-rows":[{"grid-rows":[Mi]}],"row-start-end":[{row:["auto",{span:[zi,Z]},Z]}],"row-start":[{"row-start":Pe()}],"row-end":[{"row-end":Pe()}],"grid-flow":[{"grid-flow":["row","col","dense","row-dense","col-dense"]}],"auto-cols":[{"auto-cols":["auto","min","max","fr",Z]}],"auto-rows":[{"auto-rows":["auto","min","max","fr",Z]}],gap:[{gap:[q]}],"gap-x":[{"gap-x":[q]}],"gap-y":[{"gap-y":[q]}],"justify-content":[{justify:["normal",...M()]}],"justify-items":[{"justify-items":["start","end","center","stretch"]}],"justify-self":[{"justify-self":["auto","start","end","center","stretch"]}],"align-content":[{content:["normal",...M(),"baseline"]}],"align-items":[{items:["start","end","center","baseline","stretch"]}],"align-self":[{self:["auto","start","end","center","stretch","baseline"]}],"place-content":[{"place-content":[...M(),"baseline"]}],"place-items":[{"place-items":["start","end","center","baseline","stretch"]}],"place-self":[{"place-self":["auto","start","end","center","stretch"]}],p:[{p:[Q]}],px:[{px:[Q]}],py:[{py:[Q]}],ps:[{ps:[Q]}],pe:[{pe:[Q]}],pt:[{pt:[Q]}],pr:[{pr:[Q]}],pb:[{pb:[Q]}],pl:[{pl:[Q]}],m:[{m:[C]}],mx:[{mx:[C]}],my:[{my:[C]}],ms:[{ms:[C]}],me:[{me:[C]}],mt:[{mt:[C]}],mr:[{mr:[C]}],mb:[{mb:[C]}],ml:[{ml:[C]}],"space-x":[{"space-x":[Ae]}],"space-x-reverse":["space-x-reverse"],"space-y":[{"space-y":[Ae]}],"space-y-reverse":["space-y-reverse"],w:[{w:["auto","min","max","fit","svw","lvw","dvw",Z,c]}],"min-w":[{"min-w":[Z,c,"min","max","fit"]}],"max-w":[{"max-w":[Z,c,"none","full","min","max","fit","prose",{screen:[nn]},nn]}],h:[{h:[Z,c,"auto","min","max","fit","svh","lvh","dvh"]}],"min-h":[{"min-h":[Z,c,"min","max","fit","svh","lvh","dvh"]}],"max-h":[{"max-h":[Z,c,"min","max","fit","svh","lvh","dvh"]}],size:[{size:[Z,c,"auto","min","max","fit"]}],"font-size":[{text:["base",nn,tn]}],"font-smoothing":["antialiased","subpixel-antialiased"],"font-style":["italic","not-italic"],"font-weight":[{font:["thin","extralight","light","normal","medium","semibold","bold","extrabold","black",ns]}],"font-family":[{font:[Mi]}],"fvn-normal":["normal-nums"],"fvn-ordinal":["ordinal"],"fvn-slashed-zero":["slashed-zero"],"fvn-figure":["lining-nums","oldstyle-nums"],"fvn-spacing":["proportional-nums","tabular-nums"],"fvn-fraction":["diagonal-fractions","stacked-fractions"],tracking:[{tracking:["tighter","tight","normal","wide","wider","widest",Z]}],"line-clamp":[{"line-clamp":["none",Qn,ns]}],leading:[{leading:["none","tight","snug","normal","relaxed","loose",Ot,Z]}],"list-image":[{"list-image":["none",Z]}],"list-style-type":[{list:["none","disc","decimal",Z]}],"list-style-position":[{list:["inside","outside"]}],"placeholder-color":[{placeholder:[o]}],"placeholder-opacity":[{"placeholder-opacity":[Y]}],"text-alignment":[{text:["left","center","right","justify","start","end"]}],"text-color":[{text:[o]}],"text-opacity":[{"text-opacity":[Y]}],"text-decoration":["underline","overline","line-through","no-underline"],"text-decoration-style":[{decoration:[...ue(),"wavy"]}],"text-decoration-thickness":[{decoration:["auto","from-font",Ot,tn]}],"underline-offset":[{"underline-offset":["auto",Ot,Z]}],"text-decoration-color":[{decoration:[o]}],"text-transform":["uppercase","lowercase","capitalize","normal-case"],"text-overflow":["truncate","text-ellipsis","text-clip"],"text-wrap":[{text:["wrap","nowrap","balance","pretty"]}],indent:[{indent:ne()}],"vertical-align":[{align:["baseline","top","middle","bottom","text-top","text-bottom","sub","super",Z]}],whitespace:[{whitespace:["normal","nowrap","pre","pre-line","pre-wrap","break-spaces"]}],break:[{break:["normal","words","all","keep"]}],hyphens:[{hyphens:["none","manual","auto"]}],content:[{content:["none",Z]}],"bg-attachment":[{bg:["fixed","local","scroll"]}],"bg-clip":[{"bg-clip":["border","padding","content","text"]}],"bg-opacity":[{"bg-opacity":[Y]}],"bg-origin":[{"bg-origin":["border","padding","content"]}],"bg-position":[{bg:[...Qe(),Hh]}],"bg-repeat":[{bg:["no-repeat",{repeat:["","x","y","round","space"]}]}],"bg-size":[{bg:["auto","cover","contain",Gh]}],"bg-image":[{bg:["none",{"gradient-to":["t","tr","r","br","b","bl","l","tl"]},Vh]}],"bg-color":[{bg:[o]}],"gradient-from-pos":[{from:[W]}],"gradient-via-pos":[{via:[W]}],"gradient-to-pos":[{to:[W]}],"gradient-from":[{from:[z]}],"gradient-via":[{via:[z]}],"gradient-to":[{to:[z]}],rounded:[{rounded:[g]}],"rounded-s":[{"rounded-s":[g]}],"rounded-e":[{"rounded-e":[g]}],"rounded-t":[{"rounded-t":[g]}],"rounded-r":[{"rounded-r":[g]}],"rounded-b":[{"rounded-b":[g]}],"rounded-l":[{"rounded-l":[g]}],"rounded-ss":[{"rounded-ss":[g]}],"rounded-se":[{"rounded-se":[g]}],"rounded-ee":[{"rounded-ee":[g]}],"rounded-es":[{"rounded-es":[g]}],"rounded-tl":[{"rounded-tl":[g]}],"rounded-tr":[{"rounded-tr":[g]}],"rounded-br":[{"rounded-br":[g]}],"rounded-bl":[{"rounded-bl":[g]}],"border-w":[{border:[y]}],"border-w-x":[{"border-x":[y]}],"border-w-y":[{"border-y":[y]}],"border-w-s":[{"border-s":[y]}],"border-w-e":[{"border-e":[y]}],"border-w-t":[{"border-t":[y]}],"border-w-r":[{"border-r":[y]}],"border-w-b":[{"border-b":[y]}],"border-w-l":[{"border-l":[y]}],"border-opacity":[{"border-opacity":[Y]}],"border-style":[{border:[...ue(),"hidden"]}],"divide-x":[{"divide-x":[y]}],"divide-x-reverse":["divide-x-reverse"],"divide-y":[{"divide-y":[y]}],"divide-y-reverse":["divide-y-reverse"],"divide-opacity":[{"divide-opacity":[Y]}],"divide-style":[{divide:ue()}],"border-color":[{border:[h]}],"border-color-x":[{"border-x":[h]}],"border-color-y":[{"border-y":[h]}],"border-color-s":[{"border-s":[h]}],"border-color-e":[{"border-e":[h]}],"border-color-t":[{"border-t":[h]}],"border-color-r":[{"border-r":[h]}],"border-color-b":[{"border-b":[h]}],"border-color-l":[{"border-l":[h]}],"divide-color":[{divide:[h]}],"outline-style":[{outline:["",...ue()]}],"outline-offset":[{"outline-offset":[Ot,Z]}],"outline-w":[{outline:[Ot,tn]}],"outline-color":[{outline:[o]}],"ring-w":[{ring:Ie()}],"ring-w-inset":["ring-inset"],"ring-color":[{ring:[o]}],"ring-opacity":[{"ring-opacity":[Y]}],"ring-offset-w":[{"ring-offset":[Ot,tn]}],"ring-offset-color":[{"ring-offset":[o]}],shadow:[{shadow:["","inner","none",nn,Jh]}],"shadow-color":[{shadow:[Mi]}],opacity:[{opacity:[Y]}],"mix-blend":[{"mix-blend":[...ge(),"plus-lighter","plus-darker"]}],"bg-blend":[{"bg-blend":ge()}],filter:[{filter:["","none"]}],blur:[{blur:[l]}],brightness:[{brightness:[u]}],contrast:[{contrast:[k]}],"drop-shadow":[{"drop-shadow":["","none",nn,Z]}],grayscale:[{grayscale:[A]}],"hue-rotate":[{"hue-rotate":[T]}],invert:[{invert:[E]}],saturate:[{saturate:[K]}],sepia:[{sepia:[fe]}],"backdrop-filter":[{"backdrop-filter":["","none"]}],"backdrop-blur":[{"backdrop-blur":[l]}],"backdrop-brightness":[{"backdrop-brightness":[u]}],"backdrop-contrast":[{"backdrop-contrast":[k]}],"backdrop-grayscale":[{"backdrop-grayscale":[A]}],"backdrop-hue-rotate":[{"backdrop-hue-rotate":[T]}],"backdrop-invert":[{"backdrop-invert":[E]}],"backdrop-opacity":[{"backdrop-opacity":[Y]}],"backdrop-saturate":[{"backdrop-saturate":[K]}],"backdrop-sepia":[{"backdrop-sepia":[fe]}],"border-collapse":[{border:["collapse","separate"]}],"border-spacing":[{"border-spacing":[S]}],"border-spacing-x":[{"border-spacing-x":[S]}],"border-spacing-y":[{"border-spacing-y":[S]}],"table-layout":[{table:["auto","fixed"]}],caption:[{caption:["top","bottom"]}],transition:[{transition:["none","all","","colors","opacity","shadow","transform",Z]}],duration:[{duration:m()}],ease:[{ease:["linear","in","out","in-out",Z]}],delay:[{delay:m()}],animate:[{animate:["none","spin","ping","pulse","bounce",Z]}],transform:[{transform:["","gpu","none"]}],scale:[{scale:[X]}],"scale-x":[{"scale-x":[X]}],"scale-y":[{"scale-y":[X]}],rotate:[{rotate:[zi,Z]}],"translate-x":[{"translate-x":[le]}],"translate-y":[{"translate-y":[le]}],"skew-x":[{"skew-x":[me]}],"skew-y":[{"skew-y":[me]}],"transform-origin":[{origin:["center","top","top-right","right","bottom-right","bottom","bottom-left","left","top-left",Z]}],accent:[{accent:["auto",o]}],appearance:[{appearance:["none","auto"]}],cursor:[{cursor:["auto","default","pointer","wait","text","move","help","not-allowed","none","context-menu","progress","cell","crosshair","vertical-text","alias","copy","no-drop","grab","grabbing","all-scroll","col-resize","row-resize","n-resize","e-resize","s-resize","w-resize","ne-resize","nw-resize","se-resize","sw-resize","ew-resize","ns-resize","nesw-resize","nwse-resize","zoom-in","zoom-out",Z]}],"caret-color":[{caret:[o]}],"pointer-events":[{"pointer-events":["none","auto"]}],resize:[{resize:["none","y","x",""]}],"scroll-behavior":[{scroll:["auto","smooth"]}],"scroll-m":[{"scroll-m":ne()}],"scroll-mx":[{"scroll-mx":ne()}],"scroll-my":[{"scroll-my":ne()}],"scroll-ms":[{"scroll-ms":ne()}],"scroll-me":[{"scroll-me":ne()}],"scroll-mt":[{"scroll-mt":ne()}],"scroll-mr":[{"scroll-mr":ne()}],"scroll-mb":[{"scroll-mb":ne()}],"scroll-ml":[{"scroll-ml":ne()}],"scroll-p":[{"scroll-p":ne()}],"scroll-px":[{"scroll-px":ne()}],"scroll-py":[{"scroll-py":ne()}],"scroll-ps":[{"scroll-ps":ne()}],"scroll-pe":[{"scroll-pe":ne()}],"scroll-pt":[{"scroll-pt":ne()}],"scroll-pr":[{"scroll-pr":ne()}],"scroll-pb":[{"scroll-pb":ne()}],"scroll-pl":[{"scroll-pl":ne()}],"snap-align":[{snap:["start","end","center","align-none"]}],"snap-stop":[{snap:["normal","always"]}],"snap-type":[{snap:["none","x","y","both"]}],"snap-strictness":[{snap:["mandatory","proximity"]}],touch:[{touch:["auto","none","manipulation"]}],"touch-x":[{"touch-pan":["x","left","right"]}],"touch-y":[{"touch-pan":["y","up","down"]}],"touch-pz":["touch-pinch-zoom"],select:[{select:["none","text","all","auto"]}],"will-change":[{"will-change":["auto","scroll","contents","transform",Z]}],fill:[{fill:[o,"none"]}],"stroke-w":[{stroke:[Ot,tn,ns]}],stroke:[{stroke:[o,"none"]}],sr:["sr-only","not-sr-only"],"forced-color-adjust":[{"forced-color-adjust":["auto","none"]}]},conflictingClassGroups:{overflow:["overflow-x","overflow-y"],overscroll:["overscroll-x","overscroll-y"],inset:["inset-x","inset-y","start","end","top","right","bottom","left"],"inset-x":["right","left"],"inset-y":["top","bottom"],flex:["basis","grow","shrink"],gap:["gap-x","gap-y"],p:["px","py","ps","pe","pt","pr","pb","pl"],px:["pr","pl"],py:["pt","pb"],m:["mx","my","ms","me","mt","mr","mb","ml"],mx:["mr","ml"],my:["mt","mb"],size:["w","h"],"font-size":["leading"],"fvn-normal":["fvn-ordinal","fvn-slashed-zero","fvn-figure","fvn-spacing","fvn-fraction"],"fvn-ordinal":["fvn-normal"],"fvn-slashed-zero":["fvn-normal"],"fvn-figure":["fvn-normal"],"fvn-spacing":["fvn-normal"],"fvn-fraction":["fvn-normal"],"line-clamp":["display","overflow"],rounded:["rounded-s","rounded-e","rounded-t","rounded-r","rounded-b","rounded-l","rounded-ss","rounded-se","rounded-ee","rounded-es","rounded-tl","rounded-tr","rounded-br","rounded-bl"],"rounded-s":["rounded-ss","rounded-es"],"rounded-e":["rounded-se","rounded-ee"],"rounded-t":["rounded-tl","rounded-tr"],"rounded-r":["rounded-tr","rounded-br"],"rounded-b":["rounded-br","rounded-bl"],"rounded-l":["rounded-tl","rounded-bl"],"border-spacing":["border-spacing-x","border-spacing-y"],"border-w":["border-w-s","border-w-e","border-w-t","border-w-r","border-w-b","border-w-l"],"border-w-x":["border-w-r","border-w-l"],"border-w-y":["border-w-t","border-w-b"],"border-color":["border-color-s","border-color-e","border-color-t","border-color-r","border-color-b","border-color-l"],"border-color-x":["border-color-r","border-color-l"],"border-color-y":["border-color-t","border-color-b"],"scroll-m":["scroll-mx","scroll-my","scroll-ms","scroll-me","scroll-mt","scroll-mr","scroll-mb","scroll-ml"],"scroll-mx":["scroll-mr","scroll-ml"],"scroll-my":["scroll-mt","scroll-mb"],"scroll-p":["scroll-px","scroll-py","scroll-ps","scroll-pe","scroll-pt","scroll-pr","scroll-pb","scroll-pl"],"scroll-px":["scroll-pr","scroll-pl"],"scroll-py":["scroll-pt","scroll-pb"],touch:["touch-x","touch-y","touch-pz"],"touch-x":["touch"],"touch-y":["touch"],"touch-pz":["touch"]},conflictingClassGroupModifiers:{"font-size":["leading"]}}},ef=Ph(Zh);function wt(...o){return ef(Nd(o))}const Gn=D.forwardRef(({className:o,...c},l)=>b.jsx("div",{ref:l,className:wt("rounded-xl border border-zinc-200 bg-white text-zinc-950 shadow dark:border-zinc-800 dark:bg-zinc-950 dark:text-zinc-50",o),...c}));Gn.displayName="Card";const Hn=D.forwardRef(({className:o,...c},l)=>b.jsx("div",{ref:l,className:wt("flex flex-col space-y-1.5 p-6",o),...c}));Hn.displayName="CardHeader";const Yn=D.forwardRef(({className:o,...c},l)=>b.jsx("div",{ref:l,className:wt("font-semibold leading-none tracking-tight",o),...c}));Yn.displayName="CardTitle";const Pd=D.forwardRef(({className:o,...c},l)=>b.jsx("div",{ref:l,className:wt("text-sm text-zinc-500 dark:text-zinc-400",o),...c}));Pd.displayName="CardDescription";const Vn=D.forwardRef(({className:o,...c},l)=>b.jsx("div",{ref:l,className:wt("p-6 pt-0",o),...c}));Vn.displayName="CardContent";const ds=D.forwardRef(({className:o,...c},l)=>b.jsx("div",{ref:l,className:wt("flex items-center p-6 pt-0",o),...c}));ds.displayName="CardFooter";function xd(o,c){if(typeof o=="function")return o(c);o!=null&&(o.current=c)}function zd(...o){return c=>{let l=!1;const u=o.map(h=>{const g=xd(h,c);return!l&&typeof g=="function"&&(l=!0),g});if(l)return()=>{for(let h=0;h<u.length;h++){const g=u[h];typeof g=="function"?g():xd(o[h],null)}}}}function wn(...o){return D.useCallback(zd(...o),o)}function Za(o){const c=nf(o),l=D.forwardRef((u,h)=>{const{children:g,...S}=u,y=D.Children.toArray(g),k=y.find(rf);if(k){const A=k.props.children,T=y.map(E=>E===k?D.Children.count(A)>1?D.Children.only(null):D.isValidElement(A)?A.props.children:null:E);return b.jsx(c,{...S,ref:h,children:D.isValidElement(A)?D.cloneElement(A,void 0,T):null})}return b.jsx(c,{...S,ref:h,children:g})});return l.displayName=`${o}.Slot`,l}var tf=Za("Slot");function nf(o){const c=D.forwardRef((l,u)=>{const{children:h,...g}=l;if(D.isValidElement(h)){const S=sf(h),y=of(g,h.props);return h.type!==D.Fragment&&(y.ref=u?zd(u,S):S),D.cloneElement(h,y)}return D.Children.count(h)>1?D.Children.only(null):null});return c.displayName=`${o}.SlotClone`,c}var af=Symbol("radix.slottable");function rf(o){return D.isValidElement(o)&&typeof o.type=="function"&&"__radixId"in o.type&&o.type.__radixId===af}function of(o,c){const l={...c};for(const u in c){const h=o[u],g=c[u];/^on[A-Z]/.test(u)?h&&g?l[u]=(...y)=>{const k=g(...y);return h(...y),k}:h&&(l[u]=h):u==="style"?l[u]={...h,...g}:u==="className"&&(l[u]=[h,g].filter(Boolean).join(" "))}return{...o,...l}}function sf(o){var u,h;let c=(u=Object.getOwnPropertyDescriptor(o.props,"ref"))==null?void 0:u.get,l=c&&"isReactWarning"in c&&c.isReactWarning;return l?o.ref:(c=(h=Object.getOwnPropertyDescriptor(o,"ref"))==null?void 0:h.get,l=c&&"isReactWarning"in c&&c.isReactWarning,l?o.props.ref:o.props.ref||o.ref)}const Sd=o=>typeof o=="boolean"?`${o}`:o===0?"0":o,bd=Nd,Md=(o,c)=>l=>{var u;if((c==null?void 0:c.variants)==null)return bd(o,l==null?void 0:l.class,l==null?void 0:l.className);const{variants:h,defaultVariants:g}=c,S=Object.keys(h).map(A=>{const T=l==null?void 0:l[A],E=g==null?void 0:g[A];if(T===null)return null;const q=Sd(T)||Sd(E);return h[A][q]}),y=l&&Object.entries(l).reduce((A,T)=>{let[E,q]=T;return q===void 0||(A[E]=q),A},{}),k=c==null||(u=c.compoundVariants)===null||u===void 0?void 0:u.reduce((A,T)=>{let{class:E,className:q,...z}=T;return Object.entries(z).every(W=>{let[P,C]=W;return Array.isArray(C)?C.includes({...g,...y}[P]):{...g,...y}[P]===C})?[...A,E,q]:A},[]);return bd(o,S,k,l==null?void 0:l.class,l==null?void 0:l.className)},lf=Md("inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-zinc-950 disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0 dark:focus-visible:ring-zinc-300",{variants:{variant:{default:"bg-zinc-900 text-zinc-50 shadow hover:bg-zinc-900/90 dark:bg-zinc-50 dark:text-zinc-900 dark:hover:bg-zinc-50/90",destructive:"bg-red-500 text-zinc-50 shadow-sm hover:bg-red-500/90 dark:bg-red-900 dark:text-zinc-50 dark:hover:bg-red-900/90",outline:"border border-zinc-200 bg-white shadow-sm hover:bg-zinc-100 hover:text-zinc-900 dark:border-zinc-800 dark:bg-zinc-950 dark:hover:bg-zinc-800 dark:hover:text-zinc-50",secondary:"bg-zinc-100 text-zinc-900 shadow-sm hover:bg-zinc-100/80 dark:bg-zinc-800 dark:text-zinc-50 dark:hover:bg-zinc-800/80",ghost:"hover:bg-zinc-100 hover:text-zinc-900 dark:hover:bg-zinc-800 dark:hover:text-zinc-50",link:"text-zinc-900 underline-offset-4 hover:underline dark:text-zinc-50"},size:{default:"h-9 px-4 py-2",sm:"h-8 rounded-md px-3 text-xs",lg:"h-10 rounded-md px-8",icon:"h-9 w-9"}},defaultVariants:{variant:"default",size:"default"}}),qt=D.forwardRef(({className:o,variant:c,size:l,asChild:u=!1,...h},g)=>{const S=u?tf:"button";return b.jsx(S,{className:wt(lf({variant:c,size:l,className:o})),ref:g,...h})});qt.displayName="Button";function cf({tests:o,onSelectTest:c}){return b.jsxs("div",{className:"container mx-auto py-8",children:[b.jsx("h1",{className:"text-3xl font-bold text-center mb-8",children:"Snowflake Advanced Data Engineering Certificate"}),b.jsx("p",{className:"text-center mb-8 text-lg",children:"Select a practice test to begin"}),b.jsx("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-6",children:o.map(l=>b.jsxs(Gn,{className:"hover:shadow-lg transition-shadow",children:[b.jsxs(Hn,{children:[b.jsx(Yn,{children:l.title}),b.jsx(Pd,{children:"65 questions covering all exam domains"})]}),b.jsxs(Vn,{children:[b.jsx("p",{className:"text-sm",children:"This practice test follows the official exam domain weightings:"}),b.jsx("ul",{className:"list-disc pl-5 mt-2 text-sm",children:l.domains.map(u=>b.jsx("li",{children:u.name},u.id))})]}),b.jsx(ds,{children:b.jsx(qt,{className:"w-full",onClick:()=>c(l.id),children:"Start Test"})})]},l.id))})]})}function _t(o,c,{checkForDefaultPrevented:l=!0}={}){return function(h){if(o==null||o(h),l===!1||!h.defaultPrevented)return c==null?void 0:c(h)}}function Ui(o,c=[]){let l=[];function u(g,S){const y=D.createContext(S),k=l.length;l=[...l,S];const A=E=>{var Y;const{scope:q,children:z,...W}=E,P=((Y=q==null?void 0:q[o])==null?void 0:Y[k])||y,C=D.useMemo(()=>W,Object.values(W));return b.jsx(P.Provider,{value:C,children:z})};A.displayName=g+"Provider";function T(E,q){var P;const z=((P=q==null?void 0:q[o])==null?void 0:P[k])||y,W=D.useContext(z);if(W)return W;if(S!==void 0)return S;throw new Error(`\`${E}\` must be used within \`${g}\``)}return[A,T]}const h=()=>{const g=l.map(S=>D.createContext(S));return function(y){const k=(y==null?void 0:y[o])||g;return D.useMemo(()=>({[`__scope${o}`]:{...y,[o]:k}}),[y,k])}};return h.scopeName=o,[u,df(h,...c)]}function df(...o){const c=o[0];if(o.length===1)return c;const l=()=>{const u=o.map(h=>({useScope:h(),scopeName:h.scopeName}));return function(g){const S=u.reduce((y,{useScope:k,scopeName:A})=>{const E=k(g)[`__scope${A}`];return{...y,...E}},{});return D.useMemo(()=>({[`__scope${c.scopeName}`]:S}),[S])}};return l.scopeName=c.scopeName,l}Ed();var uf=["a","button","div","form","h2","h3","img","input","label","li","nav","ol","p","select","span","svg","ul"],Pt=uf.reduce((o,c)=>{const l=Za(`Primitive.${c}`),u=D.forwardRef((h,g)=>{const{asChild:S,...y}=h,k=S?l:c;return typeof window<"u"&&(window[Symbol.for("radix-ui")]=!0),b.jsx(k,{...y,ref:g})});return u.displayName=`Primitive.${c}`,{...o,[c]:u}},{});function pf(o){const c=o+"CollectionProvider",[l,u]=Ui(c),[h,g]=l(c,{collectionRef:{current:null},itemMap:new Map}),S=P=>{const{scope:C,children:Y}=P,Q=an.useRef(null),K=an.useRef(new Map).current;return b.jsx(h,{scope:C,itemMap:K,collectionRef:Q,children:Y})};S.displayName=c;const y=o+"CollectionSlot",k=Za(y),A=an.forwardRef((P,C)=>{const{scope:Y,children:Q}=P,K=g(y,Y),X=wn(C,K.collectionRef);return b.jsx(k,{ref:X,children:Q})});A.displayName=y;const T=o+"CollectionItemSlot",E="data-radix-collection-item",q=Za(T),z=an.forwardRef((P,C)=>{const{scope:Y,children:Q,...K}=P,X=an.useRef(null),fe=wn(C,X),me=g(T,Y);return an.useEffect(()=>(me.itemMap.set(X,{ref:X,...K}),()=>void me.itemMap.delete(X))),b.jsx(q,{[E]:"",ref:fe,children:Q})});z.displayName=T;function W(P){const C=g(o+"CollectionConsumer",P);return an.useCallback(()=>{const Q=C.collectionRef.current;if(!Q)return[];const K=Array.from(Q.querySelectorAll(`[${E}]`));return Array.from(C.itemMap.values()).sort((me,Ae)=>K.indexOf(me.ref.current)-K.indexOf(Ae.ref.current))},[C.collectionRef,C.itemMap])}return[{Provider:S,Slot:A,ItemSlot:z},W,u]}var Li=globalThis!=null&&globalThis.document?D.useLayoutEffect:()=>{},hf=Td[" useId ".trim().toString()]||(()=>{}),ff=0;function mf(o){const[c,l]=D.useState(hf());return Li(()=>{l(u=>u??String(ff++))},[o]),c?`radix-${c}`:""}function gf(o){const c=D.useRef(o);return D.useEffect(()=>{c.current=o}),D.useMemo(()=>(...l)=>{var u;return(u=c.current)==null?void 0:u.call(c,...l)},[])}var vf=Td[" useInsertionEffect ".trim().toString()]||Li;function Ld({prop:o,defaultProp:c,onChange:l=()=>{},caller:u}){const[h,g,S]=wf({defaultProp:c,onChange:l}),y=o!==void 0,k=y?o:h;{const T=D.useRef(o!==void 0);D.useEffect(()=>{const E=T.current;E!==y&&console.warn(`${u} is changing from ${E?"controlled":"uncontrolled"} to ${y?"controlled":"uncontrolled"}. Components should not switch from controlled to uncontrolled (or vice versa). Decide between using a controlled or uncontrolled value for the lifetime of the component.`),T.current=y},[y,u])}const A=D.useCallback(T=>{var E;if(y){const q=yf(T)?T(o):T;q!==o&&((E=S.current)==null||E.call(S,q))}else g(T)},[y,o,g,S]);return[k,A]}function wf({defaultProp:o,onChange:c}){const[l,u]=D.useState(o),h=D.useRef(l),g=D.useRef(c);return vf(()=>{g.current=c},[c]),D.useEffect(()=>{var S;h.current!==l&&((S=g.current)==null||S.call(g,l),h.current=l)},[l,h]),[l,u,g]}function yf(o){return typeof o=="function"}var xf=D.createContext(void 0);function Ud(o){const c=D.useContext(xf);return o||c||"ltr"}var is="rovingFocusGroup.onEntryFocus",Sf={bubbles:!1,cancelable:!0},Bi="RovingFocusGroup",[rs,Bd,bf]=pf(Bi),[kf,jd]=Ui(Bi,[bf]),[Af,Cf]=kf(Bi),Wd=D.forwardRef((o,c)=>b.jsx(rs.Provider,{scope:o.__scopeRovingFocusGroup,children:b.jsx(rs.Slot,{scope:o.__scopeRovingFocusGroup,children:b.jsx(Tf,{...o,ref:c})})}));Wd.displayName=Bi;var Tf=D.forwardRef((o,c)=>{const{__scopeRovingFocusGroup:l,orientation:u,loop:h=!1,dir:g,currentTabStopId:S,defaultCurrentTabStopId:y,onCurrentTabStopIdChange:k,onEntryFocus:A,preventScrollOnEntryFocus:T=!1,...E}=o,q=D.useRef(null),z=wn(c,q),W=Ud(g),[P,C]=Ld({prop:S,defaultProp:y??null,onChange:k,caller:Bi}),[Y,Q]=D.useState(!1),K=gf(A),X=Bd(l),fe=D.useRef(!1),[me,Ae]=D.useState(0);return D.useEffect(()=>{const le=q.current;if(le)return le.addEventListener(is,K),()=>le.removeEventListener(is,K)},[K]),b.jsx(Af,{scope:l,orientation:u,dir:W,loop:h,currentTabStopId:P,onItemFocus:D.useCallback(le=>C(le),[C]),onItemShiftTab:D.useCallback(()=>Q(!0),[]),onFocusableItemAdd:D.useCallback(()=>Ae(le=>le+1),[]),onFocusableItemRemove:D.useCallback(()=>Ae(le=>le-1),[]),children:b.jsx(Pt.div,{tabIndex:Y||me===0?-1:0,"data-orientation":u,...E,ref:z,style:{outline:"none",...o.style},onMouseDown:_t(o.onMouseDown,()=>{fe.current=!0}),onFocus:_t(o.onFocus,le=>{const Ye=!fe.current;if(le.target===le.currentTarget&&Ye&&!Y){const Le=new CustomEvent(is,Sf);if(le.currentTarget.dispatchEvent(Le),!Le.defaultPrevented){const Fe=X().filter(ue=>ue.focusable),ne=Fe.find(ue=>ue.active),Ie=Fe.find(ue=>ue.id===P),Qe=[ne,Ie,...Fe].filter(Boolean).map(ue=>ue.ref.current);Gd(Qe,T)}}fe.current=!1}),onBlur:_t(o.onBlur,()=>Q(!1))})})}),Fd="RovingFocusGroupItem",Qd=D.forwardRef((o,c)=>{const{__scopeRovingFocusGroup:l,focusable:u=!0,active:h=!1,tabStopId:g,children:S,...y}=o,k=mf(),A=g||k,T=Cf(Fd,l),E=T.currentTabStopId===A,q=Bd(l),{onFocusableItemAdd:z,onFocusableItemRemove:W,currentTabStopId:P}=T;return D.useEffect(()=>{if(u)return z(),()=>W()},[u,z,W]),b.jsx(rs.ItemSlot,{scope:l,id:A,focusable:u,active:h,children:b.jsx(Pt.span,{tabIndex:E?0:-1,"data-orientation":T.orientation,...y,ref:c,onMouseDown:_t(o.onMouseDown,C=>{u?T.onItemFocus(A):C.preventDefault()}),onFocus:_t(o.onFocus,()=>T.onItemFocus(A)),onKeyDown:_t(o.onKeyDown,C=>{if(C.key==="Tab"&&C.shiftKey){T.onItemShiftTab();return}if(C.target!==C.currentTarget)return;const Y=Nf(C,T.orientation,T.dir);if(Y!==void 0){if(C.metaKey||C.ctrlKey||C.altKey||C.shiftKey)return;C.preventDefault();let K=q().filter(X=>X.focusable).map(X=>X.ref.current);if(Y==="last")K.reverse();else if(Y==="prev"||Y==="next"){Y==="prev"&&K.reverse();const X=K.indexOf(C.currentTarget);K=T.loop?Rf(K,X+1):K.slice(X+1)}setTimeout(()=>Gd(K))}}),children:typeof S=="function"?S({isCurrentTabStop:E,hasTabStop:P!=null}):S})})});Qd.displayName=Fd;var Ef={ArrowLeft:"prev",ArrowUp:"prev",ArrowRight:"next",ArrowDown:"next",PageUp:"first",Home:"first",PageDown:"last",End:"last"};function Df(o,c){return c!=="rtl"?o:o==="ArrowLeft"?"ArrowRight":o==="ArrowRight"?"ArrowLeft":o}function Nf(o,c,l){const u=Df(o.key,l);if(!(c==="vertical"&&["ArrowLeft","ArrowRight"].includes(u))&&!(c==="horizontal"&&["ArrowUp","ArrowDown"].includes(u)))return Ef[u]}function Gd(o,c=!1){const l=document.activeElement;for(const u of o)if(u===l||(u.focus({preventScroll:c}),document.activeElement!==l))return}function Rf(o,c){return o.map((l,u)=>o[(c+u)%o.length])}var If=Wd,Of=Qd;function qf(o){const[c,l]=D.useState(void 0);return Li(()=>{if(o){l({width:o.offsetWidth,height:o.offsetHeight});const u=new ResizeObserver(h=>{if(!Array.isArray(h)||!h.length)return;const g=h[0];let S,y;if("borderBoxSize"in g){const k=g.borderBoxSize,A=Array.isArray(k)?k[0]:k;S=A.inlineSize,y=A.blockSize}else S=o.offsetWidth,y=o.offsetHeight;l({width:S,height:y})});return u.observe(o,{box:"border-box"}),()=>u.unobserve(o)}else l(void 0)},[o]),c}function _f(o){const c=D.useRef({value:o,previous:o});return D.useMemo(()=>(c.current.value!==o&&(c.current.previous=c.current.value,c.current.value=o),c.current.previous),[o])}function Pf(o,c){return D.useReducer((l,u)=>c[l][u]??l,o)}var Hd=o=>{const{present:c,children:l}=o,u=zf(c),h=typeof l=="function"?l({present:u.isPresent}):D.Children.only(l),g=wn(u.ref,Mf(h));return typeof l=="function"||u.isPresent?D.cloneElement(h,{ref:g}):null};Hd.displayName="Presence";function zf(o){const[c,l]=D.useState(),u=D.useRef(null),h=D.useRef(o),g=D.useRef("none"),S=o?"mounted":"unmounted",[y,k]=Pf(S,{mounted:{UNMOUNT:"unmounted",ANIMATION_OUT:"unmountSuspended"},unmountSuspended:{MOUNT:"mounted",ANIMATION_END:"unmounted"},unmounted:{MOUNT:"mounted"}});return D.useEffect(()=>{const A=Xa(u.current);g.current=y==="mounted"?A:"none"},[y]),Li(()=>{const A=u.current,T=h.current;if(T!==o){const q=g.current,z=Xa(A);o?k("MOUNT"):z==="none"||(A==null?void 0:A.display)==="none"?k("UNMOUNT"):k(T&&q!==z?"ANIMATION_OUT":"UNMOUNT"),h.current=o}},[o,k]),Li(()=>{if(c){let A;const T=c.ownerDocument.defaultView??window,E=z=>{const P=Xa(u.current).includes(z.animationName);if(z.target===c&&P&&(k("ANIMATION_END"),!h.current)){const C=c.style.animationFillMode;c.style.animationFillMode="forwards",A=T.setTimeout(()=>{c.style.animationFillMode==="forwards"&&(c.style.animationFillMode=C)})}},q=z=>{z.target===c&&(g.current=Xa(u.current))};return c.addEventListener("animationstart",q),c.addEventListener("animationcancel",E),c.addEventListener("animationend",E),()=>{T.clearTimeout(A),c.removeEventListener("animationstart",q),c.removeEventListener("animationcancel",E),c.removeEventListener("animationend",E)}}else k("ANIMATION_END")},[c,k]),{isPresent:["mounted","unmountSuspended"].includes(y),ref:D.useCallback(A=>{u.current=A?getComputedStyle(A):null,l(A)},[])}}function Xa(o){return(o==null?void 0:o.animationName)||"none"}function Mf(o){var u,h;let c=(u=Object.getOwnPropertyDescriptor(o.props,"ref"))==null?void 0:u.get,l=c&&"isReactWarning"in c&&c.isReactWarning;return l?o.ref:(c=(h=Object.getOwnPropertyDescriptor(o,"ref"))==null?void 0:h.get,l=c&&"isReactWarning"in c&&c.isReactWarning,l?o.props.ref:o.props.ref||o.ref)}var us="Radio",[Lf,Yd]=Ui(us),[Uf,Bf]=Lf(us),Vd=D.forwardRef((o,c)=>{const{__scopeRadio:l,name:u,checked:h=!1,required:g,disabled:S,value:y="on",onCheck:k,form:A,...T}=o,[E,q]=D.useState(null),z=wn(c,C=>q(C)),W=D.useRef(!1),P=E?A||!!E.closest("form"):!0;return b.jsxs(Uf,{scope:l,checked:h,disabled:S,children:[b.jsx(Pt.button,{type:"button",role:"radio","aria-checked":h,"data-state":Xd(h),"data-disabled":S?"":void 0,disabled:S,value:y,...T,ref:z,onClick:_t(o.onClick,C=>{h||k==null||k(),P&&(W.current=C.isPropagationStopped(),W.current||C.stopPropagation())})}),P&&b.jsx(Kd,{control:E,bubbles:!W.current,name:u,value:y,checked:h,required:g,disabled:S,form:A,style:{transform:"translateX(-100%)"}})]})});Vd.displayName=us;var Jd="RadioIndicator",$d=D.forwardRef((o,c)=>{const{__scopeRadio:l,forceMount:u,...h}=o,g=Bf(Jd,l);return b.jsx(Hd,{present:u||g.checked,children:b.jsx(Pt.span,{"data-state":Xd(g.checked),"data-disabled":g.disabled?"":void 0,...h,ref:c})})});$d.displayName=Jd;var jf="RadioBubbleInput",Kd=D.forwardRef(({__scopeRadio:o,control:c,checked:l,bubbles:u=!0,...h},g)=>{const S=D.useRef(null),y=wn(S,g),k=_f(l),A=qf(c);return D.useEffect(()=>{const T=S.current;if(!T)return;const E=window.HTMLInputElement.prototype,z=Object.getOwnPropertyDescriptor(E,"checked").set;if(k!==l&&z){const W=new Event("click",{bubbles:u});z.call(T,l),T.dispatchEvent(W)}},[k,l,u]),b.jsx(Pt.input,{type:"radio","aria-hidden":!0,defaultChecked:l,...h,tabIndex:-1,ref:y,style:{...h.style,...A,position:"absolute",pointerEvents:"none",opacity:0,margin:0}})});Kd.displayName=jf;function Xd(o){return o?"checked":"unchecked"}var Wf=["ArrowUp","ArrowDown","ArrowLeft","ArrowRight"],nr="RadioGroup",[Ff,Em]=Ui(nr,[jd,Yd]),Zd=jd(),eu=Yd(),[Qf,Gf]=Ff(nr),tu=D.forwardRef((o,c)=>{const{__scopeRadioGroup:l,name:u,defaultValue:h,value:g,required:S=!1,disabled:y=!1,orientation:k,dir:A,loop:T=!0,onValueChange:E,...q}=o,z=Zd(l),W=Ud(A),[P,C]=Ld({prop:g,defaultProp:h??null,onChange:E,caller:nr});return b.jsx(Qf,{scope:l,name:u,required:S,disabled:y,value:P,onValueChange:C,children:b.jsx(If,{asChild:!0,...z,orientation:k,dir:W,loop:T,children:b.jsx(Pt.div,{role:"radiogroup","aria-required":S,"aria-orientation":k,"data-disabled":y?"":void 0,dir:W,...q,ref:c})})})});tu.displayName=nr;var nu="RadioGroupItem",iu=D.forwardRef((o,c)=>{const{__scopeRadioGroup:l,disabled:u,...h}=o,g=Gf(nu,l),S=g.disabled||u,y=Zd(l),k=eu(l),A=D.useRef(null),T=wn(c,A),E=g.value===h.value,q=D.useRef(!1);return D.useEffect(()=>{const z=P=>{Wf.includes(P.key)&&(q.current=!0)},W=()=>q.current=!1;return document.addEventListener("keydown",z),document.addEventListener("keyup",W),()=>{document.removeEventListener("keydown",z),document.removeEventListener("keyup",W)}},[]),b.jsx(Of,{asChild:!0,...y,focusable:!S,active:E,children:b.jsx(Vd,{disabled:S,required:g.required,checked:E,...k,...h,name:g.name,ref:T,onCheck:()=>g.onValueChange(h.value),onKeyDown:_t(z=>{z.key==="Enter"&&z.preventDefault()}),onFocus:_t(h.onFocus,()=>{var z;q.current&&((z=A.current)==null||z.click())})})})});iu.displayName=nu;var Hf="RadioGroupIndicator",au=D.forwardRef((o,c)=>{const{__scopeRadioGroup:l,...u}=o,h=eu(l);return b.jsx($d,{...h,...u,ref:c})});au.displayName=Hf;var ru=tu,ou=iu,Yf=au;/**
 * @license lucide-react v0.364.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */var Vf={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};/**
 * @license lucide-react v0.364.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Jf=o=>o.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase();/**
 * @license lucide-react v0.364.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const ps=(o,c)=>{const l=D.forwardRef(({color:u="currentColor",size:h=24,strokeWidth:g=2,absoluteStrokeWidth:S,className:y="",children:k,...A},T)=>D.createElement("svg",{ref:T,...Vf,width:h,height:h,stroke:u,strokeWidth:S?Number(g)*24/Number(h):g,className:["lucide",`lucide-${Jf(o)}`,y].join(" "),...A},[...c.map(([E,q])=>D.createElement(E,q)),...Array.isArray(k)?k:[k]]));return l.displayName=`${o}`,l};/**
 * @license lucide-react v0.364.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const os=ps("CircleCheckBig",[["path",{d:"M22 11.08V12a10 10 0 1 1-5.93-9.14",key:"g774vq"}],["path",{d:"m9 11 3 3L22 4",key:"1pflzl"}]]);/**
 * @license lucide-react v0.364.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const ss=ps("CircleX",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["path",{d:"m15 9-6 6",key:"1uzhvr"}],["path",{d:"m9 9 6 6",key:"z0biqf"}]]);/**
 * @license lucide-react v0.364.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const $f=ps("Circle",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}]]),su=D.forwardRef(({className:o,...c},l)=>b.jsx(ru,{className:wt("grid gap-2",o),...c,ref:l}));su.displayName=ru.displayName;const lu=D.forwardRef(({className:o,...c},l)=>b.jsx(ou,{ref:l,className:wt("aspect-square h-4 w-4 rounded-full border border-zinc-200 border-zinc-900 text-zinc-900 shadow focus:outline-none focus-visible:ring-1 focus-visible:ring-zinc-950 disabled:cursor-not-allowed disabled:opacity-50 dark:border-zinc-800 dark:border-zinc-50 dark:text-zinc-50 dark:focus-visible:ring-zinc-300",o),...c,children:b.jsx(Yf,{className:"flex items-center justify-center",children:b.jsx($f,{className:"h-3.5 w-3.5 fill-primary"})})}));lu.displayName=ou.displayName;var Kf="Label",cu=D.forwardRef((o,c)=>b.jsx(Pt.label,{...o,ref:c,onMouseDown:l=>{var h;l.target.closest("button, input, select, textarea")||((h=o.onMouseDown)==null||h.call(o,l),!l.defaultPrevented&&l.detail>1&&l.preventDefault())}}));cu.displayName=Kf;var du=cu;const Xf=Md("text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70"),uu=D.forwardRef(({className:o,...c},l)=>b.jsx(du,{ref:l,className:wt(Xf(),o),...c}));uu.displayName=du.displayName;var hs="Progress",fs=100,[Zf,Dm]=Ui(hs),[em,tm]=Zf(hs),pu=D.forwardRef((o,c)=>{const{__scopeProgress:l,value:u=null,max:h,getValueLabel:g=nm,...S}=o;(h||h===0)&&!kd(h)&&console.error(im(`${h}`,"Progress"));const y=kd(h)?h:fs;u!==null&&!Ad(u,y)&&console.error(am(`${u}`,"Progress"));const k=Ad(u,y)?u:null,A=er(k)?g(k,y):void 0;return b.jsx(em,{scope:l,value:k,max:y,children:b.jsx(Pt.div,{"aria-valuemax":y,"aria-valuemin":0,"aria-valuenow":er(k)?k:void 0,"aria-valuetext":A,role:"progressbar","data-state":mu(k,y),"data-value":k??void 0,"data-max":y,...S,ref:c})})});pu.displayName=hs;var hu="ProgressIndicator",fu=D.forwardRef((o,c)=>{const{__scopeProgress:l,...u}=o,h=tm(hu,l);return b.jsx(Pt.div,{"data-state":mu(h.value,h.max),"data-value":h.value??void 0,"data-max":h.max,...u,ref:c})});fu.displayName=hu;function nm(o,c){return`${Math.round(o/c*100)}%`}function mu(o,c){return o==null?"indeterminate":o===c?"complete":"loading"}function er(o){return typeof o=="number"}function kd(o){return er(o)&&!isNaN(o)&&o>0}function Ad(o,c){return er(o)&&!isNaN(o)&&o<=c&&o>=0}function im(o,c){return`Invalid prop \`max\` of value \`${o}\` supplied to \`${c}\`. Only numbers greater than 0 are valid max values. Defaulting to \`${fs}\`.`}function am(o,c){return`Invalid prop \`value\` of value \`${o}\` supplied to \`${c}\`. The \`value\` prop must be:
  - a positive number
  - less than the value passed to \`max\` (or ${fs} if no \`max\` prop is set)
  - \`null\` or \`undefined\` if the progress is indeterminate.

Defaulting to \`null\`.`}var gu=pu,rm=fu;const tr=D.forwardRef(({className:o,value:c,...l},u)=>b.jsx(gu,{ref:u,className:wt("relative h-2 w-full overflow-hidden rounded-full bg-zinc-900/20 dark:bg-zinc-50/20",o),...l,children:b.jsx(rm,{className:"h-full w-full flex-1 bg-zinc-900 transition-all dark:bg-zinc-50",style:{transform:`translateX(-${100-(c||0)}%)`}})}));tr.displayName=gu.displayName;function om({questions:o,onComplete:c,onExit:l}){const[u,h]=D.useState(0),[g,S]=D.useState([]),[y,k]=D.useState(!1),A=o[u],T=o.length,E=(u+1)/T*100;D.useEffect(()=>{const Q=o.map(K=>({questionId:K.id,selectedOption:null,isCorrect:!1}));S(Q)},[o]);const q=Q=>{if(y)return;const K=[...g],X=K.findIndex(fe=>fe.questionId===A.id);X!==-1&&(K[X]={...K[X],selectedOption:Q,isCorrect:Q===A.correctAnswer},S(K))},z=()=>{k(!0)},W=()=>{k(!1),u<T-1?h(u+1):c(g)},P=()=>{u>0&&(k(!1),h(u-1))},C=g.find(Q=>Q.questionId===(A==null?void 0:A.id)),Y=(C==null?void 0:C.selectedOption)!==null;return b.jsxs("div",{className:"container mx-auto py-6 max-w-4xl",children:[b.jsxs("div",{className:"flex justify-between items-center mb-4",children:[b.jsx(qt,{variant:"outline",onClick:l,children:"Exit Test"}),b.jsxs("div",{className:"text-sm",children:["Question ",u+1," of ",T]})]}),b.jsx(tr,{value:E,className:"mb-6"}),b.jsxs(Gn,{className:"mb-6",children:[b.jsx(Hn,{children:b.jsxs(Yn,{className:"text-lg",children:[A.domain," - Question ",A.id]})}),b.jsxs(Vn,{children:[b.jsx("p",{className:"mb-6 whitespace-pre-line",children:A.text}),b.jsx(su,{value:(C==null?void 0:C.selectedOption)||"",className:"space-y-4",children:A.options.map(Q=>b.jsxs("div",{className:`flex items-start space-x-2 p-3 rounded-md border ${y&&Q.id===A.correctAnswer?"bg-green-50 border-green-200":y&&Q.id===(C==null?void 0:C.selectedOption)?Q.id!==A.correctAnswer?"bg-red-50 border-red-200":"bg-green-50 border-green-200":Q.id===(C==null?void 0:C.selectedOption)?"bg-slate-100 border-slate-200":"hover:bg-slate-50"}`,onClick:()=>q(Q.id),children:[b.jsx(lu,{value:Q.id,id:`option-${Q.id}`,disabled:y}),b.jsx("div",{className:"flex-1",children:b.jsxs(uu,{htmlFor:`option-${Q.id}`,className:"flex items-start cursor-pointer",children:[b.jsxs("span",{className:"font-medium mr-2",children:[Q.id,"."]}),b.jsx("span",{className:"flex-1",children:Q.text}),y&&Q.id===A.correctAnswer&&b.jsx(os,{className:"h-5 w-5 text-green-500 ml-2 flex-shrink-0"}),y&&Q.id===(C==null?void 0:C.selectedOption)&&Q.id!==A.correctAnswer&&b.jsx(ss,{className:"h-5 w-5 text-red-500 ml-2 flex-shrink-0"})]})})]},Q.id))}),y&&b.jsxs("div",{className:"mt-6 p-4 bg-blue-50 rounded-md border border-blue-200",children:[b.jsx("h3",{className:"font-bold mb-2",children:"Explanation:"}),b.jsx("p",{className:"whitespace-pre-line",children:A.explanation})]})]}),b.jsxs(ds,{className:"flex justify-between",children:[b.jsx(qt,{variant:"outline",onClick:P,disabled:u===0,children:"Previous"}),b.jsx("div",{children:y?b.jsx(qt,{onClick:W,children:u<T-1?"Next Question":"Finish Test"}):b.jsx(qt,{onClick:z,disabled:!Y,children:"Check Answer"})})]})]})]})}function sm({test:o,userAnswers:c,onRetakeTest:l,onSelectNewTest:u}){const[h,g]=D.useState(!1),S=D.useMemo(()=>{const E=c.filter(P=>P.isCorrect).length,q=c.length,z=Math.round(E/q*100),W={};return o.questions.forEach(P=>{const C=P.domain,Y=c.find(Q=>Q.questionId===P.id);W[C]||(W[C]={correct:0,total:0,percentage:0}),W[C].total+=1,Y!=null&&Y.isCorrect&&(W[C].correct+=1)}),Object.keys(W).forEach(P=>{const{correct:C,total:Y}=W[P];W[P].percentage=Math.round(C/Y*100)}),{score:E,totalQuestions:q,percentage:z,domainScores:W}},[o,c]),y=T=>T>=80?"text-green-600":T>=70?"text-yellow-600":"text-red-600",k=T=>T>=80?"Excellent! You are well-prepared for the exam.":T>=70?"Good job! With a bit more study, you will be ready.":"More study needed. Focus on the domains with lower scores.";return b.jsxs("div",{className:"container mx-auto py-8 max-w-4xl",children:[b.jsxs("h1",{className:"text-3xl font-bold text-center mb-2",children:[o.title," Results"]}),b.jsx("p",{className:"text-center mb-8 text-lg",children:"Snowflake Advanced Data Engineering Certificate"}),b.jsxs(Gn,{className:"mb-8",children:[b.jsx(Hn,{children:b.jsx(Yn,{children:"Your Score"})}),b.jsx(Vn,{children:b.jsxs("div",{className:"flex flex-col items-center",children:[b.jsxs("div",{className:`text-6xl font-bold mb-4 ${y(S.percentage)}`,children:[S.percentage,"%"]}),b.jsxs("div",{className:"text-xl mb-4",children:[S.score," correct out of ",S.totalQuestions," questions"]}),b.jsx(tr,{value:S.percentage,className:`w-full h-4 mb-4 ${S.percentage>=70?"bg-green-600":"bg-red-600"}`}),b.jsx("div",{className:"text-center mt-2",children:k(S.percentage)})]})})]}),b.jsxs(Gn,{className:"mb-8",children:[b.jsx(Hn,{children:b.jsx(Yn,{children:"Domain Breakdown"})}),b.jsx(Vn,{children:b.jsx("div",{className:"space-y-4",children:Object.entries(S.domainScores).map(([T,E])=>b.jsxs("div",{children:[b.jsxs("div",{className:"flex justify-between mb-1",children:[b.jsx("span",{className:"font-medium",children:T}),b.jsxs("span",{className:y(E.percentage),children:[E.correct,"/",E.total," (",E.percentage,"%)"]})]}),b.jsx(tr,{value:E.percentage,className:"h-2"})]},T))})})]}),b.jsxs("div",{className:"flex justify-center space-x-4 mb-8",children:[b.jsx(qt,{onClick:l,children:"Retake Test"}),b.jsx(qt,{variant:"outline",onClick:u,children:"Select Different Test"}),b.jsx(qt,{variant:"outline",onClick:()=>g(!h),children:h?"Hide Questions":"Review All Questions"})]}),h&&b.jsxs("div",{className:"space-y-6",children:[b.jsx("h2",{className:"text-2xl font-bold",children:"Question Review"}),o.questions.map(T=>{const E=c.find(z=>z.questionId===T.id),q=E==null?void 0:E.isCorrect;return b.jsxs(Gn,{className:`border-l-4 ${q?"border-l-green-500":"border-l-red-500"}`,children:[b.jsx(Hn,{children:b.jsxs(Yn,{className:"text-lg flex items-start",children:[b.jsxs("span",{className:"mr-2",children:["Question ",T.id,":"]}),b.jsx("span",{className:"flex-1",children:T.text}),q?b.jsx(os,{className:"h-5 w-5 text-green-500 ml-2 flex-shrink-0"}):b.jsx(ss,{className:"h-5 w-5 text-red-500 ml-2 flex-shrink-0"})]})}),b.jsx(Vn,{children:b.jsxs("div",{className:"space-y-2",children:[T.options.map(z=>b.jsx("div",{className:`p-3 rounded-md ${z.id===T.correctAnswer?"bg-green-50 border border-green-200":z.id===(E==null?void 0:E.selectedOption)&&!q?"bg-red-50 border border-red-200":"bg-gray-50 border border-gray-200"}`,children:b.jsxs("div",{className:"flex items-start",children:[b.jsxs("span",{className:"font-medium mr-2",children:[z.id,"."]}),b.jsx("span",{className:"flex-1",children:z.text}),z.id===T.correctAnswer&&b.jsx(os,{className:"h-5 w-5 text-green-500 ml-2 flex-shrink-0"}),z.id===(E==null?void 0:E.selectedOption)&&!q&&z.id!==T.correctAnswer&&b.jsx(ss,{className:"h-5 w-5 text-red-500 ml-2 flex-shrink-0"})]})},z.id)),b.jsxs("div",{className:"mt-4 p-4 bg-blue-50 rounded-md border border-blue-200",children:[b.jsx("h3",{className:"font-bold mb-2",children:"Explanation:"}),b.jsx("p",{className:"whitespace-pre-line",children:T.explanation})]})]})})]},T.id)})]})]})}const lm=1,cm="Practice Test 1",dm="Snowflake Advanced Data Engineering Certificate (DEA-C02) - Practice Test 1",um=JSON.parse(`[{"id":1,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from an Amazon S3 bucket into Snowflake. The data arrives continuously throughout the day, and the business requires the data to be available in Snowflake within 15 minutes of being placed in the S3 bucket. Which approach should the data engineer implement?","options":[{"id":"A","text":"Create a Snowpipe with an SQS queue notification integration"},{"id":"B","text":"Schedule a COPY command to run every 15 minutes using Snowflake Tasks"},{"id":"C","text":"Use Snowflake Streams to capture changes and load them with a Task"},{"id":"D","text":"Implement a manual COPY command that runs when users request data"}],"correctAnswer":"A","explanation":"Snowpipe with SQS queue notification integration is the most appropriate solution for near-real-time data loading from S3. When new files are placed in the S3 bucket, S3 events trigger SQS notifications, which then automatically trigger Snowpipe to load the new data. This approach ensures data is loaded within minutes of arrival without manual intervention or scheduled tasks, meeting the 15-minute requirement efficiently.*"},{"id":2,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a data pipeline to load JSON data from an API into Snowflake. The JSON structure varies slightly between API calls. Which approach should be used to handle the varying JSON structure?","options":[{"id":"A","text":"Convert all JSON to a fixed schema before loading"},{"id":"B","text":"Use VARIANT data type to store the JSON data"},{"id":"C","text":"Normalize the JSON into multiple relational tables before loading"},{"id":"D","text":"Use external tables with a fixed schema"}],"correctAnswer":"B","explanation":"The VARIANT data type in Snowflake is specifically designed to store semi-structured data like JSON, especially when the structure may vary. It allows for flexible schema handling and enables querying the JSON data using dot notation or the FLATTEN function. This approach accommodates the varying JSON structure without requiring schema conversion or normalization before loading.*"},{"id":3,"domain":"Data Movement (17 questions)","text":"A company needs to share sensitive customer data with a partner organization that also uses Snowflake. Which method provides the most secure and efficient way to share this data?","options":[{"id":"A","text":"Export the data to S3 and provide the partner with access credentials"},{"id":"B","text":"Set up a Snowflake Data Sharing (Database Shares) connection"},{"id":"C","text":"Create database clones and provide separate login credentials"},{"id":"D","text":"Use Snowflake Data Exchange to publish the data publicly"}],"correctAnswer":"B","explanation":"Snowflake Data Sharing (Database Shares) is the most secure and efficient method for sharing data between Snowflake accounts. It allows for secure, read-only access to specific databases, schemas, or tables without copying the data. The provider maintains control over what data is shared, and the consumer can query the data directly without needing to store or manage it. This approach maintains data governance while enabling efficient collaboration.*"},{"id":4,"domain":"Data Movement (17 questions)","text":"A data engineer is loading data from multiple source systems into Snowflake. Which statement about the COPY command's ON_ERROR option is true?","options":[{"id":"A","text":"ON_ERROR=CONTINUE will load all data regardless of errors"},{"id":"B","text":"ON_ERROR=SKIP_FILE will skip only the specific rows with errors"},{"id":"C","text":"ON_ERROR=ABORT_STATEMENT is the default behavior if not specified"},{"id":"D","text":"ON_ERROR=SKIP_FILE_<n> will skip a file if it contains more than n errors"}],"correctAnswer":"C","explanation":"The default behavior of the COPY command when encountering errors is ON_ERROR=ABORT_STATEMENT, which means the entire load operation will be aborted if any error is encountered. ON_ERROR=CONTINUE will skip rows with errors and continue loading, ON_ERROR=SKIP_FILE will skip the entire file if any error is found in it, and there is no ON_ERROR=SKIP_FILE_<n> option. Understanding the default behavior is crucial for designing robust data loading processes.*"},{"id":5,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from an on-premises Oracle database to Snowflake in near real-time. Which approach is most appropriate?","options":[{"id":"A","text":"Use Snowflake Snowpipe with direct Oracle connectivity"},{"id":"B","text":"Implement a change data capture (CDC) solution with Kafka and Snowpipe"},{"id":"C","text":"Schedule hourly full extracts using the COPY command"},{"id":"D","text":"Use Snowflake's built-in Oracle connector"}],"correctAnswer":"B","explanation":"For near real-time data loading from an on-premises Oracle database, a change data capture (CDC) solution with Kafka as a message broker and Snowpipe for loading is the most appropriate approach. This architecture captures changes in the Oracle database as they occur, streams them through Kafka, and loads them into Snowflake using Snowpipe. This provides a scalable, reliable solution for near real-time data integration between on-premises systems and Snowflake.*"},{"id":6,"domain":"Data Movement (17 questions)","text":"A data engineer is troubleshooting a Snowpipe that has stopped loading data. Which Snowflake view should be checked first to identify the issue?","options":[{"id":"A","text":"SNOWFLAKE.ACCOUNT_USAGE.LOAD_HISTORY"},{"id":"B","text":"SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY"},{"id":"C","text":"SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY"},{"id":"D","text":"SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY"}],"correctAnswer":"C","explanation":"SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY is the most appropriate view to check first when troubleshooting Snowpipe issues. This view contains historical information about pipe usage, including errors and status information. It provides details about file loads, error messages, and pipe execution that can help identify why a Snowpipe has stopped loading data. The other views may provide supplementary information but are not as directly relevant to Snowpipe troubleshooting.*"},{"id":7,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from multiple CSV files with varying schemas into a single Snowflake table. Which approach should be used?","options":[{"id":"A","text":"Create a separate staging table for each schema variation, then merge into the target"},{"id":"B","text":"Use the MATCH_BY_COLUMN_NAME option in the COPY command"},{"id":"C","text":"Use the AUTO_INGEST=TRUE parameter with Snowpipe"},{"id":"D","text":"Create an external table with a superset schema of all variations"}],"correctAnswer":"B","explanation":"The MATCH_BY_COLUMN_NAME option in the COPY command allows loading data from files with varying schemas by matching column names rather than positions. This option instructs Snowflake to map source columns to target columns based on names, ignoring extra columns in the source and filling missing columns with NULL values. This is ideal for handling schema variations without creating multiple staging tables or complex transformations.*"},{"id":8,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution to load data from multiple external stages into a single Snowflake table. The data needs to be transformed during the load process. Which approach is most efficient?","options":[{"id":"A","text":"Use multiple COPY commands with different transformations"},{"id":"B","text":"Create a view on top of the external stages and insert into the table"},{"id":"C","text":"Use a single COPY command with a SELECT statement that includes transformations"},{"id":"D","text":"Load the raw data first, then transform using a separate SQL statement"}],"correctAnswer":"C","explanation":"Using a single COPY command with a SELECT statement that includes transformations is the most efficient approach for loading and transforming data from multiple external stages. This method allows for transformations to be applied during the load process (ELT approach), reducing the need for intermediate storage and additional processing steps. The SELECT statement can include joins, filters, and other transformations to shape the data as it's being loaded.*"},{"id":9,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a streaming source that produces Avro-formatted messages. Which Snowflake feature should be used to efficiently process this streaming data?","options":[{"id":"A","text":"Snowflake Streams"},{"id":"B","text":"Snowpipe with Kafka connector"},{"id":"C","text":"External Tables with AUTO_REFRESH"},{"id":"D","text":"Snowflake Tasks with COPY commands"}],"correctAnswer":"B","explanation":"Snowpipe with the Kafka connector is specifically designed to efficiently process streaming data from Kafka topics, including Avro-formatted messages. The Kafka connector handles the consumption of messages from Kafka topics and stages them for Snowpipe to load into Snowflake tables. This provides a scalable, reliable solution for processing streaming data with minimal latency, making it ideal for Avro-formatted streaming sources.*"},{"id":10,"domain":"Data Movement (17 questions)","text":"A data engineer is setting up a data pipeline to load data from an external stage. Which parameter in the COPY command is used to specify the file format for the data being loaded?","options":[{"id":"A","text":"FILE_TYPE"},{"id":"B","text":"FORMAT_NAME"},{"id":"C","text":"DATA_FORMAT"},{"id":"D","text":"SOURCE_FORMAT"}],"correctAnswer":"B","explanation":"The FORMAT_NAME parameter in the COPY command is used to specify the file format for the data being loaded from an external stage. This parameter references a named file format object that has been created in Snowflake, which defines properties such as field delimiter, record delimiter, compression type, and other format-specific options. Using named file formats promotes reusability and consistency across data loading operations.*"},{"id":11,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a REST API that requires OAuth authentication. Which approach should be used to securely manage the authentication credentials in Snowflake?","options":[{"id":"A","text":"Store credentials in a Snowflake table with encryption"},{"id":"B","text":"Use Snowflake secure external functions"},{"id":"C","text":"Create a Snowflake network rule with embedded credentials"},{"id":"D","text":"Use Snowflake OAuth security integration"}],"correctAnswer":"D","explanation":"Snowflake OAuth security integration is the most secure approach for managing OAuth authentication credentials when accessing external REST APIs. This feature allows you to configure and store OAuth parameters securely within Snowflake, and then reference them when calling external services. This approach keeps sensitive credentials secure and centrally managed, following security best practices for authentication to external systems.*"},{"id":12,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution to handle late-arriving data in a Snowflake data warehouse. Which approach is most appropriate?","options":[{"id":"A","text":"Use Snowflake Streams to capture changes and apply them"},{"id":"B","text":"Implement a slowly changing dimension (SCD) Type 2 approach"},{"id":"C","text":"Use Snowflake Time Travel to reprocess historical data"},{"id":"D","text":"Create a separate table for late-arriving data"}],"correctAnswer":"B","explanation":"Implementing a slowly changing dimension (SCD) Type 2 approach is most appropriate for handling late-arriving data in a data warehouse. This method maintains historical accuracy by creating new records with effective dates when data changes, rather than overwriting existing records. This preserves the historical context of the data and ensures that analytics based on point-in-time accuracy remain valid, even when data arrives late or out of sequence.*"},{"id":13,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from multiple CSV files where some files may contain corrupted records. Which VALIDATION_MODE setting in the COPY command should be used to identify files with corrupted records before loading?","options":[{"id":"A","text":"VALIDATION_MODE = RETURN_n_ROWS"},{"id":"B","text":"VALIDATION_MODE = RETURN_ERRORS"},{"id":"C","text":"VALIDATION_MODE = RETURN_ALL_ERRORS"},{"id":"D","text":"VALIDATION_MODE = VALIDATE_ONLY"}],"correctAnswer":"D","explanation":"VALIDATION_MODE = VALIDATE_ONLY is the appropriate setting to identify files with corrupted records before actually loading the data. This mode validates the format and structure of the input files against the target table without loading any data. It returns any errors found during validation, allowing the data engineer to identify and fix issues before performing the actual data load. This approach prevents partial loads and the need to roll back data.*"},{"id":14,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution to replicate data from a Snowflake account in AWS to another Snowflake account in Azure. Which feature should be used?","options":[{"id":"A","text":"Snowflake Database Replication"},{"id":"B","text":"Snowflake Failover Groups"},{"id":"C","text":"Snowflake Data Sharing"},{"id":"D","text":"Snowflake External Tables"}],"correctAnswer":"A","explanation":"Snowflake Database Replication is specifically designed to replicate databases across different Snowflake accounts, including across different cloud providers (AWS to Azure in this case). This feature provides a way to maintain synchronized copies of databases in different regions or cloud platforms for disaster recovery, global data distribution, or cloud migration scenarios. Data Sharing provides read-only access but doesn't replicate the data, while the other options don't address cross-account replication needs.*"},{"id":15,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source that produces files with inconsistent column ordering. Which option in the COPY command should be used to handle this situation?","options":[{"id":"A","text":"MATCH_BY_COLUMN_NAME = TRUE"},{"id":"B","text":"FORCE = TRUE"},{"id":"C","text":"COLUMN_ORDER = FLEXIBLE"},{"id":"D","text":"PARSE_HEADER = TRUE"}],"correctAnswer":"A","explanation":"The MATCH_BY_COLUMN_NAME = TRUE option in the COPY command is specifically designed to handle files with inconsistent column ordering. This option instructs Snowflake to match columns in the source file to the target table based on column names rather than positions. This ensures that data is loaded correctly regardless of the order of columns in the source files, providing flexibility when dealing with inconsistent source data formats.*"},{"id":16,"domain":"Data Movement (17 questions)","text":"A data engineer is setting up a Snowpipe to load data from an AWS S3 bucket. Which AWS service is required to enable auto-ingest functionality?","options":[{"id":"A","text":"AWS Lambda"},{"id":"B","text":"Amazon SQS"},{"id":"C","text":"Amazon SNS"},{"id":"D","text":"AWS Glue"}],"correctAnswer":"B","explanation":"Amazon Simple Queue Service (SQS) is required to enable auto-ingest functionality for Snowpipe when loading data from an AWS S3 bucket. SQS is used to queue the S3 event notifications that are generated when new files are added to the bucket. Snowpipe then consumes these notifications from the SQS queue to trigger the automatic loading of new data files. This integration enables near-real-time data loading without manual intervention or scheduled tasks.*"},{"id":17,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source system that produces files with a custom format not directly supported by Snowflake's built-in file formats. Which approach should be used?","options":[{"id":"A","text":"Convert the files to a supported format before loading"},{"id":"B","text":"Use an external function to transform the data during loading"},{"id":"C","text":"Create a custom file format with the FORMAT_TYPE = 'CSV' and appropriate options"},{"id":"D","text":"Use Snowflake's VARIANT data type with JSON_PARSE"}],"correctAnswer":"A","explanation":"When dealing with custom file formats not directly supported by Snowflake, the most reliable approach is to convert the files to a supported format before loading. While Snowflake supports various file formats (CSV, JSON, Avro, Parquet, ORC, XML), truly custom formats require pre-processing. This can be done using external ETL tools, custom scripts, or cloud functions that transform the data into a Snowflake-compatible format before staging it for loading. This approach ensures compatibility and reduces potential errors during the load process.*"},{"id":18,"domain":"Data Movement (17 questions)","text":"A data engineer notices that a query joining two large tables is running slowly. The query filter includes a WHERE clause on a high-cardinality column. Which optimization technique would most likely improve performance?","options":[{"id":"A","text":"Add a materialized view on the joined tables"},{"id":"B","text":"Increase the warehouse size"},{"id":"C","text":"Add a clustering key on the filter column"},{"id":"D","text":"Enable query result caching"}],"correctAnswer":"C","explanation":"Adding a clustering key on the high-cardinality column used in the WHERE clause would most likely improve query performance. Clustering keys in Snowflake organize the micro-partitions to co-locate similar data values, enabling more efficient pruning during query execution. When a query filters on a clustered column, Snowflake can skip scanning micro-partitions that don't contain relevant data, significantly reducing the amount of data scanned and improving query performance. This is particularly effective for high-cardinality columns used frequently in filters.*"},{"id":19,"domain":"Data Movement (17 questions)","text":"A data engineer is analyzing query performance and notices that a specific query is scanning a large amount of data but returning only a small result set. Which Snowflake feature should be used to identify the specific operations causing this inefficiency?","options":[{"id":"A","text":"EXPLAIN"},{"id":"B","text":"QUERY_HISTORY view"},{"id":"C","text":"INFORMATION_SCHEMA.TABLES"},{"id":"D","text":"Query Profile"}],"correctAnswer":"D","explanation":"The Query Profile feature in Snowflake provides a detailed visual breakdown of query execution, showing exactly which operations are consuming the most resources and scanning the most data. Unlike EXPLAIN, which shows the execution plan before running the query, Query Profile shows actual runtime statistics after execution. This makes it the most effective tool for identifying specific operations causing inefficiencies in queries that scan large amounts of data but return small result sets. The profile shows metrics like partition pruning efficiency, bytes scanned, and execution time for each operation.*"},{"id":20,"domain":"Data Movement (17 questions)","text":"A data warehouse contains a large fact table with billions of rows that is frequently queried with filters on date ranges. Which approach would provide the best query performance?","options":[{"id":"A","text":"Create a materialized view for each common date range"},{"id":"B","text":"Implement a clustering key on the date column"},{"id":"C","text":"Increase the warehouse size to X-Large"},{"id":"D","text":"Create multiple smaller tables partitioned by date ranges"}],"correctAnswer":"B","explanation":"Implementing a clustering key on the date column is the most effective approach for improving query performance on a large fact table frequently filtered by date ranges. Clustering organizes the micro-partitions based on the date values, enabling Snowflake to efficiently prune irrelevant micro-partitions during query execution. This significantly reduces the amount of data scanned for date-filtered queries. Unlike creating multiple materialized views or tables, clustering provides a single, maintainable solution that works for any date range query, not just predefined ones, while avoiding data duplication.*"},{"id":21,"domain":"Data Movement (17 questions)","text":"A data engineer is optimizing a warehouse for a mixed workload of both short, interactive queries and long-running transformations. Which warehouse configuration would be most appropriate?","options":[{"id":"A","text":"A single X-Large warehouse for all workloads"},{"id":"B","text":"Separate warehouses for different query types with appropriate sizing"},{"id":"C","text":"A multi-cluster warehouse with maximum clusters set to 10"},{"id":"D","text":"A single warehouse with auto-suspend set to 60 seconds"}],"correctAnswer":"B","explanation":"Using separate warehouses for different query types with appropriate sizing is the most effective approach for a mixed workload. Short, interactive queries benefit from smaller warehouses with quick scaling, while long-running transformations may require larger, more stable resources. Separating these workloads prevents resource contention, where long-running jobs block interactive queries. This approach also allows for workload-specific configurations (auto-suspend timing, scaling policies, resource monitors) and provides better cost control and performance predictability than using a single warehouse for all workloads.*"},{"id":22,"domain":"Data Movement (17 questions)","text":"A data engineer notices that queries against a large table are not benefiting from micro-partition pruning despite having appropriate filters. What is the most likely cause?","options":[{"id":"A","text":"The warehouse size is too small"},{"id":"B","text":"The table has too many micro-partitions"},{"id":"C","text":"The table's clustering key is not aligned with common query filters"},{"id":"D","text":"Query result caching is disabled"}],"correctAnswer":"C","explanation":"The most likely cause for queries not benefiting from micro-partition pruning despite having appropriate filters is that the table's clustering key is not aligned with the common query filters. Effective micro-partition pruning depends on having clustering keys that match the columns frequently used in query filters. If the table is clustered on columns different from those used in query predicates, Snowflake cannot efficiently determine which micro-partitions to skip, resulting in full table scans and poor performance despite the presence of filters.*"},{"id":23,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs an aggregation over a large dataset with a GROUP BY clause on a high-cardinality column. Which approach would be most effective?","options":[{"id":"A","text":"Use a larger warehouse size"},{"id":"B","text":"Create a materialized view with the pre-computed aggregation"},{"id":"C","text":"Add a clustering key on the GROUP BY column"},{"id":"D","text":"Enable query result caching"}],"correctAnswer":"C","explanation":"Adding a clustering key on the high-cardinality column used in the GROUP BY clause would be most effective for optimizing this query. Clustering improves the locality of data with similar values, which significantly enhances the performance of GROUP BY operations. When data is clustered by the grouping column, Snowflake can process each group more efficiently by accessing co-located data, reducing shuffling and improving aggregation performance. While a materialized view could help, it's less flexible if query parameters change, and simply increasing warehouse size doesn't address the fundamental data organization issue.*"},{"id":24,"domain":"Data Movement (17 questions)","text":"A data engineer is troubleshooting a slow-running query and notices in the query profile that a large amount of data is being spilled to remote storage. What is the most likely cause?","options":[{"id":"A","text":"The warehouse has insufficient memory for the operation"},{"id":"B","text":"The query is scanning too many micro-partitions"},{"id":"C","text":"The table is not properly clustered"},{"id":"D","text":"The query cache is full"}],"correctAnswer":"A","explanation":"Data being spilled to remote storage during query execution indicates that the warehouse has insufficient memory for the operation. When Snowflake cannot fit all the data needed for an operation (like a large join or aggregation) in memory, it \\"spills\\" excess data to remote storage, which significantly slows down processing due to the additional I/O operations. This is typically caused by using a warehouse size that's too small for the data volume being processed or by inefficient queries that generate large intermediate results. Increasing the warehouse size or optimizing the query to reduce memory requirements would address this issue.*"},{"id":25,"domain":"Data Movement (17 questions)","text":"A data engineer needs to improve the performance of a dashboard that queries the same large dataset multiple times with different parameters. Which Snowflake feature would be most effective?","options":[{"id":"A","text":"Result caching"},{"id":"B","text":"Materialized views"},{"id":"C","text":"Multi-cluster warehouses"},{"id":"D","text":"Zero-copy cloning"}],"correctAnswer":"B","explanation":"Materialized views would be most effective for improving dashboard performance that queries the same large dataset with different parameters. Materialized views pre-compute and store query results, including aggregations and joins, and automatically maintain these results as the underlying data changes. Unlike result caching, which only helps if the exact same query is run again, materialized views can accelerate a variety of queries against the same base tables. This is particularly valuable for dashboards where the base data is consistent but is analyzed from different angles or with different filters.*"},{"id":26,"domain":"Data Movement (17 questions)","text":"A data engineer is optimizing a large join operation between two tables. The join condition uses a column that has many duplicate values. Which join type would likely be most efficient in Snowflake?","options":[{"id":"A","text":"MERGE join"},{"id":"B","text":"HASH join"},{"id":"C","text":"NESTED LOOP join"},{"id":"D","text":"SORT MERGE join"}],"correctAnswer":"B","explanation":"A HASH join would likely be most efficient for joining tables on a column with many duplicate values in Snowflake. Snowflake's query optimizer typically chooses hash joins for equi-joins (using equality conditions), especially when joining large tables. Hash joins build a hash table on the smaller table and then probe it with values from the larger table, making them efficient for columns with duplicates. While Snowflake's optimizer automatically selects the join strategy, understanding that hash joins are typically used for this scenario helps in designing efficient join operations and interpreting query plans.*"},{"id":27,"domain":"Data Movement (17 questions)","text":"A data engineer notices that a critical ETL process with multiple transformation steps is running slowly. Which approach would most effectively improve the end-to-end performance?","options":[{"id":"A","text":"Increase the warehouse size for all transformations"},{"id":"B","text":"Use a separate warehouse for each transformation step"},{"id":"C","text":"Combine all transformations into a single SQL statement"},{"id":"D","text":"Analyze the query profile to identify and optimize the slowest steps"}],"correctAnswer":"D","explanation":"Analyzing the query profile to identify and optimize the slowest steps is the most effective approach for improving end-to-end ETL performance. This targeted approach focuses resources on the specific bottlenecks rather than making broad changes that may not address the root causes. By examining the query profile, the data engineer can identify which specific transformations are consuming the most time or resources and then apply appropriate optimizations (clustering, query rewrites, materialized views, etc.) to those specific steps. This methodical approach yields better results than general solutions like increasing warehouse size or restructuring the entire process.*"},{"id":28,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs a window function calculation over a large dataset ordered by a timestamp column. Which approach would most improve performance?","options":[{"id":"A","text":"Add a clustering key on the timestamp column"},{"id":"B","text":"Use a larger warehouse size"},{"id":"C","text":"Create a materialized view with the window function pre-computed"},{"id":"D","text":"Rewrite the query to use a GROUP BY instead of a window function"}],"correctAnswer":"A","explanation":"Adding a clustering key on the timestamp column would most improve performance for a query with window functions ordered by that timestamp. Window functions that operate on ordered data benefit significantly from clustering because it physically organizes the data in a way that aligns with the ORDER BY clause. When data is clustered by timestamp, the window function can process consecutive rows more efficiently with reduced data movement. This is particularly effective for large datasets where the ordering operation would otherwise require substantial resources to sort the data during query execution.*"},{"id":29,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution for a reporting system that needs to query the same dataset multiple times per hour. The data is updated daily during a nightly batch process. Which feature would provide the best query performance?","options":[{"id":"A","text":"Automatic clustering"},{"id":"B","text":"Search optimization"},{"id":"C","text":"Query result cache"},{"id":"D","text":"Multi-cluster warehouses"}],"correctAnswer":"C","explanation":"Query result cache would provide the best performance for a reporting system that repeatedly queries the same dataset that only updates once daily. Snowflake automatically caches the results of each query for 24 hours, and subsequent identical queries can retrieve results from the cache instead of re-executing the query. Since the data only changes during the nightly batch process, cached results remain valid throughout the day. This provides the fastest possible response time for repeated queries without consuming additional compute resources, making it ideal for reporting systems with frequent, identical queries against relatively static data.*"},{"id":30,"domain":"Data Movement (17 questions)","text":"A data engineer is working with a table that contains both hot and cold data, where queries typically only access the most recent data. Which approach would optimize both storage costs and query performance?","options":[{"id":"A","text":"Implement a clustering key on the date column"},{"id":"B","text":"Use table partitioning to separate hot and cold data"},{"id":"C","text":"Create a materialized view for the hot data"},{"id":"D","text":"Implement a data retention policy using Time Travel"}],"correctAnswer":"A","explanation":"Implementing a clustering key on the date column is the most effective approach for optimizing both storage costs and query performance when working with hot and cold data in the same table. Clustering ensures that recent (hot) data is co-located in the same micro-partitions, enabling efficient pruning when queries filter on recent dates. This significantly improves query performance by reducing the amount of data scanned. Additionally, Snowflake's automatic clustering maintenance focuses on the most frequently accessed micro-partitions, naturally optimizing for hot data access patterns while minimizing maintenance overhead for cold data regions.*"},{"id":31,"domain":"Data Movement (17 questions)","text":"A data engineer notices that a query with a JOIN operation is performing poorly. The query profile shows that one of the joined tables is being broadcast to all nodes. Which condition would cause Snowflake to choose a broadcast join strategy?","options":[{"id":"A","text":"When joining tables of similar size"},{"id":"B","text":"When one table is significantly smaller than the other"},{"id":"C","text":"When both tables have clustering keys"},{"id":"D","text":"When the join is on a non-equality condition"}],"correctAnswer":"B","explanation":"Snowflake chooses a broadcast join strategy when one table is significantly smaller than the other. In this approach, the smaller table is broadcast (copied) to all compute nodes where the larger table's data resides. This eliminates the need to shuffle the larger table's data across the network, which would be more expensive. Broadcast joins are efficient when the smaller table can fit comfortably in memory. Understanding when Snowflake uses this strategy helps in designing efficient joins and interpreting query profiles to diagnose performance issues.*"},{"id":32,"domain":"Data Movement (17 questions)","text":"A data engineer accidentally dropped a critical table and needs to recover it. The table was dropped 3 days ago, and the Time Travel retention period is set to the default value. Can the table be recovered?","options":[{"id":"A","text":"Yes, using the UNDROP command"},{"id":"B","text":"No, the default Time Travel retention period is only 1 day"},{"id":"C","text":"Yes, using the CLONE command with a timestamp"},{"id":"D","text":"No, dropped tables cannot be recovered"}],"correctAnswer":"B","explanation":"The default Time Travel retention period in Snowflake is 1 day (24 hours), so a table dropped 3 days ago cannot be recovered using Time Travel features. Time Travel allows access to data that has been changed or deleted within the retention period, but once that period expires, the data moves to Fail-safe (which is not user-accessible). To enable recovery beyond the default period, the Time Travel retention would need to have been explicitly extended (up to 90 days with Enterprise Edition) before the table was dropped.*"},{"id":33,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows querying historical data as of specific points in time within the last 60 days. Which Snowflake feature and edition should be used?","options":[{"id":"A","text":"Time Travel with Standard Edition"},{"id":"B","text":"Time Travel with Enterprise Edition"},{"id":"C","text":"Fail-safe with Standard Edition"},{"id":"D","text":"Zero-copy cloning with any edition"}],"correctAnswer":"B","explanation":"Time Travel with Enterprise Edition is the correct solution for querying historical data within the last 60 days. Snowflake's Enterprise Edition allows extending the Time Travel retention period up to 90 days, while Standard Edition limits it to just 1 day. Time Travel enables querying data as it existed at specific points in time using the AT or BEFORE clause in queries. This feature is ideal for historical analysis, auditing, and recovering from data errors or corruption. Fail-safe is not user-accessible, and zero-copy cloning creates a new object rather than providing temporal query capabilities.*"},{"id":34,"domain":"Data Movement (17 questions)","text":"A data engineer needs to create a development environment with a copy of a production database for testing. The copy should not consume additional storage space initially but should allow independent modifications. Which Snowflake feature should be used?","options":[{"id":"A","text":"Database replication"},{"id":"B","text":"Zero-copy cloning"},{"id":"C","text":"Time Travel"},{"id":"D","text":"Fail-safe"}],"correctAnswer":"B","explanation":"Zero-copy cloning is the ideal feature for creating a development environment that initially shares storage with production but allows independent modifications. When a clone is created, it references the same micro-partitions as the source object without duplicating data. Only when changes are made to either the source or the clone does Snowflake create new micro-partitions to store the differences. This provides storage efficiency while allowing the development environment to diverge from production as needed. This approach is perfect for testing, development, and what-if scenarios without incurring significant additional storage costs.*"},{"id":35,"domain":"Data Movement (17 questions)","text":"A data engineer needs to ensure that data can be recovered in case of accidental deletion or corruption. Which combination of Snowflake features provides the longest possible recovery window?","options":[{"id":"A","text":"Time Travel (90 days) + Fail-safe (7 days)"},{"id":"B","text":"Time Travel (1 day) + Fail-safe (7 days)"},{"id":"C","text":"Time Travel (90 days) + Database replication"},{"id":"D","text":"Zero-copy cloning + Time Travel (1 day)"}],"correctAnswer":"A","explanation":"The combination of Time Travel set to its maximum of 90 days (available in Enterprise Edition) plus the automatic 7-day Fail-safe period provides the longest possible recovery window in Snowflake, totaling 97 days. Time Travel allows user-controlled recovery of dropped or modified objects within its retention period, while Fail-safe is an additional 7-day period after Time Travel expiration during which Snowflake can recover data (though this requires contacting Snowflake Support). This combination maximizes data protection against accidental deletion or corruption while balancing storage costs.*"},{"id":36,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution to protect against ransomware attacks that might corrupt data in Snowflake. Which approach provides the best protection?","options":[{"id":"A","text":"Implement column-level encryption"},{"id":"B","text":"Set a longer Time Travel retention period"},{"id":"C","text":"Create regular database clones with different ownership"},{"id":"D","text":"Enable multi-factor authentication"}],"correctAnswer":"C","explanation":"Creating regular database clones with different ownership provides the best protection against ransomware attacks that might corrupt data in Snowflake. By maintaining clones with different ownership and restricted access permissions, you create isolated copies that would remain unaffected if the primary data were compromised. This approach creates an \\"air gap\\" between production data and backups, ensuring that malicious actions affecting one copy cannot propagate to others. While Time Travel helps with recovery, it doesn't protect against systematic corruption if an attacker gains access to your account. The clone approach provides a more robust defense-in-depth strategy.*"},{"id":37,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a cost-effective solution for long-term storage of historical data that is rarely accessed but must remain queryable. Which approach should be used?","options":[{"id":"A","text":"Export data to external storage and create external tables"},{"id":"B","text":"Maintain data in Snowflake with extended Time Travel"},{"id":"C","text":"Create a separate database with a smaller warehouse"},{"id":"D","text":"Implement automatic clustering on time-based columns"}],"correctAnswer":"A","explanation":"Exporting rarely accessed historical data to external storage (like S3, Azure Blob, or GCS) and creating external tables is the most cost-effective solution for long-term queryable storage. This approach leverages Snowflake's external tables feature, which allows querying data stored in cloud storage without loading it into Snowflake storage. Since Snowflake storage costs are higher than raw cloud storage costs, this significantly reduces storage expenses for cold data while maintaining query capabilities. When the data needs to be queried, Snowflake can access it directly from external storage, though with somewhat reduced performance compared to internal tables.*"},{"id":38,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a data retention strategy and needs to understand the difference between Time Travel and Fail-safe. Which statement is correct?","options":[{"id":"A","text":"Both Time Travel and Fail-safe data can be accessed directly by users"},{"id":"B","text":"Time Travel is user-configurable, while Fail-safe is fixed at 7 days"},{"id":"C","text":"Fail-safe provides faster data recovery than Time Travel"},{"id":"D","text":"Time Travel is only available in Enterprise Edition"}],"correctAnswer":"B","explanation":"The correct statement is that Time Travel is user-configurable (1-90 days depending on edition), while Fail-safe is fixed at 7 days. Time Travel retention can be set at the account, database, schema, or table level, giving users control over the recovery window based on business needs and cost considerations. In contrast, Fail-safe is a fixed 7-day period that begins after Time Travel retention expires and cannot be modified. Additionally, Time Travel data is directly accessible by users, while Fail-safe data can only be recovered by Snowflake Support, making this a key distinction in recovery capabilities and planning.*"},{"id":39,"domain":"Data Movement (17 questions)","text":"A data engineer needs to create a backup of a production database that will not be affected by changes to the source database and will persist beyond the Time Travel retention period. Which approach should be used?","options":[{"id":"A","text":"Create a zero-copy clone of the database"},{"id":"B","text":"Use CREATE DATABASE ... CLONE"},{"id":"C","text":"Export the database to external storage"},{"id":"D","text":"Set a longer Time Travel retention period"}],"correctAnswer":"A","explanation":"Creating a zero-copy clone of the database is the most appropriate approach for creating a persistent backup that's unaffected by changes to the source. Once created, a clone is independent of its source - changes to the source don't affect the clone, and the clone persists regardless of what happens to the source, including beyond Time Travel retention periods. While \\"CREATE DATABASE ... CLONE\\" is the specific syntax used, the conceptual answer focuses on the cloning approach. This provides a complete, queryable backup with minimal storage overhead, as only the differences between the clone and source consume additional storage.*"},{"id":40,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows for recovering a table to any point in time within the last 30 days. The organization uses Snowflake Standard Edition. What should the data engineer do?","options":[{"id":"A","text":"Enable Time Travel with a 30-day retention period"},{"id":"B","text":"Upgrade to Enterprise Edition to extend Time Travel beyond 1 day"},{"id":"C","text":"Create daily snapshots using zero-copy clones"},{"id":"D","text":"Use Fail-safe for point-in-time recovery"}],"correctAnswer":"B","explanation":"To enable point-in-time recovery within the last 30 days, the organization needs to upgrade to Enterprise Edition. Standard Edition limits Time Travel retention to a maximum of 1 day, which doesn't meet the 30-day requirement. Enterprise Edition allows extending Time Travel retention up to 90 days, enabling the required 30-day recovery window. While creating daily snapshots with clones could provide discrete recovery points, it wouldn't allow recovery to any point in time. Fail-safe is not user-accessible and doesn't provide point-in-time recovery capabilities.*"},{"id":41,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows specific users to see only masked versions of sensitive columns in a customer table, while allowing analysts to see the actual data. Which Snowflake feature should be used?","options":[{"id":"A","text":"Row-level security"},{"id":"B","text":"Column-level security"},{"id":"C","text":"Dynamic data masking"},{"id":"D","text":"Secure views"}],"correctAnswer":"C","explanation":"Dynamic data masking is the most appropriate feature for this requirement. It allows defining masking policies that show different versions of the same data to different users based on their roles and privileges. Unlike column-level security which simply grants or denies access to entire columns, dynamic masking allows some users to see the actual data while others see masked versions (like partial credit card numbers or anonymized values). This provides fine-grained access control without duplicating data or creating multiple views, making it ideal for scenarios where different user groups need different levels of access to sensitive information.*"},{"id":42,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a security model where users can only see customer data for their assigned region. Which Snowflake feature should be used?","options":[{"id":"A","text":"Role-based access control"},{"id":"B","text":"Column-level security"},{"id":"C","text":"Row-level security"},{"id":"D","text":"Secure views with session variables"}],"correctAnswer":"C","explanation":"Row-level security is the most appropriate feature for restricting access to customer data based on a user's assigned region. This feature allows defining security policies that filter rows dynamically based on attributes of the current session (like the user's region assignment). When users query the table, they only see rows that match their authorized regions. This provides fine-grained access control at the row level without requiring separate tables or views for each region, making it ideal for implementing data segregation based on attributes like region, department, or customer.*"},{"id":43,"domain":"Data Movement (17 questions)","text":"A data engineer needs to share sensitive data with a partner organization that doesn't have a Snowflake account. Which approach provides the most secure method for sharing this data?","options":[{"id":"A","text":"Create a reader account for the partner"},{"id":"B","text":"Export the data to a secure FTP server"},{"id":"C","text":"Use Snowflake Data Exchange"},{"id":"D","text":"Create database shares with IP restrictions"}],"correctAnswer":"A","explanation":"Creating a reader account is the most secure method for sharing sensitive data with a partner organization that doesn't have a Snowflake account. A reader account is a special type of Snowflake account that allows external organizations to securely access shared data without needing their own full Snowflake account. The data provider maintains control over what data is shared and can revoke access at any time. This approach keeps the data within Snowflake's secure environment, eliminating the risks associated with exporting data, while providing a controlled, auditable access method for external partners.*"},{"id":44,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that automatically classifies and tags sensitive data in new tables as they are created. Which approach should be used?","options":[{"id":"A","text":"Implement tag-based masking policies"},{"id":"B","text":"Use Snowflake's automatic classification service"},{"id":"C","text":"Create a stored procedure triggered on table creation"},{"id":"D","text":"Implement column-level security policies"}],"correctAnswer":"C","explanation":"Creating a stored procedure triggered on table creation is the most effective approach for automatically classifying and tagging sensitive data in new tables. This procedure can analyze column names, sample data, or metadata to identify potentially sensitive information and apply appropriate tags. By registering this procedure to execute automatically when new tables are created (using event notifications or regular scheduled tasks), the data engineer can ensure consistent classification without manual intervention. This programmatic approach provides flexibility to implement custom classification logic while leveraging Snowflake's native tagging capabilities for governance.*"},{"id":45,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that prevents any single user from accessing both a customer's personally identifiable information (PII) and their financial transaction data. Which approach should be used?","options":[{"id":"A","text":"Implement column-level security with mutually exclusive roles"},{"id":"B","text":"Create separate secure views for PII and transaction data"},{"id":"C","text":"Use dynamic data masking with role-based policies"},{"id":"D","text":"Implement row-level security based on user attributes"}],"correctAnswer":"A","explanation":"Implementing column-level security with mutually exclusive roles is the most effective approach for enforcing separation of duties between PII and financial transaction data. By creating roles that have access to either PII columns or financial transaction columns, but never both, and ensuring users can only activate one of these roles at a time, the data engineer can enforce strict segregation of access. This approach directly addresses the requirement to prevent any single user from accessing both types of sensitive data simultaneously, implementing a fundamental security principle known as separation of duties.*"},{"id":46,"domain":"Data Movement (17 questions)","text":"A data engineer needs to audit all query access to tables containing sensitive customer data. Which Snowflake feature should be used?","options":[{"id":"A","text":"ACCOUNT_USAGE.ACCESS_HISTORY"},{"id":"B","text":"ACCOUNT_USAGE.QUERY_HISTORY"},{"id":"C","text":"Snowflake's automatic audit logging"},{"id":"D","text":"Custom logging with stored procedures"}],"correctAnswer":"B","explanation":"ACCOUNT_USAGE.QUERY_HISTORY is the most appropriate feature for auditing query access to sensitive tables. This view in the Snowflake shared database contains a record of all queries executed in the account, including the SQL text, user information, timestamp, and affected objects. By querying this view with filters for the sensitive tables, the data engineer can create comprehensive audit reports showing who accessed what data and when. This built-in capability provides the necessary visibility for compliance and security monitoring without requiring custom implementation, making it the most direct solution for the auditing requirement.*"},{"id":47,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows granting access to all tables in a schema without explicitly naming each table, including tables that will be created in the future. Which approach should be used?","options":[{"id":"A","text":"Use GRANT SELECT ON ALL TABLES IN SCHEMA"},{"id":"B","text":"Create a role hierarchy with schema ownership"},{"id":"C","text":"Use GRANT SELECT ON FUTURE TABLES IN SCHEMA"},{"id":"D","text":"Implement managed access schemas"}],"correctAnswer":"C","explanation":"Using GRANT SELECT ON FUTURE TABLES IN SCHEMA is the correct approach for granting access to all tables in a schema, including those that will be created in the future. This command specifically grants the specified privilege on all tables that will be created in the schema after the grant is issued. Combined with a separate grant for existing tables (GRANT SELECT ON ALL TABLES IN SCHEMA), this ensures comprehensive access without requiring manual grants for each new table. This approach simplifies privilege management in dynamic environments where tables are frequently created or modified.*"},{"id":48,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows tracking the lineage of data as it moves through various transformations in Snowflake. Which feature or approach should be used?","options":[{"id":"A","text":"INFORMATION_SCHEMA.TABLE_STORAGE_METRICS"},{"id":"B","text":"ACCOUNT_USAGE.ACCESS_HISTORY"},{"id":"C","text":"Object tagging with custom metadata"},{"id":"D","text":"Snowflake's native data lineage tracking"}],"correctAnswer":"C","explanation":"Object tagging with custom metadata is the most effective approach for tracking data lineage in Snowflake. By creating and applying tags that document source systems, transformation steps, data quality checks, and other lineage information to databases, schemas, tables, and columns, the data engineer can implement a comprehensive lineage tracking system. These tags can be queried through Snowflake's metadata views to generate lineage reports and diagrams. While Snowflake doesn't have native end-to-end lineage tracking, this tagging approach leverages Snowflake's metadata capabilities to implement a flexible, queryable lineage solution.*"},{"id":49,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that ensures all copies of production data used in development environments have sensitive customer information automatically masked. Which approach should be used?","options":[{"id":"A","text":"Create secure views of production for development use"},{"id":"B","text":"Implement dynamic data masking policies on production tables"},{"id":"C","text":"Use post-clone scripts to apply masking when cloning databases"},{"id":"D","text":"Create separate ETL processes for development data"}],"correctAnswer":"C","explanation":"Using post-clone scripts to apply masking when cloning databases is the most effective approach for ensuring sensitive data is automatically masked in development environments. Post-clone scripts execute automatically after a database is cloned, allowing the data engineer to apply masking policies, remove sensitive data, or transform values specifically in the cloned environment. This approach ensures that all development copies created through cloning have consistent masking applied without affecting the production environment or requiring manual intervention. It provides a systematic, automated solution for protecting sensitive data in non-production environments.*"},{"id":50,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that executes a series of dependent data transformation steps in sequence, with each step starting only after the previous step successfully completes. Which Snowflake feature should be used?","options":[{"id":"A","text":"Stored procedures"},{"id":"B","text":"User-defined functions"},{"id":"C","text":"Tasks with dependencies"},{"id":"D","text":"Streams with triggers"}],"correctAnswer":"C","explanation":"Tasks with dependencies is the most appropriate feature for implementing sequential, dependent data transformation steps in Snowflake. Tasks allow scheduling SQL commands or stored procedures to run at defined intervals or in response to events. By creating a DAG (directed acyclic graph) of tasks with predecessor/successor relationships, the data engineer can ensure that each step only executes after its predecessor successfully completes. This provides a native, serverless orchestration solution within Snowflake for managing complex transformation workflows with proper dependency management and error handling.*"},{"id":51,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that captures changes to a source table and applies them to a target table incrementally. Which Snowflake feature should be used?","options":[{"id":"A","text":"Zero-copy cloning"},{"id":"B","text":"Time Travel"},{"id":"C","text":"Streams"},{"id":"D","text":"External tables"}],"correctAnswer":"C","explanation":"Streams is the most appropriate Snowflake feature for capturing and applying incremental changes from a source table to a target table. Streams track DML changes (inserts, updates, deletes) to a table and make those changes available for processing. When combined with tasks, streams enable building efficient change data capture (CDC) pipelines that only process modified data rather than reprocessing the entire dataset. This approach minimizes processing overhead and ensures that the target table stays synchronized with the source while only processing the delta of changes, making it ideal for incremental data transformation scenarios.*"},{"id":52,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement complex data transformations that require procedural logic, loops, and error handling. Which Snowflake feature should be used?","options":[{"id":"A","text":"SQL UDFs"},{"id":"B","text":"Stored procedures"},{"id":"C","text":"Tasks"},{"id":"D","text":"Streams"}],"correctAnswer":"B","explanation":"Stored procedures are the most appropriate feature for implementing complex data transformations requiring procedural logic, loops, and error handling in Snowflake. Unlike SQL UDFs which are limited to returning a single value, stored procedures support full procedural programming with control structures (IF/ELSE, LOOP), error handling (TRY/CATCH), and multiple SQL operations. Snowflake supports stored procedures written in JavaScript, SQL, and other languages, providing the flexibility needed for complex transformation logic that goes beyond what can be expressed in standard SQL alone.*"},{"id":53,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that automatically detects schema changes in a source table and propagates those changes to a target table. Which approach should be used?","options":[{"id":"A","text":"Create a stored procedure that uses INFORMATION_SCHEMA to detect and apply changes"},{"id":"B","text":"Use Snowflake Streams with schema evolution enabled"},{"id":"C","text":"Implement dynamic views that automatically adapt to schema changes"},{"id":"D","text":"Use VARIANT columns to store data in a schema-flexible format"}],"correctAnswer":"A","explanation":"Creating a stored procedure that uses INFORMATION_SCHEMA to detect and apply schema changes is the most comprehensive approach for automatically propagating schema changes from source to target tables. By querying INFORMATION_SCHEMA.COLUMNS for both tables and comparing the results, the procedure can identify new, modified, or deleted columns and generate the appropriate ALTER TABLE statements to synchronize the target schema with the source. This programmatic approach provides complete control over how different types of schema changes are handled, including column additions, data type changes, and column removals, with appropriate error handling and logging.*"},{"id":54,"domain":"Data Movement (17 questions)","text":"A data engineer needs to transform semi-structured JSON data stored in a VARIANT column into a relational format. Which SQL function should be used?","options":[{"id":"A","text":"PARSE_JSON"},{"id":"B","text":"GET_PATH"},{"id":"C","text":"FLATTEN"},{"id":"D","text":"TO_JSON"}],"correctAnswer":"C","explanation":"The FLATTEN function is the most appropriate choice for transforming semi-structured JSON data in a VARIANT column into a relational format. FLATTEN performs a lateral join that expands nested arrays into multiple rows, effectively normalizing hierarchical data into a tabular structure. When combined with other Snowflake JSON functions like GET, PARSE_JSON, or dot notation, FLATTEN enables comprehensive transformation of complex nested structures into relational tables. This is particularly useful for analytics on semi-structured data that needs to be joined with traditional relational data or analyzed using standard SQL aggregations and grouping operations.*"},{"id":55,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs different transformations on data based on its source system. The logic for determining the transformation rules is complex. Which approach should be used?","options":[{"id":"A","text":"Create separate transformation queries for each source system"},{"id":"B","text":"Implement a JavaScript stored procedure with conditional logic"},{"id":"C","text":"Use SQL CASE statements in a view"},{"id":"D","text":"Create a lookup table with transformation rules"}],"correctAnswer":"B","explanation":"Implementing a JavaScript stored procedure with conditional logic is the most flexible approach for handling complex transformation rules based on source systems. JavaScript stored procedures in Snowflake support sophisticated programming constructs, including complex conditional logic, custom functions, and external API calls if needed. This approach allows encapsulating the entire transformation logic in a single, maintainable procedure that can handle the complexity of different rules for different source systems, including edge cases and exceptions that might be difficult to express in pure SQL constructs like CASE statements.*"},{"id":56,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs data quality checks on transformed data before loading it into a target table. If the checks fail, the process should be halted and an alert should be sent. Which approach should be used?","options":[{"id":"A","text":"Use a stored procedure with error handling and custom logging"},{"id":"B","text":"Implement a stream to capture failed records"},{"id":"C","text":"Create a task that checks data quality before loading"},{"id":"D","text":"Use a secure view with row-level security to filter invalid data"}],"correctAnswer":"A","explanation":"Using a stored procedure with error handling and custom logging is the most comprehensive approach for implementing data quality checks with alerting capabilities. Stored procedures allow combining SQL validation queries with conditional logic to evaluate results, transaction control to commit or rollback based on quality checks, and integration with external alerting mechanisms through API calls or Snowflake features like notifications. This approach provides complete control over the validation process, error handling, and alerting workflow, making it ideal for implementing robust data quality gates in transformation pipelines.*"},{"id":57,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that maintains a slowly changing dimension (SCD) Type 2 for customer data, preserving the history of changes. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake Streams to capture changes and apply them with effective dates"},{"id":"B","text":"Implement a MERGE statement that handles both inserts and updates"},{"id":"C","text":"Create a view that uses Time Travel to show historical data"},{"id":"D","text":"Use zero-copy cloning to create daily snapshots"}],"correctAnswer":"A","explanation":"Using Snowflake Streams to capture changes and apply them with effective dates is the most efficient approach for implementing a Slowly Changing Dimension (SCD) Type 2. Streams automatically track changes to the source table, making it easy to identify records that need historical preservation. By combining streams with a transformation process that adds effective dates and end dates to records, the data engineer can efficiently implement SCD Type 2 semantics that maintain the complete history of changes while minimizing processing overhead by only handling changed records rather than reprocessing the entire dimension.*"},{"id":58,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that executes a complex transformation workflow on a schedule, with different steps running at different frequencies. Which approach should be used?","options":[{"id":"A","text":"Create multiple independent tasks with different schedules"},{"id":"B","text":"Use a single task with a CASE statement to determine which transformations to run"},{"id":"C","text":"Implement a stored procedure that uses the current time to decide which steps to execute"},{"id":"D","text":"Create a task tree with different schedules for different branches"}],"correctAnswer":"A","explanation":"Creating multiple independent tasks with different schedules is the most appropriate approach for implementing transformation steps that need to run at different frequencies. Each task can be configured with its own schedule expression (CRON format in Snowflake) to define exactly when it should execute, allowing for precise control over execution timing. This approach maintains separation of concerns between different transformation steps while leveraging Snowflake's native scheduling capabilities. For steps with dependencies, additional task relationships can be defined to ensure proper sequencing while still maintaining independent scheduling.*"},{"id":59,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs complex string manipulations and regular expression operations as part of a data transformation pipeline. Which Snowflake feature provides the most comprehensive support for these operations?","options":[{"id":"A","text":"SQL UDFs"},{"id":"B","text":"JavaScript UDFs"},{"id":"C","text":"Built-in SQL string functions"},{"id":"D","text":"External functions"}],"correctAnswer":"C","explanation":"Snowflake's built-in SQL string functions provide the most comprehensive and efficient support for string manipulations and regular expression operations in data transformation pipelines. Snowflake offers a rich set of native functions including REGEXP_REPLACE, REGEXP_SUBSTR, SPLIT, TRANSLATE, and many others that can handle complex string transformations directly in SQL. These functions are optimized for performance within Snowflake's engine and can operate on entire columns of data in parallel. While JavaScript UDFs offer programming flexibility, the native SQL functions provide better performance and integration with Snowflake's optimization capabilities for large-scale data processing.*"},{"id":60,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs different transformations on data based on the current processing date, with special handling for month-end, quarter-end, and year-end processing. Which approach should be used?","options":[{"id":"A","text":"Create separate tasks for each type of processing period"},{"id":"B","text":"Use a stored procedure with date logic to determine the appropriate transformations"},{"id":"C","text":"Implement a CASE statement in a SQL query based on date functions"},{"id":"D","text":"Create a calendar reference table to look up processing types"}],"correctAnswer":"B","explanation":"Using a stored procedure with date logic is the most flexible approach for implementing different transformations based on the current processing date. A stored procedure can incorporate sophisticated date calculations to determine if the current date is a month-end, quarter-end, or year-end, and then execute the appropriate transformation logic for each scenario. This approach centralizes the date determination logic and transformation selection in a single, maintainable component while providing the procedural capabilities needed to handle complex conditional processing based on temporal patterns and exceptions.*"},{"id":61,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs aggregations on large datasets with high cardinality group-by columns. The aggregations need to be refreshed daily. Which approach would provide the best performance?","options":[{"id":"A","text":"Create a materialized view with the aggregation logic"},{"id":"B","text":"Use a task to calculate and store aggregates in a separate table"},{"id":"C","text":"Implement a clustering key on the group-by columns"},{"id":"D","text":"Use a JavaScript UDF to perform custom aggregations"}],"correctAnswer":"B","explanation":"Using a task to calculate and store aggregates in a separate table is the most effective approach for high-performance aggregations on large datasets with high cardinality group-by columns. This approach pre-computes the aggregations during a scheduled maintenance window and stores the results in a dedicated table optimized for query performance. Unlike materialized views which have certain limitations with complex aggregations, this approach provides complete control over the aggregation logic, indexing strategy, and refresh schedule. The task can be scheduled to run daily, ensuring the aggregated data is refreshed at the required frequency while minimizing the performance impact on user queries.*"},{"id":62,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that transforms data from a third-party API and loads it into Snowflake. The API returns complex nested JSON structures. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake's REST API integration to directly query the third-party API"},{"id":"B","text":"Create an external function that calls the API and processes the results"},{"id":"C","text":"Use a serverless function to fetch, transform, and load the data via Snowflake's native connectors"},{"id":"D","text":"Implement a JavaScript stored procedure that uses the Snowflake HTTP client to call the API"}],"correctAnswer":"D","explanation":"Implementing a JavaScript stored procedure that uses the Snowflake HTTP client is the most direct approach for transforming data from a third-party API. Snowflake's JavaScript stored procedures can use the built-in HTTP client to make API calls, process the returned JSON using JavaScript's native JSON handling capabilities, and then insert the transformed data into Snowflake tables. This approach keeps the entire ETL process within Snowflake, eliminating the need for external components or data movement, while providing the programming flexibility needed to handle complex nested JSON structures and API-specific requirements.*"},{"id":63,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs complex window functions across multiple dimensions with different partition and order specifications. Which approach would provide the most maintainable solution?","options":[{"id":"A","text":"Create multiple CTEs, each with a specific window function"},{"id":"B","text":"Use subqueries with different GROUP BY clauses"},{"id":"C","text":"Implement a JavaScript UDF that performs custom windowing"},{"id":"D","text":"Create a stored procedure that generates dynamic SQL"}],"correctAnswer":"A","explanation":"Creating multiple Common Table Expressions (CTEs), each with a specific window function, provides the most maintainable solution for complex window calculations across multiple dimensions. This approach breaks down the complex logic into modular, readable components where each CTE handles a specific windowing operation with its own partition and order specifications. The final query can then join or combine these intermediate results as needed. This modular structure improves readability, debugging, and maintenance compared to nested subqueries or dynamic SQL approaches, while leveraging Snowflake's optimization capabilities for window functions.*"},{"id":64,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs data transformations requiring custom mathematical operations not available in standard SQL functions. Which approach should be used?","options":[{"id":"A","text":"Create a SQL UDF with mathematical formulas"},{"id":"B","text":"Implement a JavaScript UDF with custom algorithms"},{"id":"C","text":"Use a Python UDF for advanced mathematical operations"},{"id":"D","text":"Create a stored procedure with mathematical logic"}],"correctAnswer":"C","explanation":"Using a Python UDF (User-Defined Function) is the most appropriate approach for implementing custom mathematical operations not available in standard SQL. Python UDFs in Snowflake allow leveraging Python's rich ecosystem of scientific and mathematical libraries like NumPy, SciPy, and pandas, which provide advanced mathematical capabilities beyond what's available in SQL or JavaScript. This approach combines the power of Python's mathematical libraries with Snowflake's data processing capabilities, enabling sophisticated calculations like statistical analysis, machine learning scoring, or complex mathematical algorithms to be applied directly within Snowflake queries.*"},{"id":65,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs incremental data transformations on a large fact table, processing only new and changed records since the last run. Which combination of Snowflake features should be used?","options":[{"id":"A","text":"Time Travel and zero-copy cloning"},{"id":"B","text":"Streams and tasks"},{"id":"C","text":"External tables with auto refresh"},{"id":"D","text":"Materialized views with automatic clustering"}],"correctAnswer":"B","explanation":"The combination of streams and tasks is the most effective solution for implementing incremental data transformations. Streams capture the changes (inserts, updates, deletes) to the source table since the last processing run, enabling the transformation logic to process only the delta rather than the entire dataset. Tasks provide the scheduling and execution framework to run these incremental transformations automatically at defined intervals. This approach minimizes processing overhead and resource consumption while ensuring the target fact table stays current with source changes, making it ideal for large fact tables where full reprocessing would be prohibitively expensive.*"}]`),pm=[{id:1,name:"Data Movement (17 questions)"},{id:2,name:"Performance Optimization (14 questions)"},{id:3,name:"Storage and Data Protection (9 questions)"},{id:4,name:"Data Governance (9 questions)"},{id:5,name:"Data Transformation (16 questions)"}],hm={id:lm,title:cm,description:dm,questions:um,domains:pm},fm=2,mm="Practice Test 2",gm="Snowflake Advanced Data Engineering Certificate (DEA-C02) - Practice Test 2",vm=JSON.parse(`[{"id":1,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from multiple CSV files with varying delimiters into Snowflake. Which approach is most efficient?","options":[{"id":"A","text":"Create a separate file format for each delimiter type"},{"id":"B","text":"Use the AUTO_DETECT parameter in the COPY command"},{"id":"C","text":"Convert all files to the same delimiter before loading"},{"id":"D","text":"Use external tables with a custom parser"}],"correctAnswer":"A","explanation":"Creating a separate file format for each delimiter type is the most efficient approach. Snowflake file formats allow you to define specific parameters like field_delimiter, and then reference these formats in your COPY commands. This approach provides clarity and reusability, allowing you to process files with different delimiters using the appropriate format for each file type. While AUTO_DETECT can be useful, it may not always correctly identify delimiters, especially with complex data, making explicit file formats more reliable for production data loading.*"},{"id":2,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution to load data from an on-premises SQL Server database to Snowflake in near real-time. Which approach is most appropriate?","options":[{"id":"A","text":"Use Snowflake's SQL Server connector"},{"id":"B","text":"Implement a CDC solution with Kafka and Snowpipe"},{"id":"C","text":"Schedule hourly full extracts using the COPY command"},{"id":"D","text":"Use Snowflake's database replication feature"}],"correctAnswer":"B","explanation":"Implementing a Change Data Capture (CDC) solution with Kafka and Snowpipe is the most appropriate approach for near real-time data loading from an on-premises SQL Server. This architecture captures changes in SQL Server as they occur (using SQL Server's CDC or transaction log reading), streams them through Kafka, and loads them into Snowflake using Snowpipe. This provides a scalable, reliable solution for near real-time data integration between on-premises systems and Snowflake with minimal latency and resource impact on the source system.*"},{"id":3,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load JSON data where some records have missing fields. Which option in the COPY command should be used to handle this situation?","options":[{"id":"A","text":"STRIP_NULL_VALUES = TRUE"},{"id":"B","text":"NULL_IF = 'NULL'"},{"id":"C","text":"EMPTY_FIELD_AS_NULL = TRUE"},{"id":"D","text":"PARSE_JSON = 'PERMISSIVE'"}],"correctAnswer":"D","explanation":"PARSE_JSON = 'PERMISSIVE' is the appropriate option for handling JSON data with missing fields. In permissive mode, Snowflake will parse JSON records more leniently, allowing for missing fields, which will be represented as NULL values in the target table. This contrasts with the default strict mode, which would reject records with schema variations. The other options are either not valid JSON parsing parameters or address different issues (like handling of explicit NULL strings or empty fields in CSV files), making permissive parsing the correct choice for flexible JSON handling.*"},{"id":4,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from an external stage where new files are added throughout the day. The data needs to be available in Snowflake within 5 minutes of being placed in the external stage. Which approach should be implemented?","options":[{"id":"A","text":"Schedule a task to run the COPY command every 5 minutes"},{"id":"B","text":"Use Snowpipe with auto-ingest enabled"},{"id":"C","text":"Create an external table with AUTO_REFRESH = TRUE"},{"id":"D","text":"Implement a stored procedure that checks for new files"}],"correctAnswer":"B","explanation":"Snowpipe with auto-ingest enabled is the most appropriate solution for loading data within 5 minutes of arrival in an external stage. When auto-ingest is enabled, Snowflake automatically consumes cloud storage notifications (like AWS S3 events or Azure Event Grid) when new files are added to the stage. This triggers Snowpipe to load the new data immediately without manual intervention or scheduled polling, ensuring data is available in Snowflake within minutes of being placed in the external stage, meeting the 5-minute requirement efficiently.*"},{"id":5,"domain":"Data Movement (17 questions)","text":"A data engineer is loading data from a source that occasionally produces duplicate records. Which approach should be used to ensure no duplicates are loaded into the target Snowflake table?","options":[{"id":"A","text":"Use a primary key constraint on the target table"},{"id":"B","text":"Implement a MERGE statement with a unique key condition"},{"id":"C","text":"Create a unique index on the target table"},{"id":"D","text":"Use the COPY command with ON_ERROR = SKIP_DUPLICATE"}],"correctAnswer":"B","explanation":"Implementing a MERGE statement with a unique key condition is the most effective approach for preventing duplicate records during data loading. The MERGE statement allows you to define a matching condition based on a business key or unique identifier, and then specify different actions for matching records (update or ignore) versus non-matching records (insert). This gives you precise control over how potential duplicates are handled, allowing you to either update existing records or skip the insertion of duplicates while still loading new, unique records.*"},{"id":6,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a REST API that returns paginated results. Which approach should be used to handle the pagination and load all data?","options":[{"id":"A","text":"Use a JavaScript stored procedure with a loop to fetch all pages"},{"id":"B","text":"Create multiple Snowpipes, one for each page"},{"id":"C","text":"Use the COPY command with the PAGES parameter"},{"id":"D","text":"Implement an external function that returns all pages at once"}],"correctAnswer":"A","explanation":"A JavaScript stored procedure with a loop is the most appropriate approach for handling paginated API results. JavaScript procedures in Snowflake can use the built-in HTTP client to make API calls, process the response to extract both data and pagination tokens/links, and then loop through all pages by making subsequent requests with updated pagination parameters. This approach provides the programming flexibility needed to handle the API's pagination mechanism, aggregate results from all pages, and load the complete dataset into Snowflake tables in a single, cohesive process.*"},{"id":7,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from multiple Parquet files where the schema may evolve over time with new columns added. Which approach should be used?","options":[{"id":"A","text":"Use the INFER_SCHEMA parameter in the COPY command"},{"id":"B","text":"Create an external table with AUTO_REFRESH = TRUE"},{"id":"C","text":"Use the MATCH_BY_COLUMN_NAME option in the COPY command"},{"id":"D","text":"Implement a stored procedure that updates the table schema"}],"correctAnswer":"C","explanation":"The MATCH_BY_COLUMN_NAME option in the COPY command is the most appropriate approach for loading Parquet files with evolving schemas. This option instructs Snowflake to match columns in the source files to the target table based on column names rather than positions. When new columns appear in the Parquet files, they will be ignored if they don't exist in the target table (avoiding errors), and existing columns will be loaded correctly regardless of their position in the file. This provides flexibility for schema evolution without requiring schema inference or table alterations for each load.*"},{"id":8,"domain":"Data Movement (17 questions)","text":"A data engineer needs to share sensitive customer data with a partner organization that also uses Snowflake, but in a different region. Which method provides the most secure and efficient way to share this data?","options":[{"id":"A","text":"Export the data to a secure cloud storage and provide access"},{"id":"B","text":"Set up Snowflake Database Replication"},{"id":"C","text":"Use Snowflake Secure Data Sharing"},{"id":"D","text":"Create database clones and provide separate login credentials"}],"correctAnswer":"C","explanation":"Snowflake Secure Data Sharing is the most secure and efficient method for sharing data with a partner organization that uses Snowflake in a different region. This feature allows sharing read-only access to specific databases, schemas, or tables without copying or moving the data, even across different regions. The provider maintains complete control over what data is shared and can revoke access at any time. The consumer can query the shared data directly without needing to store, manage, or synchronize it. This approach maintains data governance while enabling secure, efficient cross-region collaboration.*"},{"id":9,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution to handle late-arriving data in a Snowflake data warehouse. The solution needs to ensure that historical aggregates are correctly updated when late data arrives. Which approach is most appropriate?","options":[{"id":"A","text":"Use Snowflake Time Travel to reprocess historical data"},{"id":"B","text":"Implement a slowly changing dimension (SCD) Type 2 approach"},{"id":"C","text":"Use Snowflake Streams and Tasks to process incremental changes"},{"id":"D","text":"Create a separate table for late-arriving data"}],"correctAnswer":"C","explanation":"Using Snowflake Streams and Tasks to process incremental changes is the most appropriate approach for handling late-arriving data that affects historical aggregates. Streams capture all changes (inserts, updates, deletes) to the source data, including late-arriving records. By combining streams with tasks that reprocess affected time periods in the aggregates, you can ensure that historical aggregates are correctly updated when late data arrives. This incremental approach is more efficient than full reprocessing and more automated than maintaining separate tables, providing a scalable solution for maintaining accuracy in the presence of late-arriving data.*"},{"id":10,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source system that produces files with inconsistent naming patterns. Which approach should be used to ensure all relevant files are loaded?","options":[{"id":"A","text":"Use pattern matching in the COPY command's FILES parameter"},{"id":"B","text":"Create a stored procedure that lists and loads files based on content"},{"id":"C","text":"Use Snowpipe with a custom file notification mechanism"},{"id":"D","text":"Implement an external stage with a file listing function"}],"correctAnswer":"A","explanation":"Using pattern matching in the COPY command's FILES parameter is the most straightforward approach for loading files with inconsistent naming patterns. The FILES parameter supports wildcards and pattern matching expressions (like 'data_*.csv' or 'logs/202[0-9]-\\\\\\\\d{2}-\\\\\\\\d{2}_.*.parquet') that can be used to select files based on partial name matches or regular expression patterns. This approach allows you to define flexible patterns that capture all relevant files despite naming inconsistencies, without requiring custom procedures or external mechanisms.*"},{"id":11,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows querying data directly from files in cloud storage without loading it into Snowflake tables. Which Snowflake feature should be used?","options":[{"id":"A","text":"Zero-copy cloning"},{"id":"B","text":"External tables"},{"id":"C","text":"Materialized views"},{"id":"D","text":"Secure views"}],"correctAnswer":"B","explanation":"External tables are the appropriate Snowflake feature for querying data directly from files in cloud storage without loading it into Snowflake tables. External tables create a table structure that references data stored in external cloud storage locations (like S3, Azure Blob, or GCS) rather than in Snowflake's internal storage. This allows querying the external data using standard SQL without first loading or copying it into Snowflake. External tables are ideal for scenarios where data needs to remain in cloud storage due to size, governance requirements, or integration with other systems that also access the same files.*"},{"id":12,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source system that occasionally produces malformed JSON records. Which approach should be used to handle these malformed records without failing the entire load?","options":[{"id":"A","text":"Use the VALIDATE_JSON function in a pre-processing step"},{"id":"B","text":"Set PARSE_JSON = 'PERMISSIVE' in the file format"},{"id":"C","text":"Use ON_ERROR = 'CONTINUE' in the COPY command"},{"id":"D","text":"Implement a JavaScript UDF to clean the JSON"}],"correctAnswer":"C","explanation":"Using ON_ERROR = 'CONTINUE' in the COPY command is the most direct approach for handling malformed records without failing the entire load. This option instructs Snowflake to skip individual records that cause errors (like malformed JSON) and continue processing the remaining records. Error details for skipped records can be captured and reviewed using the VALIDATION_MODE parameter or by querying load history. This approach provides resilience against occasional data quality issues while still loading all valid records, making it ideal for production pipelines that need to handle imperfect source data.*"},{"id":13,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution to track the history of data loads, including file names, row counts, and error information. Which Snowflake feature should be used?","options":[{"id":"A","text":"Query history"},{"id":"B","text":"Information schema"},{"id":"C","text":"Account usage views"},{"id":"D","text":"Automatic query optimization"}],"correctAnswer":"C","explanation":"Snowflake's Account Usage views are the most appropriate feature for tracking the history of data loads. Specifically, views like COPY_HISTORY and LOAD_HISTORY in the SNOWFLAKE.ACCOUNT_USAGE schema provide detailed information about data loading operations, including file names, row counts, error counts, and other metrics. These views maintain historical information for a longer period than the Information Schema views and provide more comprehensive details about load operations than query history alone. This makes them ideal for building load monitoring and auditing solutions.*"},{"id":14,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source that produces files with headers that should be skipped during loading. Which parameter should be used in the file format definition?","options":[{"id":"A","text":"SKIP_HEADER = 1"},{"id":"B","text":"PARSE_HEADER = TRUE"},{"id":"C","text":"HEADER_ROWS = 1"},{"id":"D","text":"IGNORE_HEADER = TRUE"}],"correctAnswer":"A","explanation":"SKIP_HEADER = 1 is the correct parameter to use in the file format definition when you need to skip header rows during data loading. This parameter instructs Snowflake to ignore the specified number of lines at the beginning of each file, which is typically used for files with header rows that contain column names rather than data. This ensures that header information is not loaded as data rows, preventing data quality issues. The other options are either not valid Snowflake parameters or have different purposes than skipping header rows during loading.*"},{"id":15,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that loads data from multiple source systems with different data formats (CSV, JSON, Parquet) into a single Snowflake table. Which approach is most efficient?","options":[{"id":"A","text":"Convert all data to a single format before loading"},{"id":"B","text":"Create separate staging tables for each format, then merge"},{"id":"C","text":"Use different file formats with the same COPY command"},{"id":"D","text":"Use separate COPY commands with appropriate file formats for each source"}],"correctAnswer":"D","explanation":"Using separate COPY commands with appropriate file formats for each source is the most efficient approach for loading multiple data formats into a single table. Each COPY command can reference a file format specifically configured for its corresponding data format (CSV, JSON, Parquet), ensuring optimal parsing and loading for each type. This approach provides clarity, maintainability, and optimal performance for each format without requiring pre-conversion or intermediate staging tables. The COPY commands can be orchestrated to run in sequence or parallel as needed to populate the single target table.*"},{"id":16,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution to handle files that occasionally arrive with no data (empty files) in an external stage. Which approach should be used to prevent errors during the load process?","options":[{"id":"A","text":"Use the VALIDATE_EMPTY_FILES = TRUE parameter in the COPY command"},{"id":"B","text":"Implement a stored procedure that checks file size before loading"},{"id":"C","text":"Use ON_ERROR = 'SKIP_FILE' in the COPY command"},{"id":"D","text":"Create a file format with EMPTY_FILE_HANDLING = 'IGNORE'"}],"correctAnswer":"B","explanation":"Implementing a stored procedure that checks file size before loading is the most robust approach for handling potentially empty files. The procedure can use Snowflake's metadata functions or stage listing capabilities to identify empty files and either skip them or handle them differently in the loading process. While Snowflake's COPY command has error handling options, there isn't a specific parameter for empty file handling like VALIDATE_EMPTY_FILES or EMPTY_FILE_HANDLING. The ON_ERROR option would only apply after an error occurs, making proactive checking more efficient and providing better control and logging of empty file scenarios.*"},{"id":17,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source system that produces files with trailing delimiters at the end of each record. Which parameter should be used in the file format definition to handle this correctly?","options":[{"id":"A","text":"TRIM_SPACE = TRUE"},{"id":"B","text":"IGNORE_TRAILING_DELIMITERS = TRUE"},{"id":"C","text":"TRIM_TRAILING = TRUE"},{"id":"D","text":"FIELD_OPTIONALLY_ENCLOSED_BY = 'NONE'"}],"correctAnswer":"B","explanation":"IGNORE_TRAILING_DELIMITERS = TRUE is the correct parameter to use in the file format definition when loading files with trailing delimiters at the end of each record. This parameter instructs Snowflake to ignore delimiter characters that appear at the end of a record, preventing them from being interpreted as indicating an additional empty field. This ensures correct field count and alignment when loading data with this common formatting issue. The other options either address different formatting challenges (like whitespace trimming) or are not valid Snowflake parameters for handling trailing delimiters.*"},{"id":18,"domain":"Data Movement (17 questions)","text":"A data engineer notices that queries against a large fact table are performing poorly despite having appropriate filters. Upon investigation, it's found that the table has over 100,000 micro-partitions. What is the most likely cause of this issue?","options":[{"id":"A","text":"The warehouse size is too small"},{"id":"B","text":"The table has too many micro-partitions for efficient pruning"},{"id":"C","text":"The clustering key is not aligned with common query filters"},{"id":"D","text":"The table needs to be re-clustered"}],"correctAnswer":"B","explanation":"Having over 100,000 micro-partitions in a table is likely causing the poor query performance. Snowflake recommends keeping the number of micro-partitions in a table between 1,000 and 10,000 for optimal performance. When a table has too many micro-partitions, the metadata overhead for partition pruning increases significantly, which can negate the benefits of pruning and lead to slower query performance. This situation typically occurs when tables have many small files loaded over time or when the table size is relatively small compared to the number of partitions, resulting in many small micro-partitions rather than fewer, optimally-sized ones.*"},{"id":19,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that joins a large fact table with several dimension tables. The fact table is properly clustered, but query performance is still poor. Which approach would most likely improve performance?","options":[{"id":"A","text":"Create a materialized view that pre-joins the tables"},{"id":"B","text":"Increase the warehouse size"},{"id":"C","text":"Add result caching"},{"id":"D","text":"Implement a clustering key on the dimension tables"}],"correctAnswer":"A","explanation":"Creating a materialized view that pre-joins the fact and dimension tables would most likely improve performance for this scenario. Materialized views in Snowflake physically store the results of the view definition query, including joins, and automatically maintain these results as the underlying tables change. For complex joins between large fact tables and dimension tables that are frequently queried together, materializing the join results eliminates the need to recompute the joins for each query. This provides significant performance benefits, especially when the same join patterns are used repeatedly in different queries or reports.*"},{"id":20,"domain":"Data Movement (17 questions)","text":"A data engineer is analyzing query performance and notices that a specific query is scanning all micro-partitions in a table despite having filters that should allow partition pruning. Which approach should be used to diagnose this issue?","options":[{"id":"A","text":"Check the clustering depth of the table"},{"id":"B","text":"Review the query profile to examine partition pruning metrics"},{"id":"C","text":"Analyze the compression ratio of the table"},{"id":"D","text":"Check if the table has too many columns"}],"correctAnswer":"B","explanation":"Reviewing the query profile to examine partition pruning metrics is the most direct approach for diagnosing why a query is scanning all micro-partitions despite having filters that should enable pruning. The query profile in Snowflake provides detailed information about partition pruning efficiency, showing exactly how many partitions were scanned versus pruned for each table in the query. This information can help identify whether the issue is related to clustering keys, filter conditions, data distribution, or other factors affecting pruning efficiency. Understanding these metrics is essential for implementing effective performance optimizations.*"},{"id":21,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs an aggregation over a large dataset with a GROUP BY clause on a date column. The date values are distributed across the entire range with some dates having significantly more data than others. Which approach would be most effective?","options":[{"id":"A","text":"Add a clustering key on the date column"},{"id":"B","text":"Create a materialized view with the pre-computed aggregation"},{"id":"C","text":"Use a larger warehouse size"},{"id":"D","text":"Implement a result cache"}],"correctAnswer":"A","explanation":"Adding a clustering key on the date column would be most effective for optimizing this query. Clustering organizes the micro-partitions to co-locate similar date values, which is particularly beneficial when data distribution is uneven across the date range. When the query filters or groups by the date column, Snowflake can efficiently prune irrelevant micro-partitions and process each date group with minimal data movement. This approach directly addresses the data distribution challenge and improves both filtering and grouping operations on the date column, making it more effective than simply increasing resources or caching results.*"},{"id":22,"domain":"Data Movement (17 questions)","text":"A data engineer is optimizing a data warehouse for a mixed workload of both interactive dashboards and long-running ETL processes. Which approach would provide the best overall performance?","options":[{"id":"A","text":"Use a single X-Large warehouse for all workloads"},{"id":"B","text":"Create separate warehouses for different workload types"},{"id":"C","text":"Implement a multi-cluster warehouse with maximum clusters set to 10"},{"id":"D","text":"Use a single warehouse with auto-suspend set to 60 seconds"}],"correctAnswer":"B","explanation":"Creating separate warehouses for different workload types (interactive dashboards vs. long-running ETL) provides the best overall performance for a mixed workload environment. This approach allows optimizing each warehouse for its specific workload characteristics - smaller, more responsive warehouses with aggressive scaling for interactive dashboards, and larger, more stable warehouses for ETL processes. Separating these workloads prevents resource contention where long-running ETL jobs might block interactive queries, ensuring consistent performance for both use cases. This approach also enables workload-specific configurations for auto-suspend, scaling policies, and resource monitoring.*"},{"id":23,"domain":"Data Movement (17 questions)","text":"A data engineer notices that a critical query is performing poorly due to a large JOIN operation between two tables. Which Snowflake feature would provide the most insight into optimizing this specific operation?","options":[{"id":"A","text":"EXPLAIN plan"},{"id":"B","text":"Query Profile"},{"id":"C","text":"INFORMATION_SCHEMA.QUERY_HISTORY"},{"id":"D","text":"SHOW WAREHOUSES"}],"correctAnswer":"B","explanation":"The Query Profile feature in Snowflake would provide the most insight into optimizing a poorly performing JOIN operation. Unlike the EXPLAIN plan which shows the intended execution plan before running the query, the Query Profile shows detailed metrics about the actual execution after it completes. For JOIN operations specifically, it provides critical information about join type selection, data distribution, spilling to disk, and operator execution times. This visual, interactive tool allows drilling into specific operators to identify exactly where and why the JOIN is performing poorly, making it the most valuable feature for diagnosing and optimizing complex JOIN operations.*"},{"id":24,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize storage costs for a large historical table where only the most recent data is frequently accessed. Which approach would be most effective?","options":[{"id":"A","text":"Implement a clustering key on the date column"},{"id":"B","text":"Use table partitioning to separate hot and cold data"},{"id":"C","text":"Create a materialized view for the recent data"},{"id":"D","text":"Move older data to a separate table with a smaller warehouse"}],"correctAnswer":"A","explanation":"Implementing a clustering key on the date column is the most effective approach for optimizing storage costs while maintaining query performance on a table with hot and cold data patterns. Clustering ensures that recent (hot) data is co-located in the same micro-partitions, enabling efficient pruning when queries filter on recent dates. Snowflake's automatic clustering maintenance focuses on the most frequently accessed micro-partitions, naturally optimizing for hot data access patterns. Additionally, Snowflake's storage billing is based on actual compressed storage used, and clustering doesn't increase this cost, making it more cost-effective than creating duplicate structures or moving data between tables.*"},{"id":25,"domain":"Data Movement (17 questions)","text":"A data engineer is analyzing a slow-running query and notices in the query profile that a large amount of data is being redistributed across nodes during a JOIN operation. Which approach would most likely improve performance?","options":[{"id":"A","text":"Increase the warehouse size"},{"id":"B","text":"Add a clustering key on the join columns"},{"id":"C","text":"Use a broadcast join instead of a hash join"},{"id":"D","text":"Implement a materialized view"}],"correctAnswer":"B","explanation":"Adding a clustering key on the join columns would most likely improve performance when excessive data redistribution is occurring during JOIN operations. Clustering the tables on their join keys improves data locality, potentially enabling Snowflake's query optimizer to choose more efficient join strategies that require less data movement between nodes. When data with the same join key values is co-located in the same micro-partitions, the database can often perform more localized join processing, reducing the need for large-scale data redistribution across the cluster. This directly addresses the specific performance issue observed in the query profile.*"},{"id":26,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs multiple window functions over the same partition and order specification. Which approach would be most effective?","options":[{"id":"A","text":"Rewrite the query to use GROUP BY instead of window functions"},{"id":"B","text":"Split the query into multiple simpler queries"},{"id":"C","text":"Use a CTE to compute the window specification once"},{"id":"D","text":"Increase the warehouse size"}],"correctAnswer":"C","explanation":"Using a Common Table Expression (CTE) to compute the window specification once is the most effective approach for optimizing a query with multiple window functions using the same partition and order specification. This approach allows defining the partitioning and ordering logic in one place and reusing it across multiple window functions, which can improve both query performance and maintainability. Snowflake's optimizer can potentially recognize the common window specification and optimize the execution accordingly, reducing redundant sorting and partitioning operations that would otherwise be repeated for each window function.*"},{"id":27,"domain":"Data Movement (17 questions)","text":"A data engineer notices that a query with a complex WHERE clause containing multiple OR conditions is performing poorly. Which approach would most likely improve performance?","options":[{"id":"A","text":"Split the query into multiple UNION ALL queries with simpler conditions"},{"id":"B","text":"Rewrite the conditions using CASE statements"},{"id":"C","text":"Add a result cache for the query"},{"id":"D","text":"Use a larger warehouse size"}],"correctAnswer":"A","explanation":"Splitting the query into multiple UNION ALL queries with simpler conditions would most likely improve performance for a query with multiple OR conditions. Complex OR conditions often prevent effective partition pruning and index usage, as the optimizer must consider the union of all possible matching data. By rewriting the query as a UNION ALL of simpler queries, each with a single condition or simpler AND conditions, you enable more efficient partition pruning for each individual query. Snowflake can then optimize each part of the UNION ALL independently, potentially resulting in significantly better overall performance, especially for large tables with appropriate clustering keys.*"},{"id":28,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs an aggregation over a large dataset with a GROUP BY clause on a high-cardinality column. The query is currently using a medium-sized warehouse and is taking too long to complete. Which approach would be most effective?","options":[{"id":"A","text":"Increase the warehouse size to X-Large"},{"id":"B","text":"Add a clustering key on the GROUP BY column"},{"id":"C","text":"Create a materialized view with the pre-computed aggregation"},{"id":"D","text":"Use approximate aggregation functions"}],"correctAnswer":"B","explanation":"Adding a clustering key on the high-cardinality GROUP BY column would be most effective for optimizing this aggregation query. Clustering improves the locality of data with similar values, which significantly enhances the performance of GROUP BY operations by reducing data shuffling and improving aggregation efficiency. When data is clustered by the grouping column, Snowflake can process each group more efficiently by accessing co-located data. While increasing warehouse size might help somewhat, it doesn't address the fundamental data organization issue, and materialized views might be less flexible if query parameters change frequently.*"},{"id":29,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution for a reporting system that needs to query the same large dataset multiple times with different parameters. The data is updated hourly. Which feature would provide the best query performance?","options":[{"id":"A","text":"Zero-copy cloning"},{"id":"B","text":"Materialized views"},{"id":"C","text":"Multi-cluster warehouses"},{"id":"D","text":"Result caching"}],"correctAnswer":"B","explanation":"Materialized views would provide the best query performance for a reporting system that repeatedly queries the same large dataset with different parameters. Materialized views pre-compute and store query results, including joins and aggregations, and automatically maintain these results as the underlying data changes hourly. Unlike result caching, which only helps if the exact same query is run again, materialized views can accelerate a variety of queries against the same base tables with different parameters. This is particularly valuable for reporting systems where the base data structure is consistent but is analyzed from different angles or with different filters.*"},{"id":30,"domain":"Data Movement (17 questions)","text":"A data engineer notices that queries against a table with time-series data are performing poorly. The table has billions of rows with timestamps spanning several years, and most queries filter on recent time ranges. Which approach would most improve query performance?","options":[{"id":"A","text":"Implement a clustering key on the timestamp column"},{"id":"B","text":"Create a separate table for each year of data"},{"id":"C","text":"Use a larger warehouse size"},{"id":"D","text":"Create an aggregate table with daily summaries"}],"correctAnswer":"A","explanation":"Implementing a clustering key on the timestamp column would most improve query performance for time-series data where queries typically filter on recent time ranges. Clustering organizes the micro-partitions based on timestamp values, enabling Snowflake to efficiently prune irrelevant micro-partitions during query execution. This is particularly effective for time-series data where queries have a temporal locality pattern (focusing on recent data). Unlike creating separate tables by year, clustering provides a seamless query experience across the entire dataset while still optimizing performance for the most common query patterns, without introducing additional complexity in table management or query writing.*"},{"id":31,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs a self-join on a large table. Which approach would be most effective?","options":[{"id":"A","text":"Create a materialized view that pre-computes the join"},{"id":"B","text":"Use a CTE to reference the table only once"},{"id":"C","text":"Increase the warehouse size"},{"id":"D","text":"Add clustering keys on the join columns"}],"correctAnswer":"D","explanation":"Adding clustering keys on the join columns would be most effective for optimizing a self-join on a large table. Clustering ensures that rows with the same join key values are co-located in the same micro-partitions, which significantly improves join performance by reducing data movement and enabling more efficient join strategies. For self-joins specifically, this approach is particularly effective because the same clustering key benefits both sides of the join. While CTEs and materialized views might help in some scenarios, they don't address the fundamental data organization issue that affects join performance, making clustering the most direct and effective optimization for this specific query pattern.*"},{"id":32,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows querying historical data as of any point in time within the last 45 days. Which Snowflake feature and configuration should be used?","options":[{"id":"A","text":"Time Travel with a 45-day retention period in Enterprise Edition"},{"id":"B","text":"Fail-safe with extended retention"},{"id":"C","text":"Zero-copy cloning with daily snapshots"},{"id":"D","text":"Continuous data protection with 45-day retention"}],"correctAnswer":"A","explanation":"Time Travel with a 45-day retention period in Enterprise Edition is the correct solution for querying historical data as of any point in time within the last 45 days. Snowflake's Enterprise Edition allows extending the Time Travel retention period up to 90 days, which covers the 45-day requirement. Time Travel enables querying data as it existed at specific points in time using the AT or BEFORE clause in queries, providing the exact point-in-time query capability needed. Fail-safe is not user-accessible for queries, and while daily snapshots with cloning could provide discrete recovery points, they wouldn't allow querying at any arbitrary point in time.*"},{"id":33,"domain":"Data Movement (17 questions)","text":"A data engineer accidentally dropped a production table and needs to recover it. The table was dropped 12 hours ago. Which command should be used to recover the table?","options":[{"id":"A","text":"UNDROP TABLE tablename"},{"id":"B","text":"RESTORE TABLE tablename"},{"id":"C","text":"CLONE TABLE tablename AT OFFSET = -12h"},{"id":"D","text":"RECOVER TABLE tablename"}],"correctAnswer":"A","explanation":"UNDROP TABLE tablename is the correct command to recover a table that was dropped 12 hours ago. Snowflake's UNDROP command restores objects (tables, schemas, databases) that have been dropped, as long as they are still within the Time Travel retention period. Since the table was dropped only 12 hours ago, it is well within even the standard 1-day retention period. The UNDROP command will restore the table to its state just before it was dropped, including all data and metadata. The other commands are either not valid Snowflake commands for this purpose or don't address the specific recovery scenario described.*"},{"id":34,"domain":"Data Movement (17 questions)","text":"A data engineer needs to create multiple test environments from a production database without consuming additional storage space initially. Which Snowflake feature should be used?","options":[{"id":"A","text":"Database replication"},{"id":"B","text":"Zero-copy cloning"},{"id":"C","text":"Table sharing"},{"id":"D","text":"Fail-safe recovery"}],"correctAnswer":"B","explanation":"Zero-copy cloning is the ideal feature for creating multiple test environments from a production database without initially consuming additional storage space. When a clone is created, it references the same micro-partitions as the source object without duplicating data. Only when changes are made to either the source or the clone does Snowflake create new micro-partitions to store the differences. This allows creating multiple independent test environments that can diverge from production as needed, while minimizing storage costs. This approach is perfect for development, testing, and QA environments that need to start with production data but evolve independently.*"},{"id":35,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution to protect against accidental updates or deletes in a critical table. Which approach provides the best protection while still allowing legitimate modifications?","options":[{"id":"A","text":"Create a backup table using zero-copy cloning"},{"id":"B","text":"Implement a stored procedure for all data modifications"},{"id":"C","text":"Use table access control with custom roles"},{"id":"D","text":"Enable Change Tracking on the table"}],"correctAnswer":"B","explanation":"Implementing a stored procedure for all data modifications provides the best protection against accidental updates or deletes while still allowing legitimate modifications. By channeling all data modifications through stored procedures, you can implement validation logic, business rules, and safety checks that prevent accidental or unauthorized changes while still allowing legitimate modifications to proceed. This approach provides a controlled interface for data modifications with proper error handling and logging, reducing the risk of accidental data corruption while maintaining the flexibility needed for normal operations. It's more targeted than general access controls and more proactive than relying on after-the-fact recovery mechanisms.*"},{"id":36,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a cost-effective solution for long-term retention of historical data that is rarely accessed. The data must remain queryable but with lower performance expectations. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake's external tables with cloud storage"},{"id":"B","text":"Implement a multi-cluster warehouse with auto-scaling"},{"id":"C","text":"Create a separate database with extended Time Travel"},{"id":"D","text":"Use table partitioning with different storage tiers"}],"correctAnswer":"A","explanation":"Using Snowflake's external tables with cloud storage is the most cost-effective solution for long-term retention of rarely accessed historical data. This approach leverages cheaper cloud storage (like S3, Azure Blob, or GCS) for the actual data files while maintaining queryability through Snowflake's external tables feature. When queries are executed against external tables, Snowflake reads the data directly from cloud storage, which is significantly less expensive than Snowflake's native storage for rarely accessed data. While query performance may be somewhat reduced compared to internal tables, this aligns with the stated lower performance expectations for this historical data.*"},{"id":37,"domain":"Data Movement (17 questions)","text":"A data engineer needs to understand the storage consumption patterns of different tables in a Snowflake database. Which view should be queried?","options":[{"id":"A","text":"SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS"},{"id":"B","text":"INFORMATION_SCHEMA.TABLES"},{"id":"C","text":"SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY"},{"id":"D","text":"INFORMATION_SCHEMA.USAGE_METRICS"}],"correctAnswer":"A","explanation":"SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS is the correct view to query for understanding storage consumption patterns of different tables. This view provides detailed information about storage usage at the table level, including active bytes (currently in use), time travel bytes (retained for historical queries), fail-safe bytes, and total bytes. It also includes metrics on the number of micro-partitions and average micro-partition size. This comprehensive storage information enables data engineers to identify tables with high storage consumption, monitor growth trends, and implement targeted optimizations to manage storage costs effectively.*"},{"id":38,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that maintains a full history of all changes to a table, including records of who made each change and when. Which approach should be used?","options":[{"id":"A","text":"Enable Change Tracking on the table"},{"id":"B","text":"Implement a slowly changing dimension (SCD) Type 2 design"},{"id":"C","text":"Use Snowflake Streams with metadata columns"},{"id":"D","text":"Create triggers that log changes to an audit table"}],"correctAnswer":"B","explanation":"Implementing a slowly changing dimension (SCD) Type 2 design is the most comprehensive approach for maintaining a full history of all changes to a table, including change metadata. In an SCD Type 2 implementation, rather than overwriting existing records, new versions are created with effective date ranges and metadata columns that can capture who made each change and when. This preserves the complete history of how data has evolved over time, allowing point-in-time analysis and full auditability. While Streams can capture changes, they don't inherently maintain the historical record, and Change Tracking only indicates that changes occurred without preserving the previous values.*"},{"id":39,"domain":"Data Movement (17 questions)","text":"A data engineer needs to estimate the storage cost savings that could be achieved by reducing the Time Travel retention period for a large table. Which Snowflake view should be queried?","options":[{"id":"A","text":"SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS"},{"id":"B","text":"INFORMATION_SCHEMA.TABLE_STORAGE_METRICS"},{"id":"C","text":"SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE"},{"id":"D","text":"INFORMATION_SCHEMA.TABLES"}],"correctAnswer":"A","explanation":"SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS is the correct view to query for estimating storage cost savings from reducing Time Travel retention. This view provides a breakdown of storage usage by category, including a specific column for TIME_TRAVEL_BYTES, which represents the storage consumed by Time Travel data retention. By analyzing this metric for the table in question, the data engineer can directly quantify how much storage is currently being used for Time Travel and estimate the potential savings from reducing the retention period. This view provides the most detailed and relevant information for this specific cost optimization analysis.*"},{"id":40,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows recovering from both accidental data corruption and regional cloud provider outages. Which combination of Snowflake features provides the most comprehensive protection?","options":[{"id":"A","text":"Time Travel and Fail-safe"},{"id":"B","text":"Database Replication across regions and Time Travel"},{"id":"C","text":"Zero-copy cloning and Fail-safe"},{"id":"D","text":"External tables and Database Replication"}],"correctAnswer":"B","explanation":"The combination of Database Replication across regions and Time Travel provides the most comprehensive protection against both data corruption and regional cloud provider outages. Database Replication creates a synchronized copy of databases in different geographical regions, protecting against regional outages by enabling failover to an unaffected region. Time Travel complements this by providing protection against logical data corruption, allowing point-in-time recovery within the retention period. Together, these features address both physical infrastructure failures and logical data integrity issues, providing a comprehensive disaster recovery solution with both geographical redundancy and temporal recovery capabilities.*"},{"id":41,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows specific users to see only masked versions of sensitive columns in a customer table, while allowing analysts to see the actual data. Which Snowflake feature should be used?","options":[{"id":"A","text":"Column-level security"},{"id":"B","text":"Row-level security"},{"id":"C","text":"Dynamic data masking"},{"id":"D","text":"Secure views"}],"correctAnswer":"C","explanation":"Dynamic data masking is the most appropriate feature for this requirement. It allows defining masking policies that show different versions of the same data to different users based on their roles and privileges. Unlike column-level security which simply grants or denies access to entire columns, dynamic masking allows some users (like analysts) to see the actual data while others see masked versions (like partial credit card numbers or anonymized values). This provides fine-grained access control without duplicating data or creating multiple views, making it ideal for scenarios where different user groups need different levels of access to sensitive information.*"},{"id":42,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a security model where analysts can only see data for customers in their assigned regions. Which Snowflake feature should be used?","options":[{"id":"A","text":"Object tagging"},{"id":"B","text":"Column-level security"},{"id":"C","text":"Row-level security"},{"id":"D","text":"Secure views with session variables"}],"correctAnswer":"C","explanation":"Row-level security is the most appropriate feature for restricting access to customer data based on an analyst's assigned region. This feature allows defining security policies that filter rows dynamically based on attributes of the current session (like the analyst's region assignment). When analysts query the table, they only see rows that match their authorized regions. This provides fine-grained access control at the row level without requiring separate tables or views for each region, making it ideal for implementing data segregation based on attributes like region, department, or customer segment.*"},{"id":43,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that automatically logs all queries that access tables containing PII (Personally Identifiable Information). Which approach should be used?","options":[{"id":"A","text":"Create a UDF that logs access to a separate table"},{"id":"B","text":"Implement custom logging with stored procedures"},{"id":"C","text":"Query the SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY view"},{"id":"D","text":"Enable automatic audit logging for PII tables"}],"correctAnswer":"C","explanation":"Querying the SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY view is the most direct approach for logging all queries that access PII tables. This view in the Snowflake shared database automatically records all access to objects, including who accessed what data and when. By creating a scheduled task or procedure that queries this view with filters for the PII tables, the data engineer can create comprehensive audit logs without modifying the tables or implementing custom logging mechanisms. This leverages Snowflake's built-in auditing capabilities, ensuring complete coverage without adding overhead to query execution or requiring changes to application code.*"},{"id":44,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that prevents any single user from being able to both create and approve data quality exceptions. Which approach should be used?","options":[{"id":"A","text":"Implement separate roles with mutually exclusive privileges"},{"id":"B","text":"Use row-level security based on user attributes"},{"id":"C","text":"Create a secure view that filters based on user role"},{"id":"D","text":"Implement column-level security for sensitive operations"}],"correctAnswer":"A","explanation":"Implementing separate roles with mutually exclusive privileges is the most effective approach for enforcing separation of duties between creating and approving data quality exceptions. By creating distinct roles for exception creation and exception approval, and ensuring users are only assigned one of these roles, the data engineer can enforce strict segregation of responsibilities. This approach directly implements the principle of separation of duties, a fundamental security control that prevents conflicts of interest and reduces the risk of fraud or errors by ensuring that critical functions are divided among different individuals.*"},{"id":45,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows tracking the lineage of sensitive data as it moves through various transformations in Snowflake. Which feature or approach should be used?","options":[{"id":"A","text":"Snowflake's native data lineage tracking"},{"id":"B","text":"Object tagging with custom metadata"},{"id":"C","text":"Query history with transformation tracking"},{"id":"D","text":"Access history with data flow analysis"}],"correctAnswer":"B","explanation":"Object tagging with custom metadata is the most effective approach for tracking data lineage in Snowflake. By creating and applying tags that document source systems, transformation steps, sensitivity levels, and other lineage information to databases, schemas, tables, and columns, the data engineer can implement a comprehensive lineage tracking system. These tags can be queried through Snowflake's metadata views to generate lineage reports and diagrams. While Snowflake doesn't have native end-to-end lineage tracking specifically for sensitive data, this tagging approach leverages Snowflake's metadata capabilities to implement a flexible, queryable lineage solution that can focus on sensitive data elements.*"},{"id":46,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that ensures all copies of production data used in development environments have sensitive customer information automatically masked. Which approach should be used?","options":[{"id":"A","text":"Create secure views of production for development use"},{"id":"B","text":"Implement dynamic data masking policies on production tables"},{"id":"C","text":"Use post-clone scripts to apply masking when cloning databases"},{"id":"D","text":"Create separate ETL processes for development data"}],"correctAnswer":"C","explanation":"Using post-clone scripts to apply masking when cloning databases is the most effective approach for ensuring sensitive data is automatically masked in development environments. Post-clone scripts execute automatically after a database is cloned, allowing the data engineer to apply masking policies, remove sensitive data, or transform values specifically in the cloned environment. This approach ensures that all development copies created through cloning have consistent masking applied without affecting the production environment or requiring manual intervention. It provides a systematic, automated solution for protecting sensitive data in non-production environments.*"},{"id":47,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows granting access to all future tables in a schema without explicitly granting privileges on each new table as it's created. Which Snowflake feature should be used?","options":[{"id":"A","text":"Schema-level grants"},{"id":"B","text":"Future grants"},{"id":"C","text":"Role inheritance"},{"id":"D","text":"Managed access schemas"}],"correctAnswer":"B","explanation":"Future grants is the specific Snowflake feature designed to grant privileges on objects that will be created in the future. Using the syntax \`GRANT <privilege> ON FUTURE TABLES IN SCHEMA <schema_name> TO ROLE <role_name>\`, you can ensure that as new tables are created in the schema, the specified role automatically receives the granted privileges without requiring additional grant statements. This feature is essential for maintaining consistent access control in dynamic environments where new objects are frequently created, reducing administrative overhead and preventing security gaps that could occur if manual grants are forgotten for new objects.*"},{"id":48,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows different departments to have their own isolated Snowflake environments while sharing common reference data. Which Snowflake feature should be used?","options":[{"id":"A","text":"Database replication"},{"id":"B","text":"Zero-copy cloning"},{"id":"C","text":"Data sharing"},{"id":"D","text":"Secure views"}],"correctAnswer":"C","explanation":"Data sharing is the most appropriate Snowflake feature for allowing different departments to have isolated environments while sharing common reference data. Snowflake's data sharing allows read-only access to specific databases, schemas, or tables across different Snowflake accounts or within the same account. This enables a central team to maintain reference data in one location while securely sharing it with multiple department-specific Snowflake environments. The shared data appears as a database in the consumer account but doesn't consume additional storage, and updates to the source data are immediately visible to all consumers, ensuring consistency across departments.*"},{"id":49,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that classifies and tags data based on sensitivity levels (Public, Internal, Confidential, Restricted) and enforces appropriate access controls based on these classifications. Which approach should be used?","options":[{"id":"A","text":"Implement column-level security based on sensitivity"},{"id":"B","text":"Create separate schemas for each sensitivity level"},{"id":"C","text":"Use object tagging with tag-based masking policies"},{"id":"D","text":"Implement row-level security with sensitivity filters"}],"correctAnswer":"C","explanation":"Using object tagging with tag-based masking policies is the most comprehensive approach for implementing data classification and enforcing appropriate access controls. This approach allows tagging tables and columns with sensitivity classifications (Public, Internal, Confidential, Restricted) and then creating masking policies that reference these tags to apply appropriate protection measures. By linking masking policies to tags rather than directly to objects, you create a scalable, maintainable system where protection automatically follows classification. This approach centralizes policy definition while providing fine-grained control, and it can be combined with other security measures like row-level security for comprehensive data protection.*"},{"id":50,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that executes a series of data transformation steps in sequence, with each step starting only after the previous step successfully completes. Which Snowflake feature should be used?","options":[{"id":"A","text":"Stored procedures"},{"id":"B","text":"User-defined functions"},{"id":"C","text":"Tasks with dependencies"},{"id":"D","text":"Streams with triggers"}],"correctAnswer":"C","explanation":"Tasks with dependencies is the most appropriate feature for implementing sequential, dependent data transformation steps in Snowflake. Tasks allow scheduling SQL commands or stored procedures to run at defined intervals or in response to events. By creating a DAG (directed acyclic graph) of tasks with predecessor/successor relationships using the AFTER parameter, the data engineer can ensure that each step only executes after its predecessor successfully completes. This provides a native, serverless orchestration solution within Snowflake for managing complex transformation workflows with proper dependency management and error handling.*"},{"id":51,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that captures changes to a source table and applies them to a target table incrementally. Which Snowflake feature should be used?","options":[{"id":"A","text":"Zero-copy cloning"},{"id":"B","text":"Time Travel"},{"id":"C","text":"Streams"},{"id":"D","text":"External tables"}],"correctAnswer":"C","explanation":"Streams is the most appropriate Snowflake feature for capturing and applying incremental changes from a source table to a target table. Streams track DML changes (inserts, updates, deletes) to a table and make those changes available for processing. When combined with tasks, streams enable building efficient change data capture (CDC) pipelines that only process modified data rather than reprocessing the entire dataset. This approach minimizes processing overhead and ensures that the target table stays synchronized with the source while only processing the delta of changes, making it ideal for incremental data transformation scenarios.*"},{"id":52,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement complex data transformations that require procedural logic, loops, and error handling. Which Snowflake feature should be used?","options":[{"id":"A","text":"SQL UDFs"},{"id":"B","text":"Stored procedures"},{"id":"C","text":"Tasks"},{"id":"D","text":"Streams"}],"correctAnswer":"B","explanation":"Stored procedures are the most appropriate feature for implementing complex data transformations requiring procedural logic, loops, and error handling in Snowflake. Unlike SQL UDFs which are limited to returning a single value, stored procedures support full procedural programming with control structures (IF/ELSE, LOOP), error handling (TRY/CATCH), and multiple SQL operations. Snowflake supports stored procedures written in JavaScript, SQL, and other languages, providing the flexibility needed for complex transformation logic that goes beyond what can be expressed in standard SQL alone.*"},{"id":53,"domain":"Data Movement (17 questions)","text":"A data engineer needs to transform semi-structured JSON data stored in a VARIANT column into a relational format for analysis. Which SQL function should be used?","options":[{"id":"A","text":"PARSE_JSON"},{"id":"B","text":"GET"},{"id":"C","text":"FLATTEN"},{"id":"D","text":"TO_JSON"}],"correctAnswer":"C","explanation":"The FLATTEN function is the most appropriate choice for transforming semi-structured JSON data in a VARIANT column into a relational format for analysis. FLATTEN performs a lateral join that expands nested arrays into multiple rows, effectively normalizing hierarchical data into a tabular structure. When combined with other Snowflake JSON functions like GET, PARSE_JSON, or dot notation, FLATTEN enables comprehensive transformation of complex nested structures into relational tables. This is particularly useful for analytics on semi-structured data that needs to be joined with traditional relational data or analyzed using standard SQL aggregations and grouping operations.*"},{"id":54,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs different transformations on data based on its source system. The logic for determining the transformation rules is complex. Which approach should be used?","options":[{"id":"A","text":"Create separate transformation queries for each source system"},{"id":"B","text":"Implement a JavaScript stored procedure with conditional logic"},{"id":"C","text":"Use SQL CASE statements in a view"},{"id":"D","text":"Create a lookup table with transformation rules"}],"correctAnswer":"B","explanation":"Implementing a JavaScript stored procedure with conditional logic is the most flexible approach for handling complex transformation rules based on source systems. JavaScript stored procedures in Snowflake support sophisticated programming constructs, including complex conditional logic, custom functions, and external API calls if needed. This approach allows encapsulating the entire transformation logic in a single, maintainable procedure that can handle the complexity of different rules for different source systems, including edge cases and exceptions that might be difficult to express in pure SQL constructs like CASE statements.*"},{"id":55,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that maintains a slowly changing dimension (SCD) Type 2 for customer data, preserving the history of changes. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake Streams to capture changes and apply them with effective dates"},{"id":"B","text":"Implement a MERGE statement that handles both inserts and updates"},{"id":"C","text":"Create a view that uses Time Travel to show historical data"},{"id":"D","text":"Use zero-copy cloning to create daily snapshots"}],"correctAnswer":"B","explanation":"Implementing a MERGE statement that handles both inserts and updates is the most comprehensive approach for maintaining a Slowly Changing Dimension (SCD) Type 2. The MERGE statement can be designed to identify changed records, expire the current records by setting an end date, and insert new records with current effective dates in a single atomic operation. This approach ensures referential integrity and transaction consistency while efficiently implementing the SCD Type 2 pattern. While Streams can capture changes, the MERGE statement provides more direct control over the SCD-specific logic of maintaining effective dates and historical records.*"},{"id":56,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs data quality checks on transformed data before loading it into a target table. If the checks fail, the process should be halted and an alert should be sent. Which approach should be used?","options":[{"id":"A","text":"Use a stored procedure with error handling and custom logging"},{"id":"B","text":"Implement a stream to capture failed records"},{"id":"C","text":"Create a task that checks data quality before loading"},{"id":"D","text":"Use a secure view with row-level security to filter invalid data"}],"correctAnswer":"A","explanation":"Using a stored procedure with error handling and custom logging is the most comprehensive approach for implementing data quality checks with alerting capabilities. Stored procedures allow combining SQL validation queries with conditional logic to evaluate results, transaction control to commit or rollback based on quality checks, and integration with external alerting mechanisms through API calls or Snowflake features like notifications. This approach provides complete control over the validation process, error handling, and alerting workflow, making it ideal for implementing robust data quality gates in transformation pipelines.*"},{"id":57,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that executes a complex transformation workflow on a schedule, with different steps running at different frequencies. Which approach should be used?","options":[{"id":"A","text":"Create multiple independent tasks with different schedules"},{"id":"B","text":"Use a single task with a CASE statement to determine which transformations to run"},{"id":"C","text":"Implement a stored procedure that uses the current time to decide which steps to execute"},{"id":"D","text":"Create a task tree with different schedules for different branches"}],"correctAnswer":"A","explanation":"Creating multiple independent tasks with different schedules is the most appropriate approach for implementing transformation steps that need to run at different frequencies. Each task can be configured with its own schedule expression (CRON format in Snowflake) to define exactly when it should execute, allowing for precise control over execution timing. This approach maintains separation of concerns between different transformation steps while leveraging Snowflake's native scheduling capabilities. For steps with dependencies, additional task relationships can be defined to ensure proper sequencing while still maintaining independent scheduling.*"},{"id":58,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs complex string manipulations and regular expression operations as part of a data transformation pipeline. Which Snowflake feature provides the most comprehensive support for these operations?","options":[{"id":"A","text":"SQL UDFs"},{"id":"B","text":"JavaScript UDFs"},{"id":"C","text":"Built-in SQL string functions"},{"id":"D","text":"External functions"}],"correctAnswer":"C","explanation":"Snowflake's built-in SQL string functions provide the most comprehensive and efficient support for string manipulations and regular expression operations in data transformation pipelines. Snowflake offers a rich set of native functions including REGEXP_REPLACE, REGEXP_SUBSTR, SPLIT, TRANSLATE, and many others that can handle complex string transformations directly in SQL. These functions are optimized for performance within Snowflake's engine and can operate on entire columns of data in parallel. While JavaScript UDFs offer programming flexibility, the native SQL functions provide better performance and integration with Snowflake's optimization capabilities for large-scale data processing.*"},{"id":59,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs different transformations on data based on the current processing date, with special handling for month-end, quarter-end, and year-end processing. Which approach should be used?","options":[{"id":"A","text":"Create separate tasks for each type of processing period"},{"id":"B","text":"Use a stored procedure with date logic to determine the appropriate transformations"},{"id":"C","text":"Implement a CASE statement in a SQL query based on date functions"},{"id":"D","text":"Create a calendar reference table to look up processing types"}],"correctAnswer":"B","explanation":"Using a stored procedure with date logic is the most flexible approach for implementing different transformations based on the current processing date. A stored procedure can incorporate sophisticated date calculations to determine if the current date is a month-end, quarter-end, or year-end, and then execute the appropriate transformation logic for each scenario. This approach centralizes the date determination logic and transformation selection in a single, maintainable component while providing the procedural capabilities needed to handle complex conditional processing based on temporal patterns and exceptions.*"},{"id":60,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs aggregations on large datasets with high cardinality group-by columns. The aggregations need to be refreshed daily. Which approach would provide the best performance?","options":[{"id":"A","text":"Create a materialized view with the aggregation logic"},{"id":"B","text":"Use a task to calculate and store aggregates in a separate table"},{"id":"C","text":"Implement a clustering key on the group-by columns"},{"id":"D","text":"Use a JavaScript UDF to perform custom aggregations"}],"correctAnswer":"B","explanation":"Using a task to calculate and store aggregates in a separate table is the most effective approach for high-performance aggregations on large datasets with high cardinality group-by columns. This approach pre-computes the aggregations during a scheduled maintenance window and stores the results in a dedicated table optimized for query performance. Unlike materialized views which have certain limitations with complex aggregations, this approach provides complete control over the aggregation logic, indexing strategy, and refresh schedule. The task can be scheduled to run daily, ensuring the aggregated data is refreshed at the required frequency while minimizing the performance impact on user queries.*"},{"id":61,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that transforms data from a third-party API and loads it into Snowflake. The API returns complex nested JSON structures. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake's REST API integration to directly query the third-party API"},{"id":"B","text":"Create an external function that calls the API and processes the results"},{"id":"C","text":"Use a serverless function to fetch, transform, and load the data via Snowflake's native connectors"},{"id":"D","text":"Implement a JavaScript stored procedure that uses the Snowflake HTTP client to call the API"}],"correctAnswer":"D","explanation":"Implementing a JavaScript stored procedure that uses the Snowflake HTTP client is the most direct approach for transforming data from a third-party API. Snowflake's JavaScript stored procedures can use the built-in HTTP client to make API calls, process the returned JSON using JavaScript's native JSON handling capabilities, and then insert the transformed data into Snowflake tables. This approach keeps the entire ETL process within Snowflake, eliminating the need for external components or data movement, while providing the programming flexibility needed to handle complex nested JSON structures and API-specific requirements.*"},{"id":62,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs complex window functions across multiple dimensions with different partition and order specifications. Which approach would provide the most maintainable solution?","options":[{"id":"A","text":"Create multiple CTEs, each with a specific window function"},{"id":"B","text":"Use subqueries with different GROUP BY clauses"},{"id":"C","text":"Implement a JavaScript UDF that performs custom windowing"},{"id":"D","text":"Create a stored procedure that generates dynamic SQL"}],"correctAnswer":"A","explanation":"Creating multiple Common Table Expressions (CTEs), each with a specific window function, provides the most maintainable solution for complex window calculations across multiple dimensions. This approach breaks down the complex logic into modular, readable components where each CTE handles a specific windowing operation with its own partition and order specifications. The final query can then join or combine these intermediate results as needed. This modular structure improves readability, debugging, and maintenance compared to nested subqueries or dynamic SQL approaches, while leveraging Snowflake's optimization capabilities for window functions.*"},{"id":63,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs data transformations requiring custom mathematical operations not available in standard SQL functions. Which approach should be used?","options":[{"id":"A","text":"Create a SQL UDF with mathematical formulas"},{"id":"B","text":"Implement a JavaScript UDF with custom algorithms"},{"id":"C","text":"Use a Python UDF for advanced mathematical operations"},{"id":"D","text":"Create a stored procedure with mathematical logic"}],"correctAnswer":"C","explanation":"Using a Python UDF (User-Defined Function) is the most appropriate approach for implementing custom mathematical operations not available in standard SQL. Python UDFs in Snowflake allow leveraging Python's rich ecosystem of scientific and mathematical libraries like NumPy, SciPy, and pandas, which provide advanced mathematical capabilities beyond what's available in SQL or JavaScript. This approach combines the power of Python's mathematical libraries with Snowflake's data processing capabilities, enabling sophisticated calculations like statistical analysis, machine learning scoring, or complex mathematical algorithms to be applied directly within Snowflake queries.*"},{"id":64,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs incremental data transformations on a large fact table, processing only new and changed records since the last run. Which combination of Snowflake features should be used?","options":[{"id":"A","text":"Time Travel and zero-copy cloning"},{"id":"B","text":"Streams and tasks"},{"id":"C","text":"External tables with auto refresh"},{"id":"D","text":"Materialized views with automatic clustering"}],"correctAnswer":"B","explanation":"The combination of streams and tasks is the most effective solution for implementing incremental data transformations. Streams capture the changes (inserts, updates, deletes) to the source table since the last processing run, enabling the transformation logic to process only the delta rather than the entire dataset. Tasks provide the scheduling and execution framework to run these incremental transformations automatically at defined intervals. This approach minimizes processing overhead and resource consumption while ensuring the target fact table stays current with source changes, making it ideal for large fact tables where full reprocessing would be prohibitively expensive.*"},{"id":65,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that dynamically generates and executes SQL statements based on metadata stored in a control table. Which approach should be used?","options":[{"id":"A","text":"Create a JavaScript stored procedure that builds and executes dynamic SQL"},{"id":"B","text":"Use a SQL stored procedure with string concatenation"},{"id":"C","text":"Implement a Python UDF that generates SQL"},{"id":"D","text":"Create a task for each possible SQL statement"}],"correctAnswer":"A","explanation":"Creating a JavaScript stored procedure that builds and executes dynamic SQL is the most flexible approach for implementing metadata-driven transformations. JavaScript stored procedures in Snowflake can read metadata from control tables, construct SQL statements dynamically using string manipulation, and execute those statements using the \`snowflake.execute()\` method. This approach provides the programming flexibility needed to handle complex logic for SQL generation while keeping the entire process within Snowflake. It allows for sophisticated error handling, logging, and conditional logic that would be difficult to implement with SQL stored procedures or other approaches.*"}]`),wm=[{id:1,name:"Data Movement (17 questions)"},{id:2,name:"Performance Optimization (14 questions)"},{id:3,name:"Storage and Data Protection (9 questions)"},{id:4,name:"Data Governance (9 questions)"},{id:5,name:"Data Transformation (16 questions)"}],ym={id:fm,title:mm,description:gm,questions:vm,domains:wm},xm=3,Sm="Practice Test 3",bm="Snowflake Advanced Data Engineering Certificate (DEA-C02) - Practice Test 3",km=JSON.parse(`[{"id":1,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from an external stage where files are continuously added throughout the day. The business requires the data to be available in Snowflake within minutes of being placed in the external stage. Which approach should be implemented?","options":[{"id":"A","text":"Schedule a task to run the COPY command every 5 minutes"},{"id":"B","text":"Use Snowpipe with auto-ingest enabled"},{"id":"C","text":"Create an external table with AUTO_REFRESH = TRUE"},{"id":"D","text":"Implement a stored procedure that checks for new files"}],"correctAnswer":"B","explanation":"Snowpipe with auto-ingest enabled is the most appropriate solution for loading data within minutes of arrival in an external stage. When auto-ingest is enabled, Snowflake automatically consumes cloud storage notifications (like AWS S3 events, Azure Event Grid, or GCP Pub/Sub) when new files are added to the stage. This triggers Snowpipe to load the new data immediately without manual intervention or scheduled polling, ensuring data is available in Snowflake within minutes of being placed in the external stage, meeting the near real-time requirement efficiently.*"},{"id":2,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution to load data from an on-premises Oracle database to Snowflake in near real-time. Which approach is most appropriate?","options":[{"id":"A","text":"Use Snowflake's Oracle connector"},{"id":"B","text":"Implement a CDC solution with Kafka and Snowpipe"},{"id":"C","text":"Schedule hourly full extracts using the COPY command"},{"id":"D","text":"Use Snowflake's database replication feature"}],"correctAnswer":"B","explanation":"Implementing a Change Data Capture (CDC) solution with Kafka and Snowpipe is the most appropriate approach for near real-time data loading from an on-premises Oracle database. This architecture captures changes in Oracle as they occur (using Oracle's LogMiner or similar technology), streams them through Kafka, and loads them into Snowflake using Snowpipe. This provides a scalable, reliable solution for near real-time data integration between on-premises systems and Snowflake with minimal latency and resource impact on the source system. The other options either don't support real-time integration or aren't applicable for Oracle to Snowflake integration.*"},{"id":3,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from multiple Parquet files where some files may contain schema variations. Which approach should be used to handle these variations without failing the load process?","options":[{"id":"A","text":"Use the MATCH_BY_COLUMN_NAME option in the COPY command"},{"id":"B","text":"Create a separate file format for each schema variation"},{"id":"C","text":"Convert all files to a consistent schema before loading"},{"id":"D","text":"Use external tables with schema evolution enabled"}],"correctAnswer":"A","explanation":"The MATCH_BY_COLUMN_NAME option in the COPY command is the most appropriate approach for handling Parquet files with schema variations. This option instructs Snowflake to match columns in the source files to the target table based on column names rather than positions. When schema variations exist in the Parquet files, columns that match the target table will be loaded correctly, extra columns will be ignored, and missing columns will be filled with NULL values. This provides flexibility for handling schema variations without requiring pre-processing or separate loading processes for different file variants.*"},{"id":4,"domain":"Data Movement (17 questions)","text":"A data engineer needs to share sensitive customer data with a partner organization that doesn't have a Snowflake account. Which method provides the most secure and efficient way to share this data?","options":[{"id":"A","text":"Export the data to a secure cloud storage and provide access"},{"id":"B","text":"Create a reader account for the partner"},{"id":"C","text":"Use Snowflake Data Exchange"},{"id":"D","text":"Create database clones and provide separate login credentials"}],"correctAnswer":"B","explanation":"Creating a reader account is the most secure and efficient method for sharing sensitive data with a partner organization that doesn't have a Snowflake account. A reader account is a special type of Snowflake account that allows external organizations to securely access shared data without needing their own full Snowflake account. The data provider maintains control over what data is shared and can revoke access at any time. This approach keeps the data within Snowflake's secure environment, eliminating the risks associated with exporting data, while providing a controlled, auditable access method for external partners.*"},{"id":5,"domain":"Data Movement (17 questions)","text":"A data engineer is loading data from a source that occasionally produces duplicate records. Which approach should be used to ensure no duplicates are loaded into the target Snowflake table?","options":[{"id":"A","text":"Use a primary key constraint on the target table"},{"id":"B","text":"Implement a MERGE statement with a unique key condition"},{"id":"C","text":"Create a unique index on the target table"},{"id":"D","text":"Use the COPY command with ON_ERROR = SKIP_DUPLICATE"}],"correctAnswer":"B","explanation":"Implementing a MERGE statement with a unique key condition is the most effective approach for preventing duplicate records during data loading. The MERGE statement allows you to define a matching condition based on a business key or unique identifier, and then specify different actions for matching records (update or ignore) versus non-matching records (insert). This gives you precise control over how potential duplicates are handled, allowing you to either update existing records or skip the insertion of duplicates while still loading new, unique records. Snowflake doesn't support traditional primary key constraints or unique indexes for enforcement, making MERGE the most appropriate solution.*"},{"id":6,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a REST API that requires OAuth authentication and returns paginated results. Which approach should be used?","options":[{"id":"A","text":"Use a JavaScript stored procedure with the Snowflake HTTP client"},{"id":"B","text":"Create an external function that calls the API"},{"id":"C","text":"Use Snowflake's REST API integration"},{"id":"D","text":"Implement a Python UDF to call the API"}],"correctAnswer":"A","explanation":"A JavaScript stored procedure with the Snowflake HTTP client is the most appropriate approach for loading data from a REST API with OAuth authentication and pagination. JavaScript procedures in Snowflake can use the built-in HTTP client to make authenticated API calls, handle OAuth token management, process the response to extract both data and pagination tokens/links, and then loop through all pages by making subsequent requests with updated pagination parameters. This approach provides the programming flexibility needed to handle the API's authentication and pagination mechanisms while keeping the entire process within Snowflake.*"},{"id":7,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that loads data from multiple source systems with different file formats into a single Snowflake table. Which approach is most efficient?","options":[{"id":"A","text":"Convert all data to a single format before loading"},{"id":"B","text":"Create separate staging tables for each format, then merge"},{"id":"C","text":"Use different file formats with the same COPY command"},{"id":"D","text":"Use separate COPY commands with appropriate file formats for each source"}],"correctAnswer":"D","explanation":"Using separate COPY commands with appropriate file formats for each source is the most efficient approach for loading multiple data formats into a single table. Each COPY command can reference a file format specifically configured for its corresponding data format (CSV, JSON, Parquet, etc.), ensuring optimal parsing and loading for each type. This approach provides clarity, maintainability, and optimal performance for each format without requiring pre-conversion or intermediate staging tables. The COPY commands can be orchestrated to run in sequence or parallel as needed to populate the single target table.*"},{"id":8,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution to handle late-arriving data in a Snowflake data warehouse. The solution needs to ensure that historical aggregates are correctly updated when late data arrives. Which approach is most appropriate?","options":[{"id":"A","text":"Use Snowflake Time Travel to reprocess historical data"},{"id":"B","text":"Implement a slowly changing dimension (SCD) Type 2 approach"},{"id":"C","text":"Use Snowflake Streams and Tasks to process incremental changes"},{"id":"D","text":"Create a separate table for late-arriving data"}],"correctAnswer":"C","explanation":"Using Snowflake Streams and Tasks to process incremental changes is the most appropriate approach for handling late-arriving data that affects historical aggregates. Streams capture all changes (inserts, updates, deletes) to the source data, including late-arriving records. By combining streams with tasks that reprocess affected time periods in the aggregates, you can ensure that historical aggregates are correctly updated when late data arrives. This incremental approach is more efficient than full reprocessing and more automated than maintaining separate tables, providing a scalable solution for maintaining accuracy in the presence of late-arriving data.*"},{"id":9,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source system that produces files with inconsistent naming patterns. Which approach should be used to ensure all relevant files are loaded?","options":[{"id":"A","text":"Use pattern matching in the COPY command's FILES parameter"},{"id":"B","text":"Create a stored procedure that lists and loads files based on content"},{"id":"C","text":"Use Snowpipe with a custom file notification mechanism"},{"id":"D","text":"Implement an external stage with a file listing function"}],"correctAnswer":"A","explanation":"Using pattern matching in the COPY command's FILES parameter is the most straightforward approach for loading files with inconsistent naming patterns. The FILES parameter supports wildcards and pattern matching expressions (like 'data_*.csv' or 'logs/202[0-9]-\\\\\\\\d{2}-\\\\\\\\d{2}_.*.parquet') that can be used to select files based on partial name matches or regular expression patterns. This approach allows you to define flexible patterns that capture all relevant files despite naming inconsistencies, without requiring custom procedures or external mechanisms.*"},{"id":10,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows querying data directly from files in cloud storage without loading it into Snowflake tables. Which Snowflake feature should be used?","options":[{"id":"A","text":"Zero-copy cloning"},{"id":"B","text":"External tables"},{"id":"C","text":"Materialized views"},{"id":"D","text":"Secure views"}],"correctAnswer":"B","explanation":"External tables are the appropriate Snowflake feature for querying data directly from files in cloud storage without loading it into Snowflake tables. External tables create a table structure that references data stored in external cloud storage locations (like S3, Azure Blob, or GCS) rather than in Snowflake's internal storage. This allows querying the external data using standard SQL without first loading or copying it into Snowflake. External tables are ideal for scenarios where data needs to remain in cloud storage due to size, governance requirements, or integration with other systems that also access the same files.*"},{"id":11,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source system that occasionally produces malformed JSON records. Which approach should be used to handle these malformed records without failing the entire load?","options":[{"id":"A","text":"Use the VALIDATE_JSON function in a pre-processing step"},{"id":"B","text":"Set PARSE_JSON = 'PERMISSIVE' in the file format"},{"id":"C","text":"Use ON_ERROR = 'CONTINUE' in the COPY command"},{"id":"D","text":"Implement a JavaScript UDF to clean the JSON"}],"correctAnswer":"C","explanation":"Using ON_ERROR = 'CONTINUE' in the COPY command is the most direct approach for handling malformed records without failing the entire load. This option instructs Snowflake to skip individual records that cause errors (like malformed JSON) and continue processing the remaining records. Error details for skipped records can be captured and reviewed using the VALIDATION_MODE parameter or by querying load history. This approach provides resilience against occasional data quality issues while still loading all valid records, making it ideal for production pipelines that need to handle imperfect source data.*"},{"id":12,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution to track the history of data loads, including file names, row counts, and error information. Which Snowflake feature should be used?","options":[{"id":"A","text":"Query history"},{"id":"B","text":"Information schema"},{"id":"C","text":"Account usage views"},{"id":"D","text":"Automatic query optimization"}],"correctAnswer":"C","explanation":"Snowflake's Account Usage views are the most appropriate feature for tracking the history of data loads. Specifically, views like COPY_HISTORY and LOAD_HISTORY in the SNOWFLAKE.ACCOUNT_USAGE schema provide detailed information about data loading operations, including file names, row counts, error counts, and other metrics. These views maintain historical information for a longer period than the Information Schema views and provide more comprehensive details about load operations than query history alone. This makes them ideal for building load monitoring and auditing solutions.*"},{"id":13,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that replicates data from a Snowflake account in AWS to another Snowflake account in Azure. Which feature should be used?","options":[{"id":"A","text":"Snowflake Database Replication"},{"id":"B","text":"Snowflake Failover Groups"},{"id":"C","text":"Snowflake Data Sharing"},{"id":"D","text":"Snowflake External Tables"}],"correctAnswer":"A","explanation":"Snowflake Database Replication is specifically designed to replicate databases across different Snowflake accounts, including across different cloud providers (AWS to Azure in this case). This feature provides a way to maintain synchronized copies of databases in different regions or cloud platforms for disaster recovery, global data distribution, or cloud migration scenarios. Data Sharing provides read-only access but doesn't replicate the data, while the other options don't address cross-account replication needs. Database Replication ensures that changes made in the source account are automatically propagated to the target account, maintaining data consistency across clouds.*"},{"id":14,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source that produces files with headers that should be skipped during loading. Which parameter should be used in the file format definition?","options":[{"id":"A","text":"SKIP_HEADER = 1"},{"id":"B","text":"PARSE_HEADER = TRUE"},{"id":"C","text":"HEADER_ROWS = 1"},{"id":"D","text":"IGNORE_HEADER = TRUE"}],"correctAnswer":"A","explanation":"SKIP_HEADER = 1 is the correct parameter to use in the file format definition when you need to skip header rows during data loading. This parameter instructs Snowflake to ignore the specified number of lines at the beginning of each file, which is typically used for files with header rows that contain column names rather than data. This ensures that header information is not loaded as data rows, preventing data quality issues. The other options are either not valid Snowflake parameters or have different purposes than skipping header rows during loading.*"},{"id":15,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution to handle files that occasionally arrive with no data (empty files) in an external stage. Which approach should be used to prevent errors during the load process?","options":[{"id":"A","text":"Use the VALIDATE_EMPTY_FILES = TRUE parameter in the COPY command"},{"id":"B","text":"Implement a stored procedure that checks file size before loading"},{"id":"C","text":"Use ON_ERROR = 'SKIP_FILE' in the COPY command"},{"id":"D","text":"Create a file format with EMPTY_FILE_HANDLING = 'IGNORE'"}],"correctAnswer":"B","explanation":"Implementing a stored procedure that checks file size before loading is the most robust approach for handling potentially empty files. The procedure can use Snowflake's metadata functions or stage listing capabilities to identify empty files and either skip them or handle them differently in the loading process. While Snowflake's COPY command has error handling options, there isn't a specific parameter for empty file handling like VALIDATE_EMPTY_FILES or EMPTY_FILE_HANDLING. The ON_ERROR option would only apply after an error occurs, making proactive checking more efficient and providing better control and logging of empty file scenarios.*"},{"id":16,"domain":"Data Movement (17 questions)","text":"A data engineer is setting up a Snowpipe to load data from an Azure Blob Storage container. Which Azure service is required to enable auto-ingest functionality?","options":[{"id":"A","text":"Azure Functions"},{"id":"B","text":"Azure Event Grid"},{"id":"C","text":"Azure Logic Apps"},{"id":"D","text":"Azure Data Factory"}],"correctAnswer":"B","explanation":"Azure Event Grid is required to enable auto-ingest functionality for Snowpipe when loading data from Azure Blob Storage. Event Grid is used to notify Snowflake when new blobs are added to the container, which then triggers Snowpipe to automatically load the new data. This integration enables near-real-time data loading without manual intervention or scheduled tasks. The configuration involves setting up an Event Grid subscription that forwards blob creation events to a Snowflake-provided endpoint, which then triggers the appropriate Snowpipe for loading. This is the Azure equivalent of using SQS for auto-ingest from AWS S3.*"},{"id":17,"domain":"Data Movement (17 questions)","text":"A data engineer needs to load data from a source system that produces XML files. Which approach should be used to load this data into Snowflake?","options":[{"id":"A","text":"Use the XML file format in the COPY command"},{"id":"B","text":"Convert the XML to JSON or CSV before loading"},{"id":"C","text":"Use external tables with XML parsing"},{"id":"D","text":"Implement a stored procedure with XML parsing functions"}],"correctAnswer":"B","explanation":"Converting the XML to JSON or CSV before loading is the most appropriate approach for handling XML files in Snowflake. Snowflake doesn't have native support for XML as a file format in the COPY command or for external tables. By converting XML to JSON, you can leverage Snowflake's robust semi-structured data capabilities, including the VARIANT data type and JSON parsing functions. Alternatively, converting to CSV allows using Snowflake's optimized loading for structured data. This conversion can be done using external ETL tools, custom scripts, or cloud functions before staging the data for loading into Snowflake.*"},{"id":18,"domain":"Data Movement (17 questions)","text":"A data engineer notices that queries against a large fact table are performing poorly despite having appropriate filters. Upon investigation, it's found that the table has over 100,000 micro-partitions. What is the most likely cause of this issue?","options":[{"id":"A","text":"The warehouse size is too small"},{"id":"B","text":"The table has too many micro-partitions for efficient pruning"},{"id":"C","text":"The clustering key is not aligned with common query filters"},{"id":"D","text":"The table needs to be re-clustered"}],"correctAnswer":"B","explanation":"Having over 100,000 micro-partitions in a table is likely causing the poor query performance. Snowflake recommends keeping the number of micro-partitions in a table between 1,000 and 10,000 for optimal performance. When a table has too many micro-partitions, the metadata overhead for partition pruning increases significantly, which can negate the benefits of pruning and lead to slower query performance. This situation typically occurs when tables have many small files loaded over time or when the table size is relatively small compared to the number of partitions, resulting in many small micro-partitions rather than fewer, optimally-sized ones.*"},{"id":19,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that joins a large fact table with several dimension tables. The fact table is properly clustered, but query performance is still poor. Which approach would most likely improve performance?","options":[{"id":"A","text":"Create a materialized view that pre-joins the tables"},{"id":"B","text":"Increase the warehouse size"},{"id":"C","text":"Add result caching"},{"id":"D","text":"Implement a clustering key on the dimension tables"}],"correctAnswer":"A","explanation":"Creating a materialized view that pre-joins the fact and dimension tables would most likely improve performance for this scenario. Materialized views in Snowflake physically store the results of the view definition query, including joins, and automatically maintain these results as the underlying tables change. For complex joins between large fact tables and dimension tables that are frequently queried together, materializing the join results eliminates the need to recompute the joins for each query. This provides significant performance benefits, especially when the same join patterns are used repeatedly in different queries or reports.*"},{"id":20,"domain":"Data Movement (17 questions)","text":"A data engineer is analyzing query performance and notices that a specific query is scanning all micro-partitions in a table despite having filters that should allow partition pruning. Which approach should be used to diagnose this issue?","options":[{"id":"A","text":"Check the clustering depth of the table"},{"id":"B","text":"Review the query profile to examine partition pruning metrics"},{"id":"C","text":"Analyze the compression ratio of the table"},{"id":"D","text":"Check if the table has too many columns"}],"correctAnswer":"B","explanation":"Reviewing the query profile to examine partition pruning metrics is the most direct approach for diagnosing why a query is scanning all micro-partitions despite having filters that should enable pruning. The query profile in Snowflake provides detailed information about partition pruning efficiency, showing exactly how many partitions were scanned versus pruned for each table in the query. This information can help identify whether the issue is related to clustering keys, filter conditions, data distribution, or other factors affecting pruning efficiency. Understanding these metrics is essential for implementing effective performance optimizations.*"},{"id":21,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs an aggregation over a large dataset with a GROUP BY clause on a date column. The date values are distributed across the entire range with some dates having significantly more data than others. Which approach would be most effective?","options":[{"id":"A","text":"Add a clustering key on the date column"},{"id":"B","text":"Create a materialized view with the pre-computed aggregation"},{"id":"C","text":"Use a larger warehouse size"},{"id":"D","text":"Implement a result cache"}],"correctAnswer":"A","explanation":"Adding a clustering key on the date column would be most effective for optimizing this query. Clustering organizes the micro-partitions to co-locate similar date values, which is particularly beneficial when data distribution is uneven across the date range. When the query filters or groups by the date column, Snowflake can efficiently prune irrelevant micro-partitions and process each date group with minimal data movement. This approach directly addresses the data distribution challenge and improves both filtering and grouping operations on the date column, making it more effective than simply increasing resources or caching results.*"},{"id":22,"domain":"Data Movement (17 questions)","text":"A data engineer is optimizing a data warehouse for a mixed workload of both interactive dashboards and long-running ETL processes. Which approach would provide the best overall performance?","options":[{"id":"A","text":"Use a single X-Large warehouse for all workloads"},{"id":"B","text":"Create separate warehouses for different workload types"},{"id":"C","text":"Implement a multi-cluster warehouse with maximum clusters set to 10"},{"id":"D","text":"Use a single warehouse with auto-suspend set to 60 seconds"}],"correctAnswer":"B","explanation":"Creating separate warehouses for different workload types (interactive dashboards vs. long-running ETL) provides the best overall performance for a mixed workload environment. This approach allows optimizing each warehouse for its specific workload characteristics - smaller, more responsive warehouses with aggressive scaling for interactive dashboards, and larger, more stable warehouses for ETL processes. Separating these workloads prevents resource contention where long-running ETL jobs might block interactive queries, ensuring consistent performance for both use cases. This approach also enables workload-specific configurations for auto-suspend, scaling policies, and resource monitoring.*"},{"id":23,"domain":"Data Movement (17 questions)","text":"A data engineer notices that a critical query is performing poorly due to a large JOIN operation between two tables. Which Snowflake feature would provide the most insight into optimizing this specific operation?","options":[{"id":"A","text":"EXPLAIN plan"},{"id":"B","text":"Query Profile"},{"id":"C","text":"INFORMATION_SCHEMA.QUERY_HISTORY"},{"id":"D","text":"SHOW WAREHOUSES"}],"correctAnswer":"B","explanation":"The Query Profile feature in Snowflake would provide the most insight into optimizing a poorly performing JOIN operation. Unlike the EXPLAIN plan which shows the intended execution plan before running the query, the Query Profile shows detailed metrics about the actual execution after it completes. For JOIN operations specifically, it provides critical information about join type selection, data distribution, spilling to disk, and operator execution times. This visual, interactive tool allows drilling into specific operators to identify exactly where and why the JOIN is performing poorly, making it the most valuable feature for diagnosing and optimizing complex JOIN operations.*"},{"id":24,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize storage costs for a large historical table where only the most recent data is frequently accessed. Which approach would be most effective?","options":[{"id":"A","text":"Implement a clustering key on the date column"},{"id":"B","text":"Use table partitioning to separate hot and cold data"},{"id":"C","text":"Create a materialized view for the recent data"},{"id":"D","text":"Move older data to a separate table with a smaller warehouse"}],"correctAnswer":"A","explanation":"Implementing a clustering key on the date column is the most effective approach for optimizing storage costs while maintaining query performance on a table with hot and cold data patterns. Clustering ensures that recent (hot) data is co-located in the same micro-partitions, enabling efficient pruning when queries filter on recent dates. Snowflake's automatic clustering maintenance focuses on the most frequently accessed micro-partitions, naturally optimizing for hot data access patterns. Additionally, Snowflake's storage billing is based on actual compressed storage used, and clustering doesn't increase this cost, making it more cost-effective than creating duplicate structures or moving data between tables.*"},{"id":25,"domain":"Data Movement (17 questions)","text":"A data engineer is analyzing a slow-running query and notices in the query profile that a large amount of data is being redistributed across nodes during a JOIN operation. Which approach would most likely improve performance?","options":[{"id":"A","text":"Increase the warehouse size"},{"id":"B","text":"Add a clustering key on the join columns"},{"id":"C","text":"Use a broadcast join instead of a hash join"},{"id":"D","text":"Implement a materialized view"}],"correctAnswer":"B","explanation":"Adding a clustering key on the join columns would most likely improve performance when excessive data redistribution is occurring during JOIN operations. Clustering the tables on their join keys improves data locality, potentially enabling Snowflake's query optimizer to choose more efficient join strategies that require less data movement between nodes. When data with the same join key values is co-located in the same micro-partitions, the database can often perform more localized join processing, reducing the need for large-scale data redistribution across the cluster. This directly addresses the specific performance issue observed in the query profile.*"},{"id":26,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs multiple window functions over the same partition and order specification. Which approach would be most effective?","options":[{"id":"A","text":"Rewrite the query to use GROUP BY instead of window functions"},{"id":"B","text":"Split the query into multiple simpler queries"},{"id":"C","text":"Use a CTE to compute the window specification once"},{"id":"D","text":"Increase the warehouse size"}],"correctAnswer":"C","explanation":"Using a Common Table Expression (CTE) to compute the window specification once is the most effective approach for optimizing a query with multiple window functions using the same partition and order specification. This approach allows defining the partitioning and ordering logic in one place and reusing it across multiple window functions, which can improve both query performance and maintainability. Snowflake's optimizer can potentially recognize the common window specification and optimize the execution accordingly, reducing redundant sorting and partitioning operations that would otherwise be repeated for each window function.*"},{"id":27,"domain":"Data Movement (17 questions)","text":"A data engineer notices that a query with a complex WHERE clause containing multiple OR conditions is performing poorly. Which approach would most likely improve performance?","options":[{"id":"A","text":"Split the query into multiple UNION ALL queries with simpler conditions"},{"id":"B","text":"Rewrite the conditions using CASE statements"},{"id":"C","text":"Add a result cache for the query"},{"id":"D","text":"Use a larger warehouse size"}],"correctAnswer":"A","explanation":"Splitting the query into multiple UNION ALL queries with simpler conditions would most likely improve performance for a query with multiple OR conditions. Complex OR conditions often prevent effective partition pruning and index usage, as the optimizer must consider the union of all possible matching data. By rewriting the query as a UNION ALL of simpler queries, each with a single condition or simpler AND conditions, you enable more efficient partition pruning for each individual query. Snowflake can then optimize each part of the UNION ALL independently, potentially resulting in significantly better overall performance, especially for large tables with appropriate clustering keys.*"},{"id":28,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs an aggregation over a large dataset with a GROUP BY clause on a high-cardinality column. The query is currently using a medium-sized warehouse and is taking too long to complete. Which approach would be most effective?","options":[{"id":"A","text":"Increase the warehouse size to X-Large"},{"id":"B","text":"Add a clustering key on the GROUP BY column"},{"id":"C","text":"Create a materialized view with the pre-computed aggregation"},{"id":"D","text":"Use approximate aggregation functions"}],"correctAnswer":"B","explanation":"Adding a clustering key on the high-cardinality GROUP BY column would be most effective for optimizing this aggregation query. Clustering improves the locality of data with similar values, which significantly enhances the performance of GROUP BY operations by reducing data shuffling and improving aggregation efficiency. When data is clustered by the grouping column, Snowflake can process each group more efficiently by accessing co-located data. While increasing warehouse size might help somewhat, it doesn't address the fundamental data organization issue, and materialized views might be less flexible if query parameters change frequently.*"},{"id":29,"domain":"Data Movement (17 questions)","text":"A data engineer is designing a solution for a reporting system that needs to query the same large dataset multiple times with different parameters. The data is updated hourly. Which feature would provide the best query performance?","options":[{"id":"A","text":"Zero-copy cloning"},{"id":"B","text":"Materialized views"},{"id":"C","text":"Multi-cluster warehouses"},{"id":"D","text":"Result caching"}],"correctAnswer":"B","explanation":"Materialized views would provide the best query performance for a reporting system that repeatedly queries the same large dataset with different parameters. Materialized views pre-compute and store query results, including joins and aggregations, and automatically maintain these results as the underlying data changes hourly. Unlike result caching, which only helps if the exact same query is run again, materialized views can accelerate a variety of queries against the same base tables with different parameters. This is particularly valuable for reporting systems where the base data structure is consistent but is analyzed from different angles or with different filters.*"},{"id":30,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that performs a self-join on a large table. Which approach would be most effective?","options":[{"id":"A","text":"Create a materialized view that pre-computes the join"},{"id":"B","text":"Use a CTE to reference the table only once"},{"id":"C","text":"Increase the warehouse size"},{"id":"D","text":"Add clustering keys on the join columns"}],"correctAnswer":"D","explanation":"Adding clustering keys on the join columns would be most effective for optimizing a self-join on a large table. Clustering ensures that rows with the same join key values are co-located in the same micro-partitions, which significantly improves join performance by reducing data movement and enabling more efficient join strategies. For self-joins specifically, this approach is particularly effective because the same clustering key benefits both sides of the join. While CTEs and materialized views might help in some scenarios, they don't address the fundamental data organization issue that affects join performance, making clustering the most direct and effective optimization for this specific query pattern.*"},{"id":31,"domain":"Data Movement (17 questions)","text":"A data engineer needs to optimize a query that filters on a date range but is not benefiting from micro-partition pruning. What is the most likely cause?","options":[{"id":"A","text":"The date column is not the first column in the clustering key"},{"id":"B","text":"The date values are stored as strings instead of DATE type"},{"id":"C","text":"The query is using a function on the date column in the filter"},{"id":"D","text":"The warehouse size is too small for effective pruning"}],"correctAnswer":"C","explanation":"The most likely cause for a date range query not benefiting from micro-partition pruning is that the query is using a function on the date column in the filter condition. When functions or expressions are applied to a column in a WHERE clause (e.g., \`WHERE MONTH(date_col) = 3\` instead of \`WHERE date_col BETWEEN '2023-03-01' AND '2023-03-31'\`), Snowflake cannot use the metadata about the column's values in each micro-partition for pruning. This forces a full scan of the column data. Rewriting the query to apply conditions directly to the column without wrapping it in functions will enable Snowflake to use partition pruning and significantly improve performance.*"},{"id":32,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows querying historical data as of any point in time within the last 60 days. Which Snowflake feature and configuration should be used?","options":[{"id":"A","text":"Time Travel with a 60-day retention period in Enterprise Edition"},{"id":"B","text":"Fail-safe with extended retention"},{"id":"C","text":"Zero-copy cloning with daily snapshots"},{"id":"D","text":"Continuous data protection with 60-day retention"}],"correctAnswer":"A","explanation":"Time Travel with a 60-day retention period in Enterprise Edition is the correct solution for querying historical data as of any point in time within the last 60 days. Snowflake's Enterprise Edition allows extending the Time Travel retention period up to 90 days, which covers the 60-day requirement. Time Travel enables querying data as it existed at specific points in time using the AT or BEFORE clause in queries, providing the exact point-in-time query capability needed. Fail-safe is not user-accessible for queries, and while daily snapshots with cloning could provide discrete recovery points, they wouldn't allow querying at any arbitrary point in time.*"},{"id":33,"domain":"Data Movement (17 questions)","text":"A data engineer accidentally dropped a production schema and needs to recover it. The schema was dropped 6 hours ago. Which command should be used to recover the schema?","options":[{"id":"A","text":"UNDROP SCHEMA schemaname"},{"id":"B","text":"RESTORE SCHEMA schemaname"},{"id":"C","text":"CLONE SCHEMA schemaname AT OFFSET = -6h"},{"id":"D","text":"RECOVER SCHEMA schemaname"}],"correctAnswer":"A","explanation":"UNDROP SCHEMA schemaname is the correct command to recover a schema that was dropped 6 hours ago. Snowflake's UNDROP command restores objects (tables, schemas, databases) that have been dropped, as long as they are still within the Time Travel retention period. Since the schema was dropped only 6 hours ago, it is well within even the standard 1-day retention period. The UNDROP command will restore the schema to its state just before it was dropped, including all contained objects like tables and views. The other commands are either not valid Snowflake commands for this purpose or don't address the specific recovery scenario described.*"},{"id":34,"domain":"Data Movement (17 questions)","text":"A data engineer needs to create a development environment from a production database without consuming additional storage space initially. Which Snowflake feature should be used?","options":[{"id":"A","text":"Database replication"},{"id":"B","text":"Zero-copy cloning"},{"id":"C","text":"Table sharing"},{"id":"D","text":"Fail-safe recovery"}],"correctAnswer":"B","explanation":"Zero-copy cloning is the ideal feature for creating a development environment from a production database without initially consuming additional storage space. When a clone is created, it references the same micro-partitions as the source object without duplicating data. Only when changes are made to either the source or the clone does Snowflake create new micro-partitions to store the differences. This allows creating an independent development environment that can diverge from production as needed, while minimizing storage costs. This approach is perfect for development and testing environments that need to start with production data but evolve independently.*"},{"id":35,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution to protect against accidental updates or deletes in a critical table. Which approach provides the best protection while still allowing legitimate modifications?","options":[{"id":"A","text":"Create a backup table using zero-copy cloning"},{"id":"B","text":"Implement a stored procedure for all data modifications"},{"id":"C","text":"Use table access control with custom roles"},{"id":"D","text":"Enable Change Tracking on the table"}],"correctAnswer":"B","explanation":"Implementing a stored procedure for all data modifications provides the best protection against accidental updates or deletes while still allowing legitimate modifications. By channeling all data modifications through stored procedures, you can implement validation logic, business rules, and safety checks that prevent accidental or unauthorized changes while still allowing legitimate modifications to proceed. This approach provides a controlled interface for data modifications with proper error handling and logging, reducing the risk of accidental data corruption while maintaining the flexibility needed for normal operations. It's more targeted than general access controls and more proactive than relying on after-the-fact recovery mechanisms.*"},{"id":36,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a cost-effective solution for long-term retention of historical data that is rarely accessed. The data must remain queryable but with lower performance expectations. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake's external tables with cloud storage"},{"id":"B","text":"Implement a multi-cluster warehouse with auto-scaling"},{"id":"C","text":"Create a separate database with extended Time Travel"},{"id":"D","text":"Use table partitioning with different storage tiers"}],"correctAnswer":"A","explanation":"Using Snowflake's external tables with cloud storage is the most cost-effective solution for long-term retention of rarely accessed historical data. This approach leverages cheaper cloud storage (like S3, Azure Blob, or GCS) for the actual data files while maintaining queryability through Snowflake's external tables feature. When queries are executed against external tables, Snowflake reads the data directly from cloud storage, which is significantly less expensive than Snowflake's native storage for rarely accessed data. While query performance may be somewhat reduced compared to internal tables, this aligns with the stated lower performance expectations for this historical data.*"},{"id":37,"domain":"Data Movement (17 questions)","text":"A data engineer needs to understand the storage consumption patterns of different tables in a Snowflake database. Which view should be queried?","options":[{"id":"A","text":"SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS"},{"id":"B","text":"INFORMATION_SCHEMA.TABLES"},{"id":"C","text":"SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY"},{"id":"D","text":"INFORMATION_SCHEMA.USAGE_METRICS"}],"correctAnswer":"A","explanation":"SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS is the correct view to query for understanding storage consumption patterns of different tables. This view provides detailed information about storage usage at the table level, including active bytes (currently in use), time travel bytes (retained for historical queries), fail-safe bytes, and total bytes. It also includes metrics on the number of micro-partitions and average micro-partition size. This comprehensive storage information enables data engineers to identify tables with high storage consumption, monitor growth trends, and implement targeted optimizations to manage storage costs effectively.*"},{"id":38,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that maintains a full history of all changes to a table, including records of who made each change and when. Which approach should be used?","options":[{"id":"A","text":"Enable Change Tracking on the table"},{"id":"B","text":"Implement a slowly changing dimension (SCD) Type 2 design"},{"id":"C","text":"Use Snowflake Streams with metadata columns"},{"id":"D","text":"Create triggers that log changes to an audit table"}],"correctAnswer":"B","explanation":"Implementing a slowly changing dimension (SCD) Type 2 design is the most comprehensive approach for maintaining a full history of all changes to a table, including change metadata. In an SCD Type 2 implementation, rather than overwriting existing records, new versions are created with effective date ranges and metadata columns that can capture who made each change and when. This preserves the complete history of how data has evolved over time, allowing point-in-time analysis and full auditability. While Streams can capture changes, they don't inherently maintain the historical record, and Change Tracking only indicates that changes occurred without preserving the previous values.*"},{"id":39,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows recovering from both accidental data corruption and regional cloud provider outages. Which combination of Snowflake features provides the most comprehensive protection?","options":[{"id":"A","text":"Time Travel and Fail-safe"},{"id":"B","text":"Database Replication across regions and Time Travel"},{"id":"C","text":"Zero-copy cloning and Fail-safe"},{"id":"D","text":"External tables and Database Replication"}],"correctAnswer":"B","explanation":"The combination of Database Replication across regions and Time Travel provides the most comprehensive protection against both data corruption and regional cloud provider outages. Database Replication creates a synchronized copy of databases in different geographical regions, protecting against regional outages by enabling failover to an unaffected region. Time Travel complements this by providing protection against logical data corruption, allowing point-in-time recovery within the retention period. Together, these features address both physical infrastructure failures and logical data integrity issues, providing a comprehensive disaster recovery solution with both geographical redundancy and temporal recovery capabilities.*"},{"id":40,"domain":"Data Movement (17 questions)","text":"A data engineer needs to estimate the potential cost savings from reducing the Time Travel retention period for a large database. Which Snowflake view should be queried?","options":[{"id":"A","text":"SNOWFLAKE.ACCOUNT_USAGE.DATABASE_STORAGE_USAGE_HISTORY"},{"id":"B","text":"INFORMATION_SCHEMA.TABLE_STORAGE_METRICS"},{"id":"C","text":"SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE"},{"id":"D","text":"INFORMATION_SCHEMA.DATABASES"}],"correctAnswer":"A","explanation":"SNOWFLAKE.ACCOUNT_USAGE.DATABASE_STORAGE_USAGE_HISTORY is the most appropriate view for estimating cost savings from reducing Time Travel retention. This view provides historical storage usage metrics at the database level, including a breakdown of storage used for active data versus Time Travel data. By analyzing the TIME_TRAVEL_BYTES metric over time for the database in question, the data engineer can quantify how much storage is currently being consumed by Time Travel and estimate the potential savings from reducing the retention period. This historical view provides more comprehensive data for cost analysis than point-in-time metrics from other views.*"},{"id":41,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows specific users to see only masked versions of sensitive columns in a customer table, while allowing analysts to see the actual data. Which Snowflake feature should be used?","options":[{"id":"A","text":"Column-level security"},{"id":"B","text":"Row-level security"},{"id":"C","text":"Dynamic data masking"},{"id":"D","text":"Secure views"}],"correctAnswer":"C","explanation":"Dynamic data masking is the most appropriate feature for this requirement. It allows defining masking policies that show different versions of the same data to different users based on their roles and privileges. Unlike column-level security which simply grants or denies access to entire columns, dynamic masking allows some users (like analysts) to see the actual data while others see masked versions (like partial credit card numbers or anonymized values). This provides fine-grained access control without duplicating data or creating multiple views, making it ideal for scenarios where different user groups need different levels of access to sensitive information.*"},{"id":42,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a security model where analysts can only see data for customers in their assigned regions. Which Snowflake feature should be used?","options":[{"id":"A","text":"Object tagging"},{"id":"B","text":"Column-level security"},{"id":"C","text":"Row-level security"},{"id":"D","text":"Secure views with session variables"}],"correctAnswer":"C","explanation":"Row-level security is the most appropriate feature for restricting access to customer data based on an analyst's assigned region. This feature allows defining security policies that filter rows dynamically based on attributes of the current session (like the analyst's region assignment). When analysts query the table, they only see rows that match their authorized regions. This provides fine-grained access control at the row level without requiring separate tables or views for each region, making it ideal for implementing data segregation based on attributes like region, department, or customer segment.*"},{"id":43,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that automatically logs all queries that access tables containing PII (Personally Identifiable Information). Which approach should be used?","options":[{"id":"A","text":"Create a UDF that logs access to a separate table"},{"id":"B","text":"Implement custom logging with stored procedures"},{"id":"C","text":"Query the SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY view"},{"id":"D","text":"Enable automatic audit logging for PII tables"}],"correctAnswer":"C","explanation":"Querying the SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY view is the most direct approach for logging all queries that access PII tables. This view in the Snowflake shared database automatically records all access to objects, including who accessed what data and when. By creating a scheduled task or procedure that queries this view with filters for the PII tables, the data engineer can create comprehensive audit logs without modifying the tables or implementing custom logging mechanisms. This leverages Snowflake's built-in auditing capabilities, ensuring complete coverage without adding overhead to query execution or requiring changes to application code.*"},{"id":44,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that prevents any single user from being able to both create and approve data quality exceptions. Which approach should be used?","options":[{"id":"A","text":"Implement separate roles with mutually exclusive privileges"},{"id":"B","text":"Use row-level security based on user attributes"},{"id":"C","text":"Create a secure view that filters based on user role"},{"id":"D","text":"Implement column-level security for sensitive operations"}],"correctAnswer":"A","explanation":"Implementing separate roles with mutually exclusive privileges is the most effective approach for enforcing separation of duties between creating and approving data quality exceptions. By creating distinct roles for exception creation and exception approval, and ensuring users are only assigned one of these roles, the data engineer can enforce strict segregation of responsibilities. This approach directly implements the principle of separation of duties, a fundamental security control that prevents conflicts of interest and reduces the risk of fraud or errors by ensuring that critical functions are divided among different individuals.*"},{"id":45,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows tracking the lineage of sensitive data as it moves through various transformations in Snowflake. Which feature or approach should be used?","options":[{"id":"A","text":"Snowflake's native data lineage tracking"},{"id":"B","text":"Object tagging with custom metadata"},{"id":"C","text":"Query history with transformation tracking"},{"id":"D","text":"Access history with data flow analysis"}],"correctAnswer":"B","explanation":"Object tagging with custom metadata is the most effective approach for tracking data lineage in Snowflake. By creating and applying tags that document source systems, transformation steps, sensitivity levels, and other lineage information to databases, schemas, tables, and columns, the data engineer can implement a comprehensive lineage tracking system. These tags can be queried through Snowflake's metadata views to generate lineage reports and diagrams. While Snowflake doesn't have native end-to-end lineage tracking specifically for sensitive data, this tagging approach leverages Snowflake's metadata capabilities to implement a flexible, queryable lineage solution that can focus on sensitive data elements.*"},{"id":46,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that ensures all copies of production data used in development environments have sensitive customer information automatically masked. Which approach should be used?","options":[{"id":"A","text":"Create secure views of production for development use"},{"id":"B","text":"Implement dynamic data masking policies on production tables"},{"id":"C","text":"Use post-clone scripts to apply masking when cloning databases"},{"id":"D","text":"Create separate ETL processes for development data"}],"correctAnswer":"C","explanation":"Using post-clone scripts to apply masking when cloning databases is the most effective approach for ensuring sensitive data is automatically masked in development environments. Post-clone scripts execute automatically after a database is cloned, allowing the data engineer to apply masking policies, remove sensitive data, or transform values specifically in the cloned environment. This approach ensures that all development copies created through cloning have consistent masking applied without affecting the production environment or requiring manual intervention. It provides a systematic, automated solution for protecting sensitive data in non-production environments.*"},{"id":47,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows granting access to all future tables in a schema without explicitly granting privileges on each new table as it's created. Which Snowflake feature should be used?","options":[{"id":"A","text":"Schema-level grants"},{"id":"B","text":"Future grants"},{"id":"C","text":"Role inheritance"},{"id":"D","text":"Managed access schemas"}],"correctAnswer":"B","explanation":"Future grants is the specific Snowflake feature designed to grant privileges on objects that will be created in the future. Using the syntax \`GRANT <privilege> ON FUTURE TABLES IN SCHEMA <schema_name> TO ROLE <role_name>\`, you can ensure that as new tables are created in the schema, the specified role automatically receives the granted privileges without requiring additional grant statements. This feature is essential for maintaining consistent access control in dynamic environments where new objects are frequently created, reducing administrative overhead and preventing security gaps that could occur if manual grants are forgotten for new objects.*"},{"id":48,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that allows different departments to have their own isolated Snowflake environments while sharing common reference data. Which Snowflake feature should be used?","options":[{"id":"A","text":"Database replication"},{"id":"B","text":"Zero-copy cloning"},{"id":"C","text":"Data sharing"},{"id":"D","text":"Secure views"}],"correctAnswer":"C","explanation":"Data sharing is the most appropriate Snowflake feature for allowing different departments to have isolated environments while sharing common reference data. Snowflake's data sharing allows read-only access to specific databases, schemas, or tables across different Snowflake accounts or within the same account. This enables a central team to maintain reference data in one location while securely sharing it with multiple department-specific Snowflake environments. The shared data appears as a database in the consumer account but doesn't consume additional storage, and updates to the source data are immediately visible to all consumers, ensuring consistency across departments.*"},{"id":49,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that classifies and tags data based on sensitivity levels (Public, Internal, Confidential, Restricted) and enforces appropriate access controls based on these classifications. Which approach should be used?","options":[{"id":"A","text":"Implement column-level security based on sensitivity"},{"id":"B","text":"Create separate schemas for each sensitivity level"},{"id":"C","text":"Use object tagging with tag-based masking policies"},{"id":"D","text":"Implement row-level security with sensitivity filters"}],"correctAnswer":"C","explanation":"Using object tagging with tag-based masking policies is the most comprehensive approach for implementing data classification and enforcing appropriate access controls. This approach allows tagging tables and columns with sensitivity classifications (Public, Internal, Confidential, Restricted) and then creating masking policies that reference these tags to apply appropriate protection measures. By linking masking policies to tags rather than directly to objects, you create a scalable, maintainable system where protection automatically follows classification. This approach centralizes policy definition while providing fine-grained control, and it can be combined with other security measures like row-level security for comprehensive data protection.*"},{"id":50,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that executes a series of data transformation steps in sequence, with each step starting only after the previous step successfully completes. Which Snowflake feature should be used?","options":[{"id":"A","text":"Stored procedures"},{"id":"B","text":"User-defined functions"},{"id":"C","text":"Tasks with dependencies"},{"id":"D","text":"Streams with triggers"}],"correctAnswer":"C","explanation":"Tasks with dependencies is the most appropriate feature for implementing sequential, dependent data transformation steps in Snowflake. Tasks allow scheduling SQL commands or stored procedures to run at defined intervals or in response to events. By creating a DAG (directed acyclic graph) of tasks with predecessor/successor relationships using the AFTER parameter, the data engineer can ensure that each step only executes after its predecessor successfully completes. This provides a native, serverless orchestration solution within Snowflake for managing complex transformation workflows with proper dependency management and error handling.*"},{"id":51,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that captures changes to a source table and applies them to a target table incrementally. Which Snowflake feature should be used?","options":[{"id":"A","text":"Zero-copy cloning"},{"id":"B","text":"Time Travel"},{"id":"C","text":"Streams"},{"id":"D","text":"External tables"}],"correctAnswer":"C","explanation":"Streams is the most appropriate Snowflake feature for capturing and applying incremental changes from a source table to a target table. Streams track DML changes (inserts, updates, deletes) to a table and make those changes available for processing. When combined with tasks, streams enable building efficient change data capture (CDC) pipelines that only process modified data rather than reprocessing the entire dataset. This approach minimizes processing overhead and ensures that the target table stays synchronized with the source while only processing the delta of changes, making it ideal for incremental data transformation scenarios.*"},{"id":52,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement complex data transformations that require procedural logic, loops, and error handling. Which Snowflake feature should be used?","options":[{"id":"A","text":"SQL UDFs"},{"id":"B","text":"Stored procedures"},{"id":"C","text":"Tasks"},{"id":"D","text":"Streams"}],"correctAnswer":"B","explanation":"Stored procedures are the most appropriate feature for implementing complex data transformations requiring procedural logic, loops, and error handling in Snowflake. Unlike SQL UDFs which are limited to returning a single value, stored procedures support full procedural programming with control structures (IF/ELSE, LOOP), error handling (TRY/CATCH), and multiple SQL operations. Snowflake supports stored procedures written in JavaScript, SQL, and other languages, providing the flexibility needed for complex transformation logic that goes beyond what can be expressed in standard SQL alone.*"},{"id":53,"domain":"Data Movement (17 questions)","text":"A data engineer needs to transform semi-structured JSON data stored in a VARIANT column into a relational format for analysis. Which SQL function should be used?","options":[{"id":"A","text":"PARSE_JSON"},{"id":"B","text":"GET"},{"id":"C","text":"FLATTEN"},{"id":"D","text":"TO_JSON"}],"correctAnswer":"C","explanation":"The FLATTEN function is the most appropriate choice for transforming semi-structured JSON data in a VARIANT column into a relational format for analysis. FLATTEN performs a lateral join that expands nested arrays into multiple rows, effectively normalizing hierarchical data into a tabular structure. When combined with other Snowflake JSON functions like GET, PARSE_JSON, or dot notation, FLATTEN enables comprehensive transformation of complex nested structures into relational tables. This is particularly useful for analytics on semi-structured data that needs to be joined with traditional relational data or analyzed using standard SQL aggregations and grouping operations.*"},{"id":54,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs different transformations on data based on its source system. The logic for determining the transformation rules is complex. Which approach should be used?","options":[{"id":"A","text":"Create separate transformation queries for each source system"},{"id":"B","text":"Implement a JavaScript stored procedure with conditional logic"},{"id":"C","text":"Use SQL CASE statements in a view"},{"id":"D","text":"Create a lookup table with transformation rules"}],"correctAnswer":"B","explanation":"Implementing a JavaScript stored procedure with conditional logic is the most flexible approach for handling complex transformation rules based on source systems. JavaScript stored procedures in Snowflake support sophisticated programming constructs, including complex conditional logic, custom functions, and external API calls if needed. This approach allows encapsulating the entire transformation logic in a single, maintainable procedure that can handle the complexity of different rules for different source systems, including edge cases and exceptions that might be difficult to express in pure SQL constructs like CASE statements.*"},{"id":55,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that maintains a slowly changing dimension (SCD) Type 2 for customer data, preserving the history of changes. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake Streams to capture changes and apply them with effective dates"},{"id":"B","text":"Implement a MERGE statement that handles both inserts and updates"},{"id":"C","text":"Create a view that uses Time Travel to show historical data"},{"id":"D","text":"Use zero-copy cloning to create daily snapshots"}],"correctAnswer":"B","explanation":"Implementing a MERGE statement that handles both inserts and updates is the most comprehensive approach for maintaining a Slowly Changing Dimension (SCD) Type 2. The MERGE statement can be designed to identify changed records, expire the current records by setting an end date, and insert new records with current effective dates in a single atomic operation. This approach ensures referential integrity and transaction consistency while efficiently implementing the SCD Type 2 pattern. While Streams can capture changes, the MERGE statement provides more direct control over the SCD-specific logic of maintaining effective dates and historical records.*"},{"id":56,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs data quality checks on transformed data before loading it into a target table. If the checks fail, the process should be halted and an alert should be sent. Which approach should be used?","options":[{"id":"A","text":"Use a stored procedure with error handling and custom logging"},{"id":"B","text":"Implement a stream to capture failed records"},{"id":"C","text":"Create a task that checks data quality before loading"},{"id":"D","text":"Use a secure view with row-level security to filter invalid data"}],"correctAnswer":"A","explanation":"Using a stored procedure with error handling and custom logging is the most comprehensive approach for implementing data quality checks with alerting capabilities. Stored procedures allow combining SQL validation queries with conditional logic to evaluate results, transaction control to commit or rollback based on quality checks, and integration with external alerting mechanisms through API calls or Snowflake features like notifications. This approach provides complete control over the validation process, error handling, and alerting workflow, making it ideal for implementing robust data quality gates in transformation pipelines.*"},{"id":57,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that executes a complex transformation workflow on a schedule, with different steps running at different frequencies. Which approach should be used?","options":[{"id":"A","text":"Create multiple independent tasks with different schedules"},{"id":"B","text":"Use a single task with a CASE statement to determine which transformations to run"},{"id":"C","text":"Implement a stored procedure that uses the current time to decide which steps to execute"},{"id":"D","text":"Create a task tree with different schedules for different branches"}],"correctAnswer":"A","explanation":"Creating multiple independent tasks with different schedules is the most appropriate approach for implementing transformation steps that need to run at different frequencies. Each task can be configured with its own schedule expression (CRON format in Snowflake) to define exactly when it should execute, allowing for precise control over execution timing. This approach maintains separation of concerns between different transformation steps while leveraging Snowflake's native scheduling capabilities. For steps with dependencies, additional task relationships can be defined to ensure proper sequencing while still maintaining independent scheduling.*"},{"id":58,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs complex string manipulations and regular expression operations as part of a data transformation pipeline. Which Snowflake feature provides the most comprehensive support for these operations?","options":[{"id":"A","text":"SQL UDFs"},{"id":"B","text":"JavaScript UDFs"},{"id":"C","text":"Built-in SQL string functions"},{"id":"D","text":"External functions"}],"correctAnswer":"C","explanation":"Snowflake's built-in SQL string functions provide the most comprehensive and efficient support for string manipulations and regular expression operations in data transformation pipelines. Snowflake offers a rich set of native functions including REGEXP_REPLACE, REGEXP_SUBSTR, SPLIT, TRANSLATE, and many others that can handle complex string transformations directly in SQL. These functions are optimized for performance within Snowflake's engine and can operate on entire columns of data in parallel. While JavaScript UDFs offer programming flexibility, the native SQL functions provide better performance and integration with Snowflake's optimization capabilities for large-scale data processing.*"},{"id":59,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs different transformations on data based on the current processing date, with special handling for month-end, quarter-end, and year-end processing. Which approach should be used?","options":[{"id":"A","text":"Create separate tasks for each type of processing period"},{"id":"B","text":"Use a stored procedure with date logic to determine the appropriate transformations"},{"id":"C","text":"Implement a CASE statement in a SQL query based on date functions"},{"id":"D","text":"Create a calendar reference table to look up processing types"}],"correctAnswer":"B","explanation":"Using a stored procedure with date logic is the most flexible approach for implementing different transformations based on the current processing date. A stored procedure can incorporate sophisticated date calculations to determine if the current date is a month-end, quarter-end, or year-end, and then execute the appropriate transformation logic for each scenario. This approach centralizes the date determination logic and transformation selection in a single, maintainable component while providing the procedural capabilities needed to handle complex conditional processing based on temporal patterns and exceptions.*"},{"id":60,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs aggregations on large datasets with high cardinality group-by columns. The aggregations need to be refreshed daily. Which approach would provide the best performance?","options":[{"id":"A","text":"Create a materialized view with the aggregation logic"},{"id":"B","text":"Use a task to calculate and store aggregates in a separate table"},{"id":"C","text":"Implement a clustering key on the group-by columns"},{"id":"D","text":"Use a JavaScript UDF to perform custom aggregations"}],"correctAnswer":"B","explanation":"Using a task to calculate and store aggregates in a separate table is the most effective approach for high-performance aggregations on large datasets with high cardinality group-by columns. This approach pre-computes the aggregations during a scheduled maintenance window and stores the results in a dedicated table optimized for query performance. Unlike materialized views which have certain limitations with complex aggregations, this approach provides complete control over the aggregation logic, indexing strategy, and refresh schedule. The task can be scheduled to run daily, ensuring the aggregated data is refreshed at the required frequency while minimizing the performance impact on user queries.*"},{"id":61,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that transforms data from a third-party API and loads it into Snowflake. The API returns complex nested JSON structures. Which approach should be used?","options":[{"id":"A","text":"Use Snowflake's REST API integration to directly query the third-party API"},{"id":"B","text":"Create an external function that calls the API and processes the results"},{"id":"C","text":"Use a serverless function to fetch, transform, and load the data via Snowflake's native connectors"},{"id":"D","text":"Implement a JavaScript stored procedure that uses the Snowflake HTTP client to call the API"}],"correctAnswer":"D","explanation":"Implementing a JavaScript stored procedure that uses the Snowflake HTTP client is the most direct approach for transforming data from a third-party API. Snowflake's JavaScript stored procedures can use the built-in HTTP client to make API calls, process the returned JSON using JavaScript's native JSON handling capabilities, and then insert the transformed data into Snowflake tables. This approach keeps the entire ETL process within Snowflake, eliminating the need for external components or data movement, while providing the programming flexibility needed to handle complex nested JSON structures and API-specific requirements.*"},{"id":62,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs complex window functions across multiple dimensions with different partition and order specifications. Which approach would provide the most maintainable solution?","options":[{"id":"A","text":"Create multiple CTEs, each with a specific window function"},{"id":"B","text":"Use subqueries with different GROUP BY clauses"},{"id":"C","text":"Implement a JavaScript UDF that performs custom windowing"},{"id":"D","text":"Create a stored procedure that generates dynamic SQL"}],"correctAnswer":"A","explanation":"Creating multiple Common Table Expressions (CTEs), each with a specific window function, provides the most maintainable solution for complex window calculations across multiple dimensions. This approach breaks down the complex logic into modular, readable components where each CTE handles a specific windowing operation with its own partition and order specifications. The final query can then join or combine these intermediate results as needed. This modular structure improves readability, debugging, and maintenance compared to nested subqueries or dynamic SQL approaches, while leveraging Snowflake's optimization capabilities for window functions.*"},{"id":63,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs data transformations requiring custom mathematical operations not available in standard SQL functions. Which approach should be used?","options":[{"id":"A","text":"Create a SQL UDF with mathematical formulas"},{"id":"B","text":"Implement a JavaScript UDF with custom algorithms"},{"id":"C","text":"Use a Python UDF for advanced mathematical operations"},{"id":"D","text":"Create a stored procedure with mathematical logic"}],"correctAnswer":"C","explanation":"Using a Python UDF (User-Defined Function) is the most appropriate approach for implementing custom mathematical operations not available in standard SQL. Python UDFs in Snowflake allow leveraging Python's rich ecosystem of scientific and mathematical libraries like NumPy, SciPy, and pandas, which provide advanced mathematical capabilities beyond what's available in SQL or JavaScript. This approach combines the power of Python's mathematical libraries with Snowflake's data processing capabilities, enabling sophisticated calculations like statistical analysis, machine learning scoring, or complex mathematical algorithms to be applied directly within Snowflake queries.*"},{"id":64,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that performs incremental data transformations on a large fact table, processing only new and changed records since the last run. Which combination of Snowflake features should be used?","options":[{"id":"A","text":"Time Travel and zero-copy cloning"},{"id":"B","text":"Streams and tasks"},{"id":"C","text":"External tables with auto refresh"},{"id":"D","text":"Materialized views with automatic clustering"}],"correctAnswer":"B","explanation":"The combination of streams and tasks is the most effective solution for implementing incremental data transformations. Streams capture the changes (inserts, updates, deletes) to the source table since the last processing run, enabling the transformation logic to process only the delta rather than the entire dataset. Tasks provide the scheduling and execution framework to run these incremental transformations automatically at defined intervals. This approach minimizes processing overhead and resource consumption while ensuring the target fact table stays current with source changes, making it ideal for large fact tables where full reprocessing would be prohibitively expensive.*"},{"id":65,"domain":"Data Movement (17 questions)","text":"A data engineer needs to implement a solution that dynamically generates and executes SQL statements based on metadata stored in a control table. Which approach should be used?","options":[{"id":"A","text":"Create a JavaScript stored procedure that builds and executes dynamic SQL"},{"id":"B","text":"Use a SQL stored procedure with string concatenation"},{"id":"C","text":"Implement a Python UDF that generates SQL"},{"id":"D","text":"Create a task for each possible SQL statement"}],"correctAnswer":"A","explanation":"Creating a JavaScript stored procedure that builds and executes dynamic SQL is the most flexible approach for implementing metadata-driven transformations. JavaScript stored procedures in Snowflake can read metadata from control tables, construct SQL statements dynamically using string manipulation, and execute those statements using the \`snowflake.execute()\` method. This approach provides the programming flexibility needed to handle complex logic for SQL generation while keeping the entire process within Snowflake. It allows for sophisticated error handling, logging, and conditional logic that would be difficult to implement with SQL stored procedures or other approaches.*"}]`),Am=[{id:1,name:"Data Movement (17 questions)"},{id:2,name:"Performance Optimization (14 questions)"},{id:3,name:"Storage and Data Protection (9 questions)"},{id:4,name:"Data Governance (9 questions)"},{id:5,name:"Data Transformation (16 questions)"}],Cm={id:xm,title:Sm,description:bm,questions:km,domains:Am},Cd=[hm,ym,Cm];function Tm(){const[o,c]=D.useState(null),[l,u]=D.useState(null),[h,g]=D.useState([]),[S,y]=D.useState(!1);D.useEffect(()=>{if(o){const q=Cd.find(z=>z.id===o);q&&(u(q),y(!1))}else u(null)},[o]);const k=q=>{c(q)},A=q=>{g(q),y(!0)},T=()=>{y(!1)},E=()=>{c(null),u(null),y(!1)};return b.jsxs("div",{className:"min-h-screen bg-slate-50",children:[!l&&b.jsx(cf,{tests:Cd,onSelectTest:k}),l&&!S&&b.jsx(om,{questions:l.questions,testId:l.id,onComplete:A,onExit:E}),l&&S&&b.jsx(sm,{test:l,userAnswers:h,onRetakeTest:T,onSelectNewTest:E})]})}bh.createRoot(document.getElementById("root")).render(b.jsx(D.StrictMode,{children:b.jsx(Tm,{})}));
